/* Generated automatically by the program `genflags'
   from the machine description file `md'.  */

#ifndef GCC_INSN_FLAGS_H
#define GCC_INSN_FLAGS_H

#define HAVE_ccmpqi (TARGET_APX_CCMP)
#define HAVE_ccmphi (TARGET_APX_CCMP)
#define HAVE_ccmpsi (TARGET_APX_CCMP)
#define HAVE_x86_sahf_1 (TARGET_SAHF)
#define HAVE_x86_stc 1
#define HAVE_pushflsi2 ((GET_MODE_CLASS (GET_MODE (operands[1])) == MODE_CC) && (word_mode == SImode))
#define HAVE_pushfldi2 ((GET_MODE_CLASS (GET_MODE (operands[1])) == MODE_CC) && (word_mode == DImode))
#define HAVE_popflsi1 (word_mode == SImode)
#define HAVE_popfldi1 (word_mode == DImode)
#define HAVE_swapsi 1
#define HAVE_insvhi_1 1
#define HAVE_insvsi_1 1
#define HAVE_push2_di (TARGET_APX_PUSH2POP2)
#define HAVE_pop2_di (TARGET_APX_PUSH2POP2)
#define HAVE_popp_di (TARGET_APX_PPX)
#define HAVE_push2p_di (TARGET_APX_PUSH2POP2 && TARGET_APX_PPX)
#define HAVE_pop2p_di (TARGET_APX_PUSH2POP2 && TARGET_APX_PPX)
#define HAVE_zero_extendqisi2_and (TARGET_ZERO_EXTEND_WITH_AND && optimize_function_for_speed_p (cfun))
#define HAVE_zero_extendhisi2_and (TARGET_ZERO_EXTEND_WITH_AND && optimize_function_for_speed_p (cfun))
#define HAVE_zero_extendqihi2_and (TARGET_ZERO_EXTEND_WITH_AND && optimize_function_for_speed_p (cfun))
#define HAVE_extendsidi2_1 1
#define HAVE_extendhisi2 1
#define HAVE_extendqisi2 1
#define HAVE_extendqihi2 1
#define HAVE_extendbfsf2_1 (TARGET_SSE2)
#define HAVE_truncdfsf2 (TARGET_80387 || (TARGET_SSE2 && TARGET_SSE_MATH))
#define HAVE_truncxfsf2 (TARGET_80387)
#define HAVE_truncxfdf2 (TARGET_80387)
#define HAVE_truncsfbf2 (TARGET_SSE2 && !HONOR_NANS (BFmode) && !flag_rounding_math \
   && (flag_unsafe_math_optimizations \
       || TARGET_AVXNECONVERT \
       || (TARGET_AVX512BF16 && TARGET_AVX512VL)))
#define HAVE_fix_trunchfsi2 (TARGET_AVX512FP16)
#define HAVE_fixuns_trunchfsi2 (TARGET_AVX512FP16)
#define HAVE_fixuns_truncsfsi2_avx512f (TARGET_AVX512F && TARGET_SSE_MATH)
#define HAVE_fixuns_truncdfsi2_avx512f (TARGET_AVX512F && TARGET_SSE_MATH)
#define HAVE_fix_truncsfsi_sse (SSE_FLOAT_MODE_P (SFmode) \
   && (!TARGET_FISTTP || TARGET_SSE_MATH))
#define HAVE_fix_truncdfsi_sse (SSE_FLOAT_MODE_P (DFmode) \
   && (!TARGET_FISTTP || TARGET_SSE_MATH))
#define HAVE_fix_trunchi_i387_fisttp (X87_FLOAT_MODE_P (GET_MODE (operands[1])) \
   && TARGET_FISTTP \
   && !((SSE_FLOAT_MODE_P (GET_MODE (operands[1])) \
	 && (TARGET_64BIT || HImode != DImode)) \
	&& TARGET_SSE_MATH))
#define HAVE_fix_truncsi_i387_fisttp (X87_FLOAT_MODE_P (GET_MODE (operands[1])) \
   && TARGET_FISTTP \
   && !((SSE_FLOAT_MODE_P (GET_MODE (operands[1])) \
	 && (TARGET_64BIT || SImode != DImode)) \
	&& TARGET_SSE_MATH))
#define HAVE_fix_truncdi_i387_fisttp (X87_FLOAT_MODE_P (GET_MODE (operands[1])) \
   && TARGET_FISTTP \
   && !((SSE_FLOAT_MODE_P (GET_MODE (operands[1])) \
	 && (TARGET_64BIT || DImode != DImode)) \
	&& TARGET_SSE_MATH))
#define HAVE_fix_truncdi_i387 (X87_FLOAT_MODE_P (GET_MODE (operands[1])) \
   && !TARGET_FISTTP \
   && !(TARGET_64BIT && SSE_FLOAT_MODE_P (GET_MODE (operands[1]))))
#define HAVE_fix_trunchi_i387 (X87_FLOAT_MODE_P (GET_MODE (operands[1])) \
   && !TARGET_FISTTP \
   && !SSE_FLOAT_MODE_P (GET_MODE (operands[1])))
#define HAVE_fix_truncsi_i387 (X87_FLOAT_MODE_P (GET_MODE (operands[1])) \
   && !TARGET_FISTTP \
   && !SSE_FLOAT_MODE_P (GET_MODE (operands[1])))
#define HAVE_x86_fnstcw_1 (TARGET_80387)
#define HAVE_floathisf2 (TARGET_80387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387))
#define HAVE_floathidf2 (TARGET_80387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387))
#define HAVE_floathixf2 (TARGET_80387 \
   && (!(SSE_FLOAT_MODE_P (XFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387))
#define HAVE_floatsixf2 (TARGET_80387)
#define HAVE_floatdixf2 (TARGET_80387)
#define HAVE_floatsihf2 (TARGET_AVX512FP16)
#define HAVE_floatunssihf2 (TARGET_AVX512FP16)
#define HAVE_floatdisf2_i387_with_xmm (!TARGET_64BIT && TARGET_INTER_UNIT_MOVES_TO_VEC \
   && TARGET_80387 && X87_ENABLE_FLOAT (SFmode, DImode) \
   && TARGET_SSE2 && optimize_function_for_speed_p (cfun))
#define HAVE_floatdidf2_i387_with_xmm (!TARGET_64BIT && TARGET_INTER_UNIT_MOVES_TO_VEC \
   && TARGET_80387 && X87_ENABLE_FLOAT (DFmode, DImode) \
   && TARGET_SSE2 && optimize_function_for_speed_p (cfun))
#define HAVE_floatdixf2_i387_with_xmm (!TARGET_64BIT && TARGET_INTER_UNIT_MOVES_TO_VEC \
   && TARGET_80387 && X87_ENABLE_FLOAT (XFmode, DImode) \
   && TARGET_SSE2 && optimize_function_for_speed_p (cfun))
#define HAVE_floatunssisf2_i387_with_xmm (!TARGET_64BIT \
   && TARGET_80387 && X87_ENABLE_FLOAT (SFmode, DImode) \
   && TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC)
#define HAVE_floatunssidf2_i387_with_xmm (!TARGET_64BIT \
   && TARGET_80387 && X87_ENABLE_FLOAT (DFmode, DImode) \
   && TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC)
#define HAVE_floatunssixf2_i387_with_xmm (!TARGET_64BIT \
   && TARGET_80387 && X87_ENABLE_FLOAT (XFmode, DImode) \
   && TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC)
#define HAVE_addvqi4_1 (ix86_binary_operator_ok (PLUS, QImode, operands, TARGET_APX_NDD) \
   && CONST_INT_P (operands[2]) \
   && INTVAL (operands[2]) == INTVAL (operands[3]))
#define HAVE_addvhi4_1 (ix86_binary_operator_ok (PLUS, HImode, operands, TARGET_APX_NDD) \
   && CONST_INT_P (operands[2]) \
   && INTVAL (operands[2]) == INTVAL (operands[3]))
#define HAVE_addvsi4_1 (ix86_binary_operator_ok (PLUS, SImode, operands, TARGET_APX_NDD) \
   && CONST_INT_P (operands[2]) \
   && INTVAL (operands[2]) == INTVAL (operands[3]))
#define HAVE_addvdi4_1 ((ix86_binary_operator_ok (PLUS, DImode, operands, TARGET_APX_NDD) \
   && CONST_INT_P (operands[2]) \
   && INTVAL (operands[2]) == INTVAL (operands[3])) && (TARGET_64BIT))
#define HAVE_subvqi4_1 (ix86_binary_operator_ok (MINUS, QImode, operands, TARGET_APX_NDD) \
   && CONST_INT_P (operands[2]) \
   && INTVAL (operands[2]) == INTVAL (operands[3]))
#define HAVE_subvhi4_1 (ix86_binary_operator_ok (MINUS, HImode, operands, TARGET_APX_NDD) \
   && CONST_INT_P (operands[2]) \
   && INTVAL (operands[2]) == INTVAL (operands[3]))
#define HAVE_subvsi4_1 (ix86_binary_operator_ok (MINUS, SImode, operands, TARGET_APX_NDD) \
   && CONST_INT_P (operands[2]) \
   && INTVAL (operands[2]) == INTVAL (operands[3]))
#define HAVE_subvdi4_1 ((ix86_binary_operator_ok (MINUS, DImode, operands, TARGET_APX_NDD) \
   && CONST_INT_P (operands[2]) \
   && INTVAL (operands[2]) == INTVAL (operands[3])) && (TARGET_64BIT))
#define HAVE_addqi3_carry (ix86_binary_operator_ok (PLUS, QImode, operands, TARGET_APX_NDD))
#define HAVE_addhi3_carry (ix86_binary_operator_ok (PLUS, HImode, operands, TARGET_APX_NDD))
#define HAVE_addsi3_carry (ix86_binary_operator_ok (PLUS, SImode, operands, TARGET_APX_NDD))
#define HAVE_adddi3_carry ((ix86_binary_operator_ok (PLUS, DImode, operands, TARGET_APX_NDD)) && (TARGET_64BIT))
#define HAVE_addcarrysi (ix86_binary_operator_ok (PLUS, SImode, operands, TARGET_APX_NDD))
#define HAVE_addcarrydi ((ix86_binary_operator_ok (PLUS, DImode, operands, TARGET_APX_NDD)) && (TARGET_64BIT))
#define HAVE_subqi3_carry (ix86_binary_operator_ok (MINUS, QImode, operands, TARGET_APX_NDD))
#define HAVE_subhi3_carry (ix86_binary_operator_ok (MINUS, HImode, operands, TARGET_APX_NDD))
#define HAVE_subsi3_carry (ix86_binary_operator_ok (MINUS, SImode, operands, TARGET_APX_NDD))
#define HAVE_subdi3_carry ((ix86_binary_operator_ok (MINUS, DImode, operands, TARGET_APX_NDD)) && (TARGET_64BIT))
#define HAVE_subsi3_carry_ccc 1
#define HAVE_subsi3_carry_ccgz 1
#define HAVE_subborrowsi (ix86_binary_operator_ok (MINUS, SImode, operands, TARGET_APX_NDD))
#define HAVE_subborrowdi ((ix86_binary_operator_ok (MINUS, DImode, operands, TARGET_APX_NDD)) && (TARGET_64BIT))
#define HAVE_addqi3_cc_overflow_1 (ix86_binary_operator_ok (PLUS, QImode, operands, TARGET_APX_NDD))
#define HAVE_addhi3_cc_overflow_1 (ix86_binary_operator_ok (PLUS, HImode, operands, TARGET_APX_NDD))
#define HAVE_addsi3_cc_overflow_1 (ix86_binary_operator_ok (PLUS, SImode, operands, TARGET_APX_NDD))
#define HAVE_adddi3_cc_overflow_1 ((ix86_binary_operator_ok (PLUS, DImode, operands, TARGET_APX_NDD)) && (TARGET_64BIT))
#define HAVE_smulsi3_highpart 1
#define HAVE_umulsi3_highpart 1
#define HAVE_divmodsi4_1 1
#define HAVE_udivmodsi4_1 1
#define HAVE_divmodhiqi3_nf (TARGET_QIMODE_MATH \
   && TARGET_APX_NF)
#define HAVE_divmodhiqi3 (TARGET_QIMODE_MATH \
   && true)
#define HAVE_udivmodhiqi3_nf (TARGET_QIMODE_MATH \
   && TARGET_APX_NF)
#define HAVE_udivmodhiqi3 (TARGET_QIMODE_MATH \
   && true)
#define HAVE_ashldi3_doubleword 1
#define HAVE_x86_64_shld_ndd_nf (TARGET_APX_NDD && TARGET_APX_NF)
#define HAVE_x86_64_shld_ndd (TARGET_APX_NDD && true)
#define HAVE_x86_64_shld_ndd_1_nf (TARGET_APX_NDD \
   && INTVAL (operands[4]) == 64 - INTVAL (operands[3]) \
   && TARGET_APX_NF)
#define HAVE_x86_64_shld_ndd_1 (TARGET_APX_NDD \
   && INTVAL (operands[4]) == 64 - INTVAL (operands[3]) \
   && true)
#define HAVE_x86_shld_nf (TARGET_APX_NF)
#define HAVE_x86_shld 1
#define HAVE_x86_shld_ndd_nf (TARGET_APX_NDD && TARGET_APX_NF)
#define HAVE_x86_shld_ndd (TARGET_APX_NDD && true)
#define HAVE_x86_shld_1_nf (INTVAL (operands[3]) == 32 - INTVAL (operands[2]) \
  && TARGET_APX_NF)
#define HAVE_x86_shld_1 (INTVAL (operands[3]) == 32 - INTVAL (operands[2]) \
  && true)
#define HAVE_x86_shld_ndd_1_nf (TARGET_APX_NDD  \
   && INTVAL (operands[4]) == 32 - INTVAL (operands[3]) \
   && TARGET_APX_NF)
#define HAVE_x86_shld_ndd_1 (TARGET_APX_NDD  \
   && INTVAL (operands[4]) == 32 - INTVAL (operands[3]) \
   && true)
#define HAVE_lshrdi3_doubleword 1
#define HAVE_ashrdi3_doubleword 1
#define HAVE_lshrdi3_doubleword_lowpart_nf ((TARGET_APX_NF && UINTVAL (operands[2]) < 4 * BITS_PER_UNIT) && (!TARGET_64BIT))
#define HAVE_ashrdi3_doubleword_lowpart_nf ((TARGET_APX_NF && UINTVAL (operands[2]) < 4 * BITS_PER_UNIT) && (!TARGET_64BIT))
#define HAVE_lshrdi3_doubleword_lowpart ((UINTVAL (operands[2]) < 4 * BITS_PER_UNIT) && (!TARGET_64BIT))
#define HAVE_ashrdi3_doubleword_lowpart ((UINTVAL (operands[2]) < 4 * BITS_PER_UNIT) && (!TARGET_64BIT))
#define HAVE_x86_64_shrd_ndd_nf (TARGET_APX_NDD && TARGET_APX_NF)
#define HAVE_x86_64_shrd_ndd (TARGET_APX_NDD && true)
#define HAVE_x86_64_shrd_ndd_1_nf (TARGET_APX_NDD \
   && INTVAL (operands[4]) == 64 - INTVAL (operands[3]) \
   && TARGET_APX_NF)
#define HAVE_x86_64_shrd_ndd_1 (TARGET_APX_NDD \
   && INTVAL (operands[4]) == 64 - INTVAL (operands[3]) \
   && true)
#define HAVE_x86_shrd_nf (TARGET_APX_NF)
#define HAVE_x86_shrd 1
#define HAVE_x86_shrd_ndd_nf (TARGET_APX_NDD && TARGET_APX_NF)
#define HAVE_x86_shrd_ndd (TARGET_APX_NDD && true)
#define HAVE_x86_shrd_1_nf (INTVAL (operands[3]) == 32 - INTVAL (operands[2]) \
   && TARGET_APX_NF)
#define HAVE_x86_shrd_1 (INTVAL (operands[3]) == 32 - INTVAL (operands[2]) \
   && true)
#define HAVE_x86_shrd_ndd_1_nf (TARGET_APX_NDD \
   && (INTVAL (operands[4]) == 32 - INTVAL (operands[3])) \
   && TARGET_APX_NF)
#define HAVE_x86_shrd_ndd_1 (TARGET_APX_NDD \
   && (INTVAL (operands[4]) == 32 - INTVAL (operands[3])) \
   && true)
#define HAVE_ashrsi3_cvt_nf (INTVAL (operands[2]) == GET_MODE_BITSIZE (SImode)-1 \
   && (TARGET_USE_CLTD || optimize_function_for_size_p (cfun)) \
   && ix86_binary_operator_ok (ASHIFTRT, SImode, operands, TARGET_APX_NDD) \
   && TARGET_APX_NF)
#define HAVE_ashrsi3_cvt (INTVAL (operands[2]) == GET_MODE_BITSIZE (SImode)-1 \
   && (TARGET_USE_CLTD || optimize_function_for_size_p (cfun)) \
   && ix86_binary_operator_ok (ASHIFTRT, SImode, operands, TARGET_APX_NDD) \
   && true)
#define HAVE_ashrdi3_cvt_nf ((INTVAL (operands[2]) == GET_MODE_BITSIZE (DImode)-1 \
   && (TARGET_USE_CLTD || optimize_function_for_size_p (cfun)) \
   && ix86_binary_operator_ok (ASHIFTRT, DImode, operands, TARGET_APX_NDD) \
   && TARGET_APX_NF) && (TARGET_64BIT))
#define HAVE_ashrdi3_cvt ((INTVAL (operands[2]) == GET_MODE_BITSIZE (DImode)-1 \
   && (TARGET_USE_CLTD || optimize_function_for_size_p (cfun)) \
   && ix86_binary_operator_ok (ASHIFTRT, DImode, operands, TARGET_APX_NDD) \
   && true) && (TARGET_64BIT))
#define HAVE_ix86_rotldi3_doubleword 1
#define HAVE_ix86_rotrdi3_doubleword 1
#define HAVE_rotl32di2_doubleword 1
#define HAVE_rotr32di2_doubleword 1
#define HAVE_rcrsi2 1
#define HAVE_lshrsi3_carry 1
#define HAVE_ashrsi3_carry 1
#define HAVE_setcc_sf_sse (SSE_FLOAT_MODE_P (SFmode))
#define HAVE_setcc_df_sse (SSE_FLOAT_MODE_P (DFmode))
#define HAVE_setcc_hf_mask (TARGET_AVX512FP16)
#define HAVE_jump 1
#define HAVE_blockage 1
#define HAVE_prologue_use 1
#define HAVE_simple_return_internal (reload_completed)
#define HAVE_interrupt_return (reload_completed)
#define HAVE_simple_return_internal_long (reload_completed)
#define HAVE_simple_return_pop_internal (reload_completed)
#define HAVE_nop 1
#define HAVE_nops (reload_completed)
#define HAVE_max_skip_align 1
#define HAVE_set_got_offset_rex64 (TARGET_LP64)
#define HAVE_eh_return_internal 1
#define HAVE_split_stack_return 1
#define HAVE_ffssi2_no_cmove (!TARGET_CMOVE)
#define HAVE_ctzsi2 1
#define HAVE_bsr 1
#define HAVE_bsr_1 (!TARGET_LZCNT)
#define HAVE_clzsi2_lzcnt_nf (TARGET_APX_NF && TARGET_LZCNT)
#define HAVE_clzsi2_lzcnt (TARGET_LZCNT)
#define HAVE_tzcnt_si_nf ((TARGET_APX_NF) && (TARGET_BMI))
#define HAVE_lzcnt_si_nf ((TARGET_APX_NF) && (TARGET_LZCNT))
#define HAVE_tzcnt_si (TARGET_BMI)
#define HAVE_lzcnt_si (TARGET_LZCNT)
#define HAVE_tzcnt_hi_nf ((TARGET_APX_NF) && (TARGET_BMI))
#define HAVE_tzcnt_hi ((true) && (TARGET_BMI))
#define HAVE_lzcnt_hi_nf ((TARGET_APX_NF) && (TARGET_LZCNT))
#define HAVE_lzcnt_hi ((true) && (TARGET_LZCNT))
#define HAVE_bmi_bextr_si (TARGET_BMI)
#define HAVE_bmi2_pdep_si3 (TARGET_BMI2)
#define HAVE_bmi2_pext_si3 (TARGET_BMI2)
#define HAVE_tbm_bextri_si (TARGET_TBM)
#define HAVE_popcountsi2_nf (TARGET_APX_NF && TARGET_POPCNT)
#define HAVE_popcountsi2 (TARGET_POPCNT)
#define HAVE_popcounthi2_nf (TARGET_POPCNT && TARGET_APX_NF)
#define HAVE_popcounthi2 (TARGET_POPCNT && true)
#define HAVE_bswaphisi2_lowpart 1
#define HAVE_parityhi2_cmp 1
#define HAVE_parityqi2_cmp 1
#define HAVE_rcphf2 (TARGET_AVX512FP16)
#define HAVE_truncxfsf2_i387_noop_unspec (TARGET_USE_FANCY_MATH_387)
#define HAVE_truncxfdf2_i387_noop_unspec (TARGET_USE_FANCY_MATH_387)
#define HAVE_sqrtxf2 (TARGET_USE_FANCY_MATH_387)
#define HAVE_rsqrthf2 (TARGET_AVX512FP16)
#define HAVE_sqrthf2 (TARGET_AVX512FP16)
#define HAVE_x86_fnstsw_1 (TARGET_80387)
#define HAVE_fpremxf4_i387 (TARGET_USE_FANCY_MATH_387)
#define HAVE_fprem1xf4_i387 (TARGET_USE_FANCY_MATH_387)
#define HAVE_sinxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_cosxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_sincosxf3 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_fptanxf4_i387 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_atan2xf3 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_fyl2xxf3_i387 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_fyl2xp1xf3_i387 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_fxtractxf3_i387 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_fscalexf4_i387 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_avx512f_scalefsf2 (TARGET_AVX512F)
#define HAVE_avx512f_scalefdf2 (TARGET_AVX512F)
#define HAVE_sse4_1_roundhf2 ((TARGET_SSE4_1) && (TARGET_AVX512FP16))
#define HAVE_sse4_1_roundsf2 (TARGET_SSE4_1)
#define HAVE_sse4_1_rounddf2 (TARGET_SSE4_1)
#define HAVE_rintxf2 (TARGET_USE_FANCY_MATH_387)
#define HAVE_lrintxfdi2 (TARGET_USE_FANCY_MATH_387)
#define HAVE_lrintxfhi2 (TARGET_USE_FANCY_MATH_387)
#define HAVE_lrintxfsi2 (TARGET_USE_FANCY_MATH_387)
#define HAVE_frndintxf2_roundeven (TARGET_USE_FANCY_MATH_387 \
   && (flag_fp_int_builtin_inexact || !flag_trapping_math) \
   && ix86_pre_reload_split ())
#define HAVE_frndintxf2_floor (TARGET_USE_FANCY_MATH_387 \
   && (flag_fp_int_builtin_inexact || !flag_trapping_math) \
   && ix86_pre_reload_split ())
#define HAVE_frndintxf2_ceil (TARGET_USE_FANCY_MATH_387 \
   && (flag_fp_int_builtin_inexact || !flag_trapping_math) \
   && ix86_pre_reload_split ())
#define HAVE_frndintxf2_trunc (TARGET_USE_FANCY_MATH_387 \
   && (flag_fp_int_builtin_inexact || !flag_trapping_math) \
   && ix86_pre_reload_split ())
#define HAVE_frndintxf2_roundeven_i387 (TARGET_USE_FANCY_MATH_387 \
   && (flag_fp_int_builtin_inexact || !flag_trapping_math))
#define HAVE_frndintxf2_floor_i387 (TARGET_USE_FANCY_MATH_387 \
   && (flag_fp_int_builtin_inexact || !flag_trapping_math))
#define HAVE_frndintxf2_ceil_i387 (TARGET_USE_FANCY_MATH_387 \
   && (flag_fp_int_builtin_inexact || !flag_trapping_math))
#define HAVE_frndintxf2_trunc_i387 (TARGET_USE_FANCY_MATH_387 \
   && (flag_fp_int_builtin_inexact || !flag_trapping_math))
#define HAVE_fistdi2_floor (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_fistdi2_ceil (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_fisthi2_floor (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_fisthi2_ceil (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_fistsi2_floor (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_fistsi2_ceil (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_fxamsf2_i387 (TARGET_USE_FANCY_MATH_387)
#define HAVE_fxamdf2_i387 (TARGET_USE_FANCY_MATH_387)
#define HAVE_fxamxf2_i387 (TARGET_USE_FANCY_MATH_387)
#define HAVE_movmsk_df (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH)
#define HAVE_cld 1
#define HAVE_movhf_mask (TARGET_AVX512FP16)
#define HAVE_smaxsf3 (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH)
#define HAVE_sminsf3 (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH)
#define HAVE_smaxdf3 (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH)
#define HAVE_smindf3 (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH)
#define HAVE_smaxhf3 (TARGET_AVX512FP16)
#define HAVE_sminhf3 (TARGET_AVX512FP16)
#define HAVE_pro_epilogue_adjust_stack_add_si (Pmode == SImode)
#define HAVE_pro_epilogue_adjust_stack_add_di (Pmode == DImode)
#define HAVE_pro_epilogue_adjust_stack_sub_si (Pmode == SImode)
#define HAVE_pro_epilogue_adjust_stack_sub_di (Pmode == DImode)
#define HAVE_allocate_stack_worker_probe_si ((ix86_target_stack_probe ()) && (Pmode == SImode))
#define HAVE_allocate_stack_worker_probe_di ((ix86_target_stack_probe ()) && (Pmode == DImode))
#define HAVE_probe_stack_1_si (word_mode == SImode)
#define HAVE_probe_stack_1_di (word_mode == DImode)
#define HAVE_adjust_stack_and_probe_si (Pmode == SImode)
#define HAVE_adjust_stack_and_probe_di (Pmode == DImode)
#define HAVE_probe_stack_range_si (Pmode == SImode)
#define HAVE_probe_stack_range_di (Pmode == DImode)
#define HAVE_stack_protect_set_1_si_si ((ptr_mode == SImode) && (word_mode == SImode))
#define HAVE_stack_protect_set_1_di_si ((ptr_mode == DImode) && (word_mode == SImode))
#define HAVE_stack_protect_set_1_si_di ((ptr_mode == SImode) && (word_mode == DImode))
#define HAVE_stack_protect_set_1_di_di ((ptr_mode == DImode) && (word_mode == DImode))
#define HAVE_stack_protect_test_1_si (ptr_mode == SImode)
#define HAVE_stack_protect_test_1_di (ptr_mode == DImode)
#define HAVE_trap 1
#define HAVE_ud2 1
#define HAVE_sse4_2_crc32qi (TARGET_CRC32)
#define HAVE_sse4_2_crc32hi (TARGET_CRC32)
#define HAVE_sse4_2_crc32si (TARGET_CRC32)
#define HAVE_rdpmc 1
#define HAVE_rdtsc 1
#define HAVE_rdtscp 1
#define HAVE_fxsave (TARGET_FXSR)
#define HAVE_fxrstor (TARGET_FXSR)
#define HAVE_xsave (!TARGET_64BIT && TARGET_XSAVE)
#define HAVE_xsaveopt ((!TARGET_64BIT && TARGET_XSAVE) && (TARGET_XSAVEOPT))
#define HAVE_xsavec ((!TARGET_64BIT && TARGET_XSAVE) && (TARGET_XSAVEC))
#define HAVE_xsaves ((!TARGET_64BIT && TARGET_XSAVE) && (TARGET_XSAVES))
#define HAVE_xrstor (!TARGET_64BIT && TARGET_XSAVE)
#define HAVE_xrstors ((!TARGET_64BIT && TARGET_XSAVE) && (TARGET_XSAVES))
#define HAVE_xsetbv (!TARGET_64BIT && TARGET_XSAVE)
#define HAVE_xgetbv (!TARGET_64BIT && TARGET_XSAVE)
#define HAVE_fnstenv (TARGET_80387)
#define HAVE_fldenv (TARGET_80387)
#define HAVE_fnstsw (TARGET_80387)
#define HAVE_fnclex (TARGET_80387)
#define HAVE_lwp_llwpcbsi ((TARGET_LWP) && (Pmode == SImode))
#define HAVE_lwp_llwpcbdi ((TARGET_LWP) && (Pmode == DImode))
#define HAVE_lwp_slwpcbsi ((TARGET_LWP) && (Pmode == SImode))
#define HAVE_lwp_slwpcbdi ((TARGET_LWP) && (Pmode == DImode))
#define HAVE_lwp_lwpvalsi (TARGET_LWP)
#define HAVE_lwp_lwpinssi (TARGET_LWP)
#define HAVE_ptwritesi (TARGET_PTWRITE)
#define HAVE_rdrandhi (TARGET_RDRND)
#define HAVE_rdrandsi (TARGET_RDRND)
#define HAVE_rdseedhi (TARGET_RDSEED)
#define HAVE_rdseedsi (TARGET_RDSEED)
#define HAVE_rdsspsi (TARGET_SHSTK || (flag_cf_protection & CF_RETURN))
#define HAVE_incsspsi (TARGET_SHSTK || (flag_cf_protection & CF_RETURN))
#define HAVE_saveprevssp (TARGET_SHSTK)
#define HAVE_rstorssp (TARGET_SHSTK)
#define HAVE_wrsssi (TARGET_SHSTK)
#define HAVE_wrusssi (TARGET_SHSTK)
#define HAVE_setssbsy (TARGET_SHSTK)
#define HAVE_clrssbsy (TARGET_SHSTK)
#define HAVE_nop_endbr ((flag_cf_protection & CF_BRANCH))
#define HAVE_xbegin_1 (TARGET_RTM)
#define HAVE_xend (TARGET_RTM)
#define HAVE_xabort (TARGET_RTM)
#define HAVE_xtest_1 (TARGET_RTM)
#define HAVE_clwb (TARGET_CLWB)
#define HAVE_clflushopt (TARGET_CLFLUSHOPT)
#define HAVE_mwaitx (TARGET_MWAITX)
#define HAVE_monitorx_si ((TARGET_MWAITX) && (Pmode == SImode))
#define HAVE_monitorx_di ((TARGET_MWAITX) && (Pmode == DImode))
#define HAVE_clzero_si ((TARGET_CLZERO) && (Pmode == SImode))
#define HAVE_clzero_di ((TARGET_CLZERO) && (Pmode == DImode))
#define HAVE_rdpid (!TARGET_64BIT && TARGET_RDPID)
#define HAVE_wbinvd 1
#define HAVE_wbnoinvd (TARGET_WBNOINVD)
#define HAVE_movdirisi (TARGET_MOVDIRI)
#define HAVE_movdir64b_si ((TARGET_MOVDIR64B) && (Pmode == SImode))
#define HAVE_movdir64b_di ((TARGET_MOVDIR64B) && (Pmode == DImode))
#define HAVE_xsusldtrk (TARGET_TSXLDTRK)
#define HAVE_xresldtrk (TARGET_TSXLDTRK)
#define HAVE_enqcmd_si ((TARGET_ENQCMD) && (Pmode == SImode))
#define HAVE_enqcmds_si ((TARGET_ENQCMD) && (Pmode == SImode))
#define HAVE_enqcmd_di ((TARGET_ENQCMD) && (Pmode == DImode))
#define HAVE_enqcmds_di ((TARGET_ENQCMD) && (Pmode == DImode))
#define HAVE_umwait (!TARGET_64BIT && TARGET_WAITPKG)
#define HAVE_umonitor_si ((TARGET_WAITPKG) && (Pmode == SImode))
#define HAVE_umonitor_di ((TARGET_WAITPKG) && (Pmode == DImode))
#define HAVE_tpause (!TARGET_64BIT && TARGET_WAITPKG)
#define HAVE_cldemote (TARGET_CLDEMOTE)
#define HAVE_speculation_barrier 1
#define HAVE_serialize (TARGET_SERIALIZE)
#define HAVE_patchable_area 1
#define HAVE_hreset (TARGET_HRESET)
#define HAVE_ldtilecfg (TARGET_AMX_TILE)
#define HAVE_sttilecfg (TARGET_AMX_TILE)
#define HAVE_sse_movntq ((TARGET_MMX || TARGET_MMX_WITH_SSE) \
   && (TARGET_SSE || TARGET_3DNOW_A))
#define HAVE_mmx_ieee_maxv2sf3 (TARGET_3DNOW)
#define HAVE_mmx_ieee_minv2sf3 (TARGET_3DNOW)
#define HAVE_mmx_rcpv2sf2 (TARGET_3DNOW)
#define HAVE_mmx_rcpit1v2sf3 (TARGET_3DNOW)
#define HAVE_mmx_rcpit2v2sf3 (TARGET_3DNOW)
#define HAVE_mmx_rsqrtv2sf2 (TARGET_3DNOW)
#define HAVE_mmx_rsqit1v2sf3 (TARGET_3DNOW)
#define HAVE_mmx_hsubv2sf3 (TARGET_3DNOW_A)
#define HAVE_mmx_gtv2sf3 (TARGET_3DNOW)
#define HAVE_mmx_gev2sf3 (TARGET_3DNOW)
#define HAVE_mmx_fix_truncv2sfv2si2 (TARGET_3DNOW)
#define HAVE_mmx_floatv2siv2sf2 (TARGET_3DNOW)
#define HAVE_mmx_pf2iw (TARGET_3DNOW_A)
#define HAVE_mmx_pi2fw (TARGET_3DNOW_A)
#define HAVE_mmx_pswapdv2sf2 (TARGET_3DNOW_A || TARGET_MMX_WITH_SSE)
#define HAVE_andv2bf3 (TARGET_SSE)
#define HAVE_iorv2bf3 (TARGET_SSE)
#define HAVE_xorv2bf3 (TARGET_SSE)
#define HAVE_andv2hf3 (TARGET_SSE)
#define HAVE_iorv2hf3 (TARGET_SSE)
#define HAVE_xorv2hf3 (TARGET_SSE)
#define HAVE_negv2qi2 (!TARGET_PARTIAL_REG_STALL || optimize_size || TARGET_SSE2)
#define HAVE_addv4qi3 (TARGET_SSE2)
#define HAVE_subv4qi3 (TARGET_SSE2)
#define HAVE_addv2hi3 (TARGET_SSE2)
#define HAVE_subv2hi3 (TARGET_SSE2)
#define HAVE_addv2qi3 (!TARGET_PARTIAL_REG_STALL || optimize_size || TARGET_SSE2)
#define HAVE_subv2qi3 (!TARGET_PARTIAL_REG_STALL || optimize_size || TARGET_SSE2)
#define HAVE_ssaddv4qi3 (TARGET_SSE2)
#define HAVE_usaddv4qi3 (TARGET_SSE2)
#define HAVE_sssubv4qi3 (TARGET_SSE2)
#define HAVE_ussubv4qi3 (TARGET_SSE2)
#define HAVE_ssaddv2qi3 (TARGET_SSE2)
#define HAVE_usaddv2qi3 (TARGET_SSE2)
#define HAVE_sssubv2qi3 (TARGET_SSE2)
#define HAVE_ussubv2qi3 (TARGET_SSE2)
#define HAVE_ssaddv2hi3 (TARGET_SSE2)
#define HAVE_usaddv2hi3 (TARGET_SSE2)
#define HAVE_sssubv2hi3 (TARGET_SSE2)
#define HAVE_ussubv2hi3 (TARGET_SSE2)
#define HAVE_mulv2hi3 (TARGET_SSE2)
#define HAVE_smulv2hi3_highpart (TARGET_SSE2)
#define HAVE_umulv2hi3_highpart (TARGET_SSE2)
#define HAVE_smaxv4qi3 (TARGET_SSE4_1)
#define HAVE_sminv4qi3 (TARGET_SSE4_1)
#define HAVE_smaxv2qi3 (TARGET_SSE4_1)
#define HAVE_sminv2qi3 (TARGET_SSE4_1)
#define HAVE_smaxv2hi3 (TARGET_SSE2)
#define HAVE_sminv2hi3 (TARGET_SSE2)
#define HAVE_umaxv4qi3 (TARGET_SSE2)
#define HAVE_uminv4qi3 (TARGET_SSE2)
#define HAVE_umaxv2qi3 (TARGET_SSE2)
#define HAVE_uminv2qi3 (TARGET_SSE2)
#define HAVE_umaxv2hi3 (TARGET_SSE4_1)
#define HAVE_uminv2hi3 (TARGET_SSE4_1)
#define HAVE_ssse3_absv8qi2 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_ssse3_absv4hi2 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_ssse3_absv2si2 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_absv4qi2 (TARGET_SSSE3)
#define HAVE_absv2qi2 (TARGET_SSSE3)
#define HAVE_absv2hi2 (TARGET_SSSE3)
#define HAVE_mmx_ashrv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_ashrv2si3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_ashlv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_lshrv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_ashlv2si3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_lshrv2si3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_ashlv1di3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_lshrv1di3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_ashlv1si3 (TARGET_SSE2)
#define HAVE_mmx_lshrv1si3 (TARGET_SSE2)
#define HAVE_ashlv2hi3 (TARGET_SSE2)
#define HAVE_lshrv2hi3 (TARGET_SSE2)
#define HAVE_ashrv2hi3 (TARGET_SSE2)
#define HAVE_ashlv2qi3 (!TARGET_PARTIAL_REG_STALL || optimize_size)
#define HAVE_lshrv2qi3 (!TARGET_PARTIAL_REG_STALL || optimize_size)
#define HAVE_ashrv2qi3 (!TARGET_PARTIAL_REG_STALL || optimize_size)
#define HAVE_mmx_gtv8qi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_gtv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_gtv2si3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_pblendvb_v4qi (TARGET_SSE4_1)
#define HAVE_mmx_pblendvb_v2qi (TARGET_SSE4_1)
#define HAVE_mmx_pblendvb_v2hi (TARGET_SSE4_1)
#define HAVE_mmx_ppermv32 (TARGET_XOP)
#define HAVE_one_cmplv4qi2 1
#define HAVE_one_cmplv2qi2 1
#define HAVE_one_cmplv2hi2 1
#define HAVE_mmx_andnotv8qi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_andnotv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_andnotv2si3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_packsswb (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_packuswb (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_packssdw (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_punpckhbw (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_punpckhbw_low (TARGET_SSE2)
#define HAVE_mmx_punpcklbw (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_punpcklbw_low (TARGET_SSE2)
#define HAVE_mmx_punpckhwd (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_punpcklwd (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_punpckhdq (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_punpckldq (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_sse4_1_sign_extendv2qiv2hi2 (TARGET_SSE4_1)
#define HAVE_sse4_1_zero_extendv2qiv2hi2 (TARGET_SSE4_1)
#define HAVE_avx512vl_truncv2hiv2qi2 (TARGET_AVX512VL && TARGET_AVX512BW)
#define HAVE_mmx_pshufbv4qi3 (TARGET_SSSE3)
#define HAVE_mmx_pshufwv4hf_1 ((TARGET_MMX || TARGET_MMX_WITH_SSE) \
   && (TARGET_SSE || TARGET_3DNOW_A))
#define HAVE_mmx_pshufwv4bf_1 ((TARGET_MMX || TARGET_MMX_WITH_SSE) \
   && (TARGET_SSE || TARGET_3DNOW_A))
#define HAVE_mmx_pshufwv4hi_1 ((TARGET_MMX || TARGET_MMX_WITH_SSE) \
   && (TARGET_SSE || TARGET_3DNOW_A))
#define HAVE_mmx_pswapdv2si2 (TARGET_3DNOW_A)
#define HAVE_uavgv4qi3_ceil (TARGET_SSE2)
#define HAVE_uavgv2qi3_ceil (TARGET_SSE2)
#define HAVE_uavgv2hi3_ceil (TARGET_SSE2)
#define HAVE_mmx_pmovmskb ((TARGET_MMX || TARGET_MMX_WITH_SSE) \
   && (TARGET_SSE || TARGET_3DNOW_A))
#define HAVE_popcountv4qi2 (TARGET_AVX512VL && TARGET_AVX512BITALG)
#define HAVE_popcountv2qi2 (TARGET_AVX512VL && TARGET_AVX512BITALG)
#define HAVE_popcountv2hi2 (TARGET_AVX512VL && TARGET_AVX512BITALG)
#define HAVE_movv64qi_internal ((TARGET_SSE \
   && (register_operand (operands[0], V64QImode) \
       || register_operand (operands[1], V64QImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv32qi_internal ((TARGET_SSE \
   && (register_operand (operands[0], V32QImode) \
       || register_operand (operands[1], V32QImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX))
#define HAVE_movv16qi_internal (TARGET_SSE \
   && (register_operand (operands[0], V16QImode) \
       || register_operand (operands[1], V16QImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1]))
#define HAVE_movv32hi_internal ((TARGET_SSE \
   && (register_operand (operands[0], V32HImode) \
       || register_operand (operands[1], V32HImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv16hi_internal ((TARGET_SSE \
   && (register_operand (operands[0], V16HImode) \
       || register_operand (operands[1], V16HImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX))
#define HAVE_movv8hi_internal (TARGET_SSE \
   && (register_operand (operands[0], V8HImode) \
       || register_operand (operands[1], V8HImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1]))
#define HAVE_movv16si_internal ((TARGET_SSE \
   && (register_operand (operands[0], V16SImode) \
       || register_operand (operands[1], V16SImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv8si_internal ((TARGET_SSE \
   && (register_operand (operands[0], V8SImode) \
       || register_operand (operands[1], V8SImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX))
#define HAVE_movv4si_internal (TARGET_SSE \
   && (register_operand (operands[0], V4SImode) \
       || register_operand (operands[1], V4SImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1]))
#define HAVE_movv8di_internal ((TARGET_SSE \
   && (register_operand (operands[0], V8DImode) \
       || register_operand (operands[1], V8DImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv4di_internal ((TARGET_SSE \
   && (register_operand (operands[0], V4DImode) \
       || register_operand (operands[1], V4DImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX))
#define HAVE_movv2di_internal (TARGET_SSE \
   && (register_operand (operands[0], V2DImode) \
       || register_operand (operands[1], V2DImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1]))
#define HAVE_movv4ti_internal ((TARGET_SSE \
   && (register_operand (operands[0], V4TImode) \
       || register_operand (operands[1], V4TImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv2ti_internal ((TARGET_SSE \
   && (register_operand (operands[0], V2TImode) \
       || register_operand (operands[1], V2TImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX))
#define HAVE_movv1ti_internal (TARGET_SSE \
   && (register_operand (operands[0], V1TImode) \
       || register_operand (operands[1], V1TImode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1]))
#define HAVE_movv32hf_internal ((TARGET_SSE \
   && (register_operand (operands[0], V32HFmode) \
       || register_operand (operands[1], V32HFmode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv16hf_internal ((TARGET_SSE \
   && (register_operand (operands[0], V16HFmode) \
       || register_operand (operands[1], V16HFmode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX))
#define HAVE_movv8hf_internal (TARGET_SSE \
   && (register_operand (operands[0], V8HFmode) \
       || register_operand (operands[1], V8HFmode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1]))
#define HAVE_movv32bf_internal ((TARGET_SSE \
   && (register_operand (operands[0], V32BFmode) \
       || register_operand (operands[1], V32BFmode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv16bf_internal ((TARGET_SSE \
   && (register_operand (operands[0], V16BFmode) \
       || register_operand (operands[1], V16BFmode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX))
#define HAVE_movv8bf_internal (TARGET_SSE \
   && (register_operand (operands[0], V8BFmode) \
       || register_operand (operands[1], V8BFmode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1]))
#define HAVE_movv16sf_internal ((TARGET_SSE \
   && (register_operand (operands[0], V16SFmode) \
       || register_operand (operands[1], V16SFmode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv8sf_internal ((TARGET_SSE \
   && (register_operand (operands[0], V8SFmode) \
       || register_operand (operands[1], V8SFmode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX))
#define HAVE_movv4sf_internal (TARGET_SSE \
   && (register_operand (operands[0], V4SFmode) \
       || register_operand (operands[1], V4SFmode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1]))
#define HAVE_movv8df_internal ((TARGET_SSE \
   && (register_operand (operands[0], V8DFmode) \
       || register_operand (operands[1], V8DFmode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv4df_internal ((TARGET_SSE \
   && (register_operand (operands[0], V4DFmode) \
       || register_operand (operands[1], V4DFmode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1])) && (TARGET_AVX))
#define HAVE_movv2df_internal (TARGET_SSE \
   && (register_operand (operands[0], V2DFmode) \
       || register_operand (operands[1], V2DFmode)) \
   && ix86_hardreg_mov_ok (operands[0], operands[1]))
#define HAVE_avx512f_movhf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_movsf_mask (TARGET_AVX512F)
#define HAVE_avx512f_movdf_mask ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_storehf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_storesf_mask (TARGET_AVX512F)
#define HAVE_avx512f_storedf_mask (TARGET_AVX512F)
#define HAVE_avx512f_blendmv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_blendmv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_blendmv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_blendmv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_blendmv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_blendmv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_blendmv16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_blendmv8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_blendmv4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_blendmv8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_blendmv4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_blendmv2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512bw_blendmv64qi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_blendmv16qi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_blendmv32qi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_blendmv32hi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_blendmv16hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_blendmv8hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_blendmv32hf ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_blendmv16hf ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_blendmv8hf ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_blendmv32bf ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_blendmv16bf ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_blendmv8bf ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_movdi_to_sse (!TARGET_64BIT && TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC)
#define HAVE_avx_lddqu256 ((TARGET_SSE3) && (TARGET_AVX))
#define HAVE_sse3_lddqu (TARGET_SSE3)
#define HAVE_sse2_movntisi (TARGET_SSE2)
#define HAVE_avx512f_movntv16sf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_movntv8sf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_sse_movntv4sf (TARGET_SSE)
#define HAVE_avx512f_movntv8df ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_movntv4df ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_sse2_movntv2df ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_avx512f_movntv8di ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_movntv4di ((TARGET_SSE2) && (TARGET_AVX))
#define HAVE_sse2_movntv2di (TARGET_SSE2)
#define HAVE_kandqi (TARGET_AVX512F)
#define HAVE_kiorqi (TARGET_AVX512F)
#define HAVE_kxorqi (TARGET_AVX512F)
#define HAVE_kandhi (TARGET_AVX512F)
#define HAVE_kiorhi (TARGET_AVX512F)
#define HAVE_kxorhi (TARGET_AVX512F)
#define HAVE_kandsi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kiorsi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kxorsi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kanddi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kiordi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kxordi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kandnqi (TARGET_AVX512F)
#define HAVE_kandnhi (TARGET_AVX512F)
#define HAVE_kandnsi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kandndi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kxnorqi (TARGET_AVX512F)
#define HAVE_kxnorhi (TARGET_AVX512F)
#define HAVE_kxnorsi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kxnordi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_knotqi (TARGET_AVX512F)
#define HAVE_knothi (TARGET_AVX512F)
#define HAVE_knotsi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_knotdi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kaddqi ((TARGET_AVX512F) && (TARGET_AVX512DQ))
#define HAVE_kaddhi ((TARGET_AVX512F) && (TARGET_AVX512DQ))
#define HAVE_kaddsi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kadddi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kashiftqi ((TARGET_AVX512F) && (TARGET_AVX512DQ))
#define HAVE_klshiftrtqi ((TARGET_AVX512F) && (TARGET_AVX512DQ))
#define HAVE_kashifthi (TARGET_AVX512F)
#define HAVE_klshiftrthi (TARGET_AVX512F)
#define HAVE_kashiftsi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_klshiftrtsi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kashiftdi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_klshiftrtdi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_ktestqi ((TARGET_AVX512F) && (TARGET_AVX512DQ))
#define HAVE_ktesthi ((TARGET_AVX512F) && (TARGET_AVX512DQ))
#define HAVE_ktestsi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_ktestdi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kortestqi_ccc ((TARGET_AVX512F) && (TARGET_AVX512DQ))
#define HAVE_kortesthi_ccc (TARGET_AVX512F)
#define HAVE_kortestsi_ccc ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kortestdi_ccc ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kortestqi_ccz ((TARGET_AVX512F) && (TARGET_AVX512DQ))
#define HAVE_kortesthi_ccz (TARGET_AVX512F)
#define HAVE_kortestsi_ccz ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kortestdi_ccz ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kunpckhi (TARGET_AVX512F)
#define HAVE_kunpcksi (TARGET_AVX512BW)
#define HAVE_kunpckdi (TARGET_AVX512BW)
#define HAVE_avx512fp16_vmaddv8hf3 ((TARGET_SSE) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vmaddv8hf3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vmaddv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vmaddv8hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16))))
#define HAVE_avx512fp16_vmsubv8hf3 ((TARGET_SSE) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vmsubv8hf3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vmsubv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vmsubv8hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16))))
#define HAVE_sse_vmaddv4sf3 (TARGET_SSE)
#define HAVE_sse_vmaddv4sf3_round ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_vmaddv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_vmaddv4sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE)))
#define HAVE_sse_vmsubv4sf3 (TARGET_SSE)
#define HAVE_sse_vmsubv4sf3_round ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_vmsubv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_vmsubv4sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE)))
#define HAVE_sse2_vmaddv2df3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_sse2_vmaddv2df3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_vmaddv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_vmaddv2df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2))))
#define HAVE_sse2_vmsubv2df3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_sse2_vmsubv2df3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_vmsubv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_vmsubv2df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2))))
#define HAVE_avx512fp16_vmmulv8hf3 ((TARGET_SSE) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vmmulv8hf3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vmmulv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vmmulv8hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16))))
#define HAVE_avx512fp16_vmdivv8hf3 ((TARGET_SSE) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vmdivv8hf3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vmdivv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vmdivv8hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16))))
#define HAVE_sse_vmmulv4sf3 (TARGET_SSE)
#define HAVE_sse_vmmulv4sf3_round ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_vmmulv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_vmmulv4sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE)))
#define HAVE_sse_vmdivv4sf3 (TARGET_SSE)
#define HAVE_sse_vmdivv4sf3_round ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_vmdivv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_vmdivv4sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE)))
#define HAVE_sse2_vmmulv2df3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_sse2_vmmulv2df3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_vmmulv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_vmmulv2df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2))))
#define HAVE_sse2_vmdivv2df3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_sse2_vmdivv2df3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_vmdivv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_vmdivv2df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2))))
#define HAVE_avx512fp16_divv32hf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512fp16_divv32hf3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512fp16_divv32hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512fp16_divv32hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_avx512fp16_divv16hf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_divv16hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_divv8hf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_divv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_divv16sf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_divv16sf3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_divv16sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_divv16sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_avx_divv8sf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX))
#define HAVE_avx_divv8sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX)))
#define HAVE_sse_divv4sf3 (TARGET_SSE && 1 && 1)
#define HAVE_sse_divv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1))
#define HAVE_avx512f_divv8df3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_divv8df3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_divv8df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_divv8df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_avx_divv4df3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX))
#define HAVE_avx_divv4df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX)))
#define HAVE_sse2_divv2df3 ((TARGET_SSE && 1 && 1) && (TARGET_SSE2))
#define HAVE_sse2_divv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_SSE2)))
#define HAVE_avx_rcpv8sf2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_sse_rcpv4sf2 (TARGET_SSE)
#define HAVE_sse_vmrcpv4sf2 (TARGET_SSE)
#define HAVE_avx512fp16_rcpv32hf2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_rcpv32hf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_rcpv16hf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_rcpv16hf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_rcpv8hf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_rcpv8hf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vmrcpv8hf2 (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vmrcpv8hf2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_rcp14v16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_rcp14v8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_rcp14v4sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_rcp14v8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_rcp14v4df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_rcp14v2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_srcp14v4sf (TARGET_AVX512F)
#define HAVE_srcp14v2df ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_srcp14v4sf_mask (TARGET_AVX512F)
#define HAVE_srcp14v2df_mask ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512fp16_sqrtv32hf2 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512fp16_sqrtv32hf2_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512fp16_sqrtv32hf2_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512fp16_sqrtv32hf2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_avx512fp16_sqrtv16hf2 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_sqrtv16hf2_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_sqrtv8hf2 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_sqrtv8hf2_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_sqrtv16sf2 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_sqrtv16sf2_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_sqrtv16sf2_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_sqrtv16sf2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_avx_sqrtv8sf2 ((TARGET_SSE && 1 && 1) && (TARGET_AVX))
#define HAVE_avx_sqrtv8sf2_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX)))
#define HAVE_sse_sqrtv4sf2 (TARGET_SSE && 1 && 1)
#define HAVE_sse_sqrtv4sf2_mask ((TARGET_AVX512F) && (TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1))
#define HAVE_avx512f_sqrtv8df2 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_sqrtv8df2_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_sqrtv8df2_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_sqrtv8df2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_avx_sqrtv4df2 ((TARGET_SSE && 1 && 1) && (TARGET_AVX))
#define HAVE_avx_sqrtv4df2_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX)))
#define HAVE_sse2_sqrtv2df2 ((TARGET_SSE && 1 && 1) && (TARGET_SSE2))
#define HAVE_sse2_sqrtv2df2_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_SSE2)))
#define HAVE_avx512fp16_vmsqrtv8hf2 ((TARGET_SSE) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vmsqrtv8hf2_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vmsqrtv8hf2_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vmsqrtv8hf2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16))))
#define HAVE_sse_vmsqrtv4sf2 (TARGET_SSE)
#define HAVE_sse_vmsqrtv4sf2_round ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_vmsqrtv4sf2_mask ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_vmsqrtv4sf2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE)))
#define HAVE_sse2_vmsqrtv2df2 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_sse2_vmsqrtv2df2_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_vmsqrtv2df2_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_vmsqrtv2df2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2))))
#define HAVE_avx_rsqrtv8sf2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_sse_rsqrtv4sf2 (TARGET_SSE)
#define HAVE_avx512fp16_rsqrtv32hf2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_rsqrtv32hf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_rsqrtv16hf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_rsqrtv16hf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_rsqrtv8hf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_rsqrtv8hf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_rsqrt14v16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_rsqrt14v8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_rsqrt14v4sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_rsqrt14v8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_rsqrt14v4df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_rsqrt14v2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_rsqrt14v4sf (TARGET_AVX512F)
#define HAVE_rsqrt14v2df ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_rsqrt14_v4sf_mask (TARGET_AVX512F)
#define HAVE_rsqrt14_v2df_mask ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_sse_vmrsqrtv4sf2 (TARGET_SSE)
#define HAVE_avx512fp16_vmrsqrtv8hf2 (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vmrsqrtv8hf2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_ieee_maxv32hf3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_ieee_maxv32hf3_round ((TARGET_AVX512F) && ((TARGET_SSE \
   && 1 \
   && (V32HFmode == V16SFmode \
									      || V32HFmode == V8DFmode \
									      || V32HFmode == V8DImode \
									      || V32HFmode == V16SImode \
									      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_ieee_maxv32hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (64 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_ieee_maxv32hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE \
   && (64 == 64 || TARGET_AVX512VL) \
   && (V32HFmode == V16SFmode \
									      || V32HFmode == V8DFmode \
									      || V32HFmode == V8DImode \
									      || V32HFmode == V16SImode \
									      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_ieee_minv32hf3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_ieee_minv32hf3_round ((TARGET_AVX512F) && ((TARGET_SSE \
   && 1 \
   && (V32HFmode == V16SFmode \
									      || V32HFmode == V8DFmode \
									      || V32HFmode == V8DImode \
									      || V32HFmode == V16SImode \
									      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_ieee_minv32hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (64 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_ieee_minv32hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE \
   && (64 == 64 || TARGET_AVX512VL) \
   && (V32HFmode == V16SFmode \
									      || V32HFmode == V8DFmode \
									      || V32HFmode == V8DImode \
									      || V32HFmode == V16SImode \
									      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_ieee_maxv16hf3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_ieee_maxv16hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (32 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_ieee_minv16hf3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_ieee_minv16hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (32 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_ieee_maxv8hf3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_ieee_maxv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (16 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_ieee_minv8hf3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_ieee_minv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (16 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_ieee_maxv16sf3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_ieee_maxv16sf3_round ((TARGET_AVX512F) && ((TARGET_SSE \
   && 1 \
   && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_ieee_maxv16sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (64 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_ieee_maxv16sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE \
   && (64 == 64 || TARGET_AVX512VL) \
   && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_ieee_minv16sf3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_ieee_minv16sf3_round ((TARGET_AVX512F) && ((TARGET_SSE \
   && 1 \
   && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_ieee_minv16sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (64 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_ieee_minv16sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE \
   && (64 == 64 || TARGET_AVX512VL) \
   && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_ieee_maxv8sf3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_AVX))
#define HAVE_ieee_maxv8sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (32 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX)))
#define HAVE_ieee_minv8sf3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_AVX))
#define HAVE_ieee_minv8sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (32 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX)))
#define HAVE_ieee_maxv4sf3 (TARGET_SSE \
   && 1 \
   && 1)
#define HAVE_ieee_maxv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE \
   && (16 == 64 || TARGET_AVX512VL) \
   && 1))
#define HAVE_ieee_minv4sf3 (TARGET_SSE \
   && 1 \
   && 1)
#define HAVE_ieee_minv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE \
   && (16 == 64 || TARGET_AVX512VL) \
   && 1))
#define HAVE_ieee_maxv8df3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_ieee_maxv8df3_round ((TARGET_AVX512F) && ((TARGET_SSE \
   && 1 \
   && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_ieee_maxv8df3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (64 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_ieee_maxv8df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE \
   && (64 == 64 || TARGET_AVX512VL) \
   && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_ieee_minv8df3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_ieee_minv8df3_round ((TARGET_AVX512F) && ((TARGET_SSE \
   && 1 \
   && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_ieee_minv8df3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (64 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_ieee_minv8df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE \
   && (64 == 64 || TARGET_AVX512VL) \
   && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_ieee_maxv4df3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_AVX))
#define HAVE_ieee_maxv4df3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (32 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX)))
#define HAVE_ieee_minv4df3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_AVX))
#define HAVE_ieee_minv4df3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (32 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX)))
#define HAVE_ieee_maxv2df3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_SSE2))
#define HAVE_ieee_maxv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (16 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_SSE2)))
#define HAVE_ieee_minv2df3 ((TARGET_SSE \
   && 1 \
   && 1) && (TARGET_SSE2))
#define HAVE_ieee_minv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE \
   && (16 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_SSE2)))
#define HAVE_avx512fp16_ieee_vmmaxv8hf3 ((TARGET_SSE) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_ieee_vmmaxv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_ieee_vmmaxv8hf3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_ieee_vmmaxv8hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16))))
#define HAVE_avx512fp16_ieee_vmminv8hf3 ((TARGET_SSE) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_ieee_vmminv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_ieee_vmminv8hf3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_ieee_vmminv8hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16))))
#define HAVE_sse_ieee_vmmaxv4sf3 (TARGET_SSE)
#define HAVE_sse_ieee_vmmaxv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_ieee_vmmaxv4sf3_round ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_ieee_vmmaxv4sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE)))
#define HAVE_sse_ieee_vmminv4sf3 (TARGET_SSE)
#define HAVE_sse_ieee_vmminv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_ieee_vmminv4sf3_round ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_ieee_vmminv4sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE)))
#define HAVE_sse2_ieee_vmmaxv2df3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_sse2_ieee_vmmaxv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_ieee_vmmaxv2df3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_ieee_vmmaxv2df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2))))
#define HAVE_sse2_ieee_vmminv2df3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_sse2_ieee_vmminv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_ieee_vmminv2df3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_ieee_vmminv2df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2))))
#define HAVE_vec_addsubv8sf3 ((TARGET_SSE3) && (TARGET_AVX))
#define HAVE_vec_addsubv4sf3 (TARGET_SSE3)
#define HAVE_vec_addsubv4df3 ((TARGET_SSE3) && (TARGET_AVX))
#define HAVE_vec_addsubv2df3 ((TARGET_SSE3) && (TARGET_SSE2))
#define HAVE_avx_haddv4df3 (TARGET_AVX)
#define HAVE_avx_hsubv4df3 (TARGET_AVX)
#define HAVE_sse3_hsubv2df3 (TARGET_SSE3)
#define HAVE_avx_haddv8sf3 (TARGET_AVX)
#define HAVE_avx_hsubv8sf3 (TARGET_AVX)
#define HAVE_sse3_haddv4sf3 (TARGET_SSE3)
#define HAVE_sse3_hsubv4sf3 (TARGET_SSE3)
#define HAVE_reducepv32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V32HFmode))) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_reducepv32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V32HFmode))) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_reducepv16hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V16HFmode))) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_reducepv16hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V16HFmode))) && (TARGET_AVX512FP16 && TARGET_AVX512VL))))
#define HAVE_reducepv8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V8HFmode))) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_reducepv8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V8HFmode))) && (TARGET_AVX512FP16 && TARGET_AVX512VL))))
#define HAVE_reducepv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V16SFmode))) && (TARGET_EVEX512)))
#define HAVE_reducepv16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V16SFmode))) && (TARGET_EVEX512))))
#define HAVE_reducepv8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V8SFmode))) && (TARGET_AVX512VL)))
#define HAVE_reducepv8sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V8SFmode))) && (TARGET_AVX512VL))))
#define HAVE_reducepv4sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V4SFmode))) && (TARGET_AVX512VL)))
#define HAVE_reducepv4sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V4SFmode))) && (TARGET_AVX512VL))))
#define HAVE_reducepv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V8DFmode))) && (TARGET_EVEX512)))
#define HAVE_reducepv8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V8DFmode))) && (TARGET_EVEX512))))
#define HAVE_reducepv4df_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V4DFmode))) && (TARGET_AVX512VL)))
#define HAVE_reducepv4df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V4DFmode))) && (TARGET_AVX512VL))))
#define HAVE_reducepv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V2DFmode))) && (TARGET_AVX512VL)))
#define HAVE_reducepv2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V2DFmode))) && (TARGET_AVX512VL))))
#define HAVE_reducesv8hf ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V8HFmode))) && (TARGET_AVX512FP16))
#define HAVE_reducesv8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V8HFmode))) && (TARGET_AVX512FP16)))
#define HAVE_reducesv8hf_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V8HFmode))) && (TARGET_AVX512FP16)))
#define HAVE_reducesv8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V8HFmode))) && (TARGET_AVX512FP16))))
#define HAVE_reducesv4sf (TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V4SFmode)))
#define HAVE_reducesv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V4SFmode))))
#define HAVE_reducesv4sf_round ((TARGET_AVX512F) && (TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V4SFmode))))
#define HAVE_reducesv4sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V4SFmode)))))
#define HAVE_reducesv2df ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V2DFmode))) && (TARGET_SSE2))
#define HAVE_reducesv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V2DFmode))) && (TARGET_SSE2)))
#define HAVE_reducesv2df_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V2DFmode))) && (TARGET_SSE2)))
#define HAVE_reducesv2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ || (VALID_AVX512FP16_REG_MODE (V2DFmode))) && (TARGET_SSE2))))
#define HAVE_avx_cmpv8sf3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_avx_cmpv4sf3 (TARGET_SSE)
#define HAVE_avx_cmpv4df3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_avx_cmpv2df3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_avx_vmcmpv4sf3 (TARGET_SSE)
#define HAVE_avx_vmcmpv2df3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_avx_maskcmpv8sf3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_sse_maskcmpv4sf3 (TARGET_SSE)
#define HAVE_avx_maskcmpv4df3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_sse2_maskcmpv2df3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_sse_vmmaskcmpv4sf3 (TARGET_SSE)
#define HAVE_sse2_vmmaskcmpv2df3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_avx512f_cmpv16si3 ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_cmpv16si3_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V16SImode == V16SFmode \
									      || V16SImode == V8DFmode \
									      || V16SImode == V8DImode \
									      || V16SImode == V16SImode \
									      || V16SImode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512f_cmpv16si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_EVEX512)))
#define HAVE_avx512f_cmpv16si3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F && (V16SImode == V16SFmode \
									      || V16SImode == V8DFmode \
									      || V16SImode == V8DImode \
									      || V16SImode == V16SImode \
									      || V16SImode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_avx512vl_cmpv8si3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cmpv8si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_cmpv4si3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cmpv4si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512f_cmpv8di3 ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_cmpv8di3_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V8DImode == V16SFmode \
									      || V8DImode == V8DFmode \
									      || V8DImode == V8DImode \
									      || V8DImode == V16SImode \
									      || V8DImode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512f_cmpv8di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_EVEX512)))
#define HAVE_avx512f_cmpv8di3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F && (V8DImode == V16SFmode \
									      || V8DImode == V8DFmode \
									      || V8DImode == V8DImode \
									      || V8DImode == V16SImode \
									      || V8DImode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_avx512vl_cmpv4di3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cmpv4di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_cmpv2di3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cmpv2di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_cmpv32hf3 ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_cmpv32hf3_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V32HFmode == V16SFmode \
									      || V32HFmode == V8DFmode \
									      || V32HFmode == V8DImode \
									      || V32HFmode == V16SImode \
									      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512bw_cmpv32hf3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512bw_cmpv32hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F && (V32HFmode == V16SFmode \
									      || V32HFmode == V8DFmode \
									      || V32HFmode == V8DImode \
									      || V32HFmode == V16SImode \
									      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_avx512vl_cmpv16hf3 ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_cmpv16hf3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_cmpv8hf3 ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_cmpv8hf3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_cmpv16sf3 ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_cmpv16sf3_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512f_cmpv16sf3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_EVEX512)))
#define HAVE_avx512f_cmpv16sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_avx512vl_cmpv8sf3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cmpv8sf3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_cmpv4sf3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cmpv4sf3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512f_cmpv8df3 ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_cmpv8df3_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512f_cmpv8df3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_EVEX512)))
#define HAVE_avx512f_cmpv8df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_avx512vl_cmpv4df3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cmpv4df3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_cmpv2df3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cmpv2df3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_cmpv64qi3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_cmpv64qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_cmpv16qi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cmpv16qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_cmpv32qi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cmpv32qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_cmpv32hi3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_cmpv32hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_cmpv16hi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cmpv16hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_cmpv8hi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cmpv8hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_ucmpv64qi3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_ucmpv64qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_ucmpv16qi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_ucmpv16qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_ucmpv32qi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_ucmpv32qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_ucmpv32hi3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_ucmpv32hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_ucmpv16hi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_ucmpv16hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_ucmpv8hi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_ucmpv8hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512f_ucmpv16si3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_ucmpv16si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_ucmpv8si3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_ucmpv8si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_ucmpv4si3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_ucmpv4si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_ucmpv8di3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_ucmpv8di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_ucmpv4di3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_ucmpv4di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_ucmpv2di3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_ucmpv2di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_vmcmpv8hf3 ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_vmcmpv8hf3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vmcmpv4sf3 (TARGET_AVX512F)
#define HAVE_avx512f_vmcmpv4sf3_round (TARGET_AVX512F)
#define HAVE_avx512f_vmcmpv2df3 ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_vmcmpv2df3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_vmcmpv8hf3_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_vmcmpv8hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vmcmpv4sf3_mask (TARGET_AVX512F)
#define HAVE_avx512f_vmcmpv4sf3_mask_round (TARGET_AVX512F)
#define HAVE_avx512f_vmcmpv2df3_mask ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_vmcmpv2df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx10_2_comxhf ((TARGET_AVX10_2) && (TARGET_AVX512FP16))
#define HAVE_avx10_2_comxhf_round ((TARGET_AVX512F) && ((TARGET_AVX10_2) && (TARGET_AVX512FP16)))
#define HAVE_avx10_2_ucomxhf ((TARGET_AVX10_2) && (TARGET_AVX512FP16))
#define HAVE_avx10_2_ucomxhf_round ((TARGET_AVX512F) && ((TARGET_AVX10_2) && (TARGET_AVX512FP16)))
#define HAVE_avx10_2_comxsf (TARGET_AVX10_2)
#define HAVE_avx10_2_comxsf_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_ucomxsf (TARGET_AVX10_2)
#define HAVE_avx10_2_ucomxsf_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_comxdf (TARGET_AVX10_2)
#define HAVE_avx10_2_comxdf_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_ucomxdf (TARGET_AVX10_2)
#define HAVE_avx10_2_ucomxdf_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx512fp16_comi ((SSE_FLOAT_MODE_P (HFmode) || HFmode == E_HFmode) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_comi_round ((TARGET_AVX512F) && ((SSE_FLOAT_MODE_P (HFmode) || HFmode == E_HFmode) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_ucomi ((SSE_FLOAT_MODE_P (HFmode) || HFmode == E_HFmode) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_ucomi_round ((TARGET_AVX512F) && ((SSE_FLOAT_MODE_P (HFmode) || HFmode == E_HFmode) && (TARGET_AVX512FP16)))
#define HAVE_sse_comi (SSE_FLOAT_MODE_P (SFmode) || SFmode == E_HFmode)
#define HAVE_sse_comi_round ((TARGET_AVX512F) && (SSE_FLOAT_MODE_P (SFmode) || SFmode == E_HFmode))
#define HAVE_sse_ucomi (SSE_FLOAT_MODE_P (SFmode) || SFmode == E_HFmode)
#define HAVE_sse_ucomi_round ((TARGET_AVX512F) && (SSE_FLOAT_MODE_P (SFmode) || SFmode == E_HFmode))
#define HAVE_sse2_comi (SSE_FLOAT_MODE_P (DFmode) || DFmode == E_HFmode)
#define HAVE_sse2_comi_round ((TARGET_AVX512F) && (SSE_FLOAT_MODE_P (DFmode) || DFmode == E_HFmode))
#define HAVE_sse2_ucomi (SSE_FLOAT_MODE_P (DFmode) || DFmode == E_HFmode)
#define HAVE_sse2_ucomi_round ((TARGET_AVX512F) && (SSE_FLOAT_MODE_P (DFmode) || DFmode == E_HFmode))
#define HAVE_avx10_2_comisbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx512bf16_andnotv16bf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_AVX))
#define HAVE_avx512bf16_andnotv8bf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_SSE2))
#define HAVE_avx512fp16_andnotv16hf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_AVX))
#define HAVE_avx512fp16_andnotv8hf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_SSE2))
#define HAVE_avx_andnotv8sf3 ((TARGET_SSE && 1 \
   && (!false || 32 != 16)) && (TARGET_AVX))
#define HAVE_avx_andnotv8sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && TARGET_AVX512VL \
   && (!true || 32 != 16)) && (TARGET_AVX)))
#define HAVE_sse_andnotv4sf3 (TARGET_SSE && 1 \
   && (!false || 32 != 16))
#define HAVE_sse_andnotv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE && TARGET_AVX512VL \
   && (!true || 32 != 16)))
#define HAVE_avx_andnotv4df3 ((TARGET_SSE && 1 \
   && (!false || 64 != 16)) && (TARGET_AVX))
#define HAVE_avx_andnotv4df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && TARGET_AVX512VL \
   && (!true || 64 != 16)) && (TARGET_AVX)))
#define HAVE_sse2_andnotv2df3 ((TARGET_SSE && 1 \
   && (!false || 64 != 16)) && (TARGET_SSE2))
#define HAVE_sse2_andnotv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && TARGET_AVX512VL \
   && (!true || 64 != 16)) && (TARGET_SSE2)))
#define HAVE_avx512bf16_andnotv32bf3 ((TARGET_AVX512F && (!false || 16 != 16)) && (TARGET_EVEX512))
#define HAVE_avx512fp16_andnotv32hf3 ((TARGET_AVX512F && (!false || 16 != 16)) && (TARGET_EVEX512))
#define HAVE_avx512f_andnotv16sf3 ((TARGET_AVX512F && (!false || 32 != 16)) && (TARGET_EVEX512))
#define HAVE_avx512f_andnotv16sf3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && (!true || 32 != 16)) && (TARGET_EVEX512)))
#define HAVE_avx512f_andnotv8df3 ((TARGET_AVX512F && (!false || 64 != 16)) && (TARGET_EVEX512))
#define HAVE_avx512f_andnotv8df3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && (!true || 64 != 16)) && (TARGET_EVEX512)))
#define HAVE_andbf3 (TARGET_SSE)
#define HAVE_iorbf3 (TARGET_SSE)
#define HAVE_xorbf3 (TARGET_SSE)
#define HAVE_andhf3 (TARGET_SSE)
#define HAVE_iorhf3 (TARGET_SSE)
#define HAVE_xorhf3 (TARGET_SSE)
#define HAVE_andsf3 (TARGET_SSE)
#define HAVE_iorsf3 (TARGET_SSE)
#define HAVE_xorsf3 (TARGET_SSE)
#define HAVE_anddf3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_iordf3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_xordf3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_fma_fmadd_v32hf_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_fma_fmadd_v32hf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_fma_fmadd_v16hf_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_fma_fmadd_v8hf_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_fma_fmadd_v16sf_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_EVEX512))
#define HAVE_fma_fmadd_v16sf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fmadd_v8sf_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmadd_v4sf_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmadd_v8df_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_EVEX512))
#define HAVE_fma_fmadd_v8df_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fmadd_v4df_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmadd_v2df_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_avx512bw_fmadd_v32hf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fmadd_v32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fmadd_v16hf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fmadd_v8hf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512f_fmadd_v16sf_mask ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fmadd_v16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmadd_v8sf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmadd_v4sf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512f_fmadd_v8df_mask ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fmadd_v8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmadd_v4df_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmadd_v2df_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512bw_fmadd_v32hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fmadd_v32hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fmadd_v16hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_fmadd_v16hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_fmadd_v8hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fmadd_v8hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_fmadd_v16sf_mask3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmadd_v16sf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmadd_v8sf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmadd_v8sf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmadd_v4sf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmadd_v4sf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fmadd_v8df_mask3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmadd_v8df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmadd_v4df_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmadd_v4df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmadd_v2df_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmadd_v2df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_fma_fmsub_v32hf_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_fma_fmsub_v32hf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_fma_fmsub_v16hf_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_fma_fmsub_v8hf_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_fma_fmsub_v16sf_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_EVEX512))
#define HAVE_fma_fmsub_v16sf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fmsub_v8sf_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmsub_v4sf_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmsub_v8df_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_EVEX512))
#define HAVE_fma_fmsub_v8df_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fmsub_v4df_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmsub_v2df_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_avx512bw_fmsub_v32hf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fmsub_v32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fmsub_v16hf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_fmsub_v16hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_fmsub_v8hf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fmsub_v8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_fmsub_v16sf_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmsub_v16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmsub_v8sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsub_v8sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmsub_v4sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsub_v4sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fmsub_v8df_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmsub_v8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmsub_v4df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsub_v4df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmsub_v2df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsub_v2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_fmsub_v32hf_mask3 ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fmsub_v32hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fmsub_v16hf_mask3 ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fmsub_v8hf_mask3 ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512f_fmsub_v16sf_mask3 ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fmsub_v16sf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmsub_v8sf_mask3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsub_v4sf_mask3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512f_fmsub_v8df_mask3 ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fmsub_v8df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmsub_v4df_mask3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsub_v2df_mask3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fnmadd_v32hf_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_fma_fnmadd_v32hf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_fma_fnmadd_v16hf_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_fma_fnmadd_v8hf_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_fma_fnmadd_v16sf_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_EVEX512))
#define HAVE_fma_fnmadd_v16sf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fnmadd_v8sf_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fnmadd_v4sf_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fnmadd_v8df_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_EVEX512))
#define HAVE_fma_fnmadd_v8df_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fnmadd_v4df_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fnmadd_v2df_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_avx512bw_fnmadd_v32hf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fnmadd_v32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fnmadd_v16hf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fnmadd_v8hf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512f_fnmadd_v16sf_mask ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fnmadd_v16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fnmadd_v8sf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmadd_v4sf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512f_fnmadd_v8df_mask ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fnmadd_v8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fnmadd_v4df_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmadd_v2df_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512bw_fnmadd_v32hf_mask3 ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fnmadd_v32hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fnmadd_v16hf_mask3 ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fnmadd_v8hf_mask3 ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512f_fnmadd_v16sf_mask3 ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fnmadd_v16sf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fnmadd_v8sf_mask3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmadd_v4sf_mask3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512f_fnmadd_v8df_mask3 ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fnmadd_v8df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fnmadd_v4df_mask3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmadd_v2df_mask3 ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fnmsub_v32hf_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_fma_fnmsub_v32hf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_fma_fnmsub_v16hf_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_fma_fnmsub_v8hf_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_fma_fnmsub_v16sf_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_EVEX512))
#define HAVE_fma_fnmsub_v16sf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fnmsub_v8sf_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fnmsub_v4sf_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fnmsub_v8df_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_EVEX512))
#define HAVE_fma_fnmsub_v8df_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fnmsub_v4df_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fnmsub_v2df_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_avx512bw_fnmsub_v32hf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fnmsub_v32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fnmsub_v16hf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fnmsub_v8hf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512f_fnmsub_v16sf_mask ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fnmsub_v16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fnmsub_v8sf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmsub_v4sf_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512f_fnmsub_v8df_mask ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fnmsub_v8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fnmsub_v4df_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmsub_v2df_mask ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512bw_fnmsub_v32hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fnmsub_v32hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fnmsub_v16hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_fnmsub_v16hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_fnmsub_v8hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fnmsub_v8hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_fnmsub_v16sf_mask3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fnmsub_v16sf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fnmsub_v8sf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmsub_v8sf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fnmsub_v4sf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmsub_v4sf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fnmsub_v8df_mask3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fnmsub_v8df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fnmsub_v4df_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmsub_v4df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fnmsub_v2df_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmsub_v2df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_fma_fmaddsub_v32hf_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_fma_fmaddsub_v32hf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_fma_fmaddsub_v16hf_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_fma_fmaddsub_v8hf_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_fma_fmaddsub_v16sf_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_EVEX512))
#define HAVE_fma_fmaddsub_v16sf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fmaddsub_v8sf_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmaddsub_v4sf_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmaddsub_v8df_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_EVEX512))
#define HAVE_fma_fmaddsub_v8df_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fmaddsub_v4df_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmaddsub_v2df_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_avx512bw_fmaddsub_v32hf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fmaddsub_v32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fmaddsub_v16hf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v16hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_fmaddsub_v8hf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fmaddsub_v8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_fmaddsub_v16sf_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmaddsub_v16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmaddsub_v8sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v8sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmaddsub_v4sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v4sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fmaddsub_v8df_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmaddsub_v8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmaddsub_v4df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v4df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmaddsub_v2df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_fmaddsub_v32hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fmaddsub_v32hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fmaddsub_v16hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v16hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_fmaddsub_v8hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fmaddsub_v8hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_fmaddsub_v16sf_mask3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmaddsub_v16sf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmaddsub_v8sf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v8sf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmaddsub_v4sf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v4sf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fmaddsub_v8df_mask3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmaddsub_v8df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmaddsub_v4df_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v4df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmaddsub_v2df_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v2df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_fma_fmsubadd_v32hf_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_fma_fmsubadd_v32hf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_fma_fmsubadd_v16hf_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_fma_fmsubadd_v8hf_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_fma_fmsubadd_v16sf_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_EVEX512))
#define HAVE_fma_fmsubadd_v16sf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fmsubadd_v8sf_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmsubadd_v4sf_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmsubadd_v8df_maskz_1 ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_EVEX512))
#define HAVE_fma_fmsubadd_v8df_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fmsubadd_v4df_maskz_1 ((TARGET_AVX512F && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmsubadd_v2df_maskz_1 ((TARGET_AVX512F && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_avx512bw_fmsubadd_v32hf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fmsubadd_v32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fmsubadd_v16hf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v16hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_fmsubadd_v8hf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fmsubadd_v8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_fmsubadd_v16sf_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmsubadd_v16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmsubadd_v8sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v8sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmsubadd_v4sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v4sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fmsubadd_v8df_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmsubadd_v8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmsubadd_v4df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v4df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmsubadd_v2df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_fmsubadd_v32hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fmsubadd_v32hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fmsubadd_v16hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v16hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_fmsubadd_v8hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fmsubadd_v8hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_fmsubadd_v16sf_mask3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmsubadd_v16sf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmsubadd_v8sf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v8sf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmsubadd_v4sf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v4sf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fmsubadd_v8df_mask3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmsubadd_v8df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmsubadd_v4df_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v4df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmsubadd_v2df_mask3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v2df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_vmfmadd_v8hf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_vmfmadd_v8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vmfmadd_v4sf_mask (TARGET_AVX512F)
#define HAVE_avx512f_vmfmadd_v4sf_mask_round (TARGET_AVX512F)
#define HAVE_avx512f_vmfmadd_v2df_mask ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_vmfmadd_v2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_vmfmadd_v8hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_vmfmadd_v8hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vmfmadd_v4sf_mask3 (TARGET_AVX512F)
#define HAVE_avx512f_vmfmadd_v4sf_mask3_round (TARGET_AVX512F)
#define HAVE_avx512f_vmfmadd_v2df_mask3 ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_vmfmadd_v2df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_vmfmadd_v8hf_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_vmfmadd_v8hf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vmfmadd_v4sf_maskz_1 (TARGET_AVX512F)
#define HAVE_avx512f_vmfmadd_v4sf_maskz_1_round (TARGET_AVX512F)
#define HAVE_avx512f_vmfmadd_v2df_maskz_1 ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_vmfmadd_v2df_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_vmfmsub_v8hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_vmfmsub_v8hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vmfmsub_v4sf_mask3 (TARGET_AVX512F)
#define HAVE_avx512f_vmfmsub_v4sf_mask3_round (TARGET_AVX512F)
#define HAVE_avx512f_vmfmsub_v2df_mask3 ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_vmfmsub_v2df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_vmfnmadd_v8hf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_vmfnmadd_v8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vmfnmadd_v4sf_mask (TARGET_AVX512F)
#define HAVE_avx512f_vmfnmadd_v4sf_mask_round (TARGET_AVX512F)
#define HAVE_avx512f_vmfnmadd_v2df_mask ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_vmfnmadd_v2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_vmfnmadd_v8hf_mask3 ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_vmfnmadd_v8hf_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vmfnmadd_v4sf_mask3 (TARGET_AVX512F)
#define HAVE_avx512f_vmfnmadd_v4sf_mask3_round (TARGET_AVX512F)
#define HAVE_avx512f_vmfnmadd_v2df_mask3 ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_vmfnmadd_v2df_mask3_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_vmfnmadd_v8hf_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_vmfnmadd_v8hf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vmfnmadd_v4sf_maskz_1 (TARGET_AVX512F)
#define HAVE_avx512f_vmfnmadd_v4sf_maskz_1_round (TARGET_AVX512F)
#define HAVE_avx512f_vmfnmadd_v2df_maskz_1 ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_vmfnmadd_v2df_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_fma_fmaddc_v32hf ((TARGET_AVX512FP16 && 1 && 1) && (TARGET_EVEX512))
#define HAVE_fma_fmaddc_v32hf_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fmaddc_v32hf_maskz_1 ((TARGET_AVX512FP16 && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_EVEX512))
#define HAVE_fma_fmaddc_v32hf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && (64 == 64 || TARGET_AVX512VL) && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fcmaddc_v32hf ((TARGET_AVX512FP16 && 1 && 1) && (TARGET_EVEX512))
#define HAVE_fma_fcmaddc_v32hf_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fcmaddc_v32hf_maskz_1 ((TARGET_AVX512FP16 && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_EVEX512))
#define HAVE_fma_fcmaddc_v32hf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && (64 == 64 || TARGET_AVX512VL) && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fma_fmaddc_v16hf ((TARGET_AVX512FP16 && 1 && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmaddc_v16hf_maskz_1 ((TARGET_AVX512FP16 && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fcmaddc_v16hf ((TARGET_AVX512FP16 && 1 && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fcmaddc_v16hf_maskz_1 ((TARGET_AVX512FP16 && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmaddc_v8hf ((TARGET_AVX512FP16 && 1 && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fmaddc_v8hf_maskz_1 ((TARGET_AVX512FP16 && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fcmaddc_v8hf ((TARGET_AVX512FP16 && 1 && 1) && (TARGET_AVX512VL))
#define HAVE_fma_fcmaddc_v8hf_maskz_1 ((TARGET_AVX512FP16 && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512VL))
#define HAVE_fma_v32hf_fadd_fmul ((TARGET_AVX512FP16 && flag_unsafe_math_optimizations \
  && ix86_pre_reload_split ()) && (TARGET_EVEX512))
#define HAVE_fma_v16hf_fadd_fmul ((TARGET_AVX512FP16 && flag_unsafe_math_optimizations \
  && ix86_pre_reload_split ()) && (TARGET_AVX512VL))
#define HAVE_fma_v8hf_fadd_fmul ((TARGET_AVX512FP16 && flag_unsafe_math_optimizations \
  && ix86_pre_reload_split ()) && (TARGET_AVX512VL))
#define HAVE_fma_v32hf_fadd_fcmul ((TARGET_AVX512FP16 && flag_unsafe_math_optimizations \
  && ix86_pre_reload_split ()) && (TARGET_EVEX512))
#define HAVE_fma_v16hf_fadd_fcmul ((TARGET_AVX512FP16 && flag_unsafe_math_optimizations \
  && ix86_pre_reload_split ()) && (TARGET_AVX512VL))
#define HAVE_fma_v8hf_fadd_fcmul ((TARGET_AVX512FP16 && flag_unsafe_math_optimizations \
  && ix86_pre_reload_split ()) && (TARGET_AVX512VL))
#define HAVE_fma_fmaddc_v32hf_fma_zero ((TARGET_AVX512FP16 && flag_unsafe_math_optimizations \
  && ix86_pre_reload_split ()) && (TARGET_EVEX512))
#define HAVE_fma_fcmaddc_v32hf_fma_zero ((TARGET_AVX512FP16 && flag_unsafe_math_optimizations \
  && ix86_pre_reload_split ()) && (TARGET_EVEX512))
#define HAVE_fma_fmaddc_v16hf_fma_zero ((TARGET_AVX512FP16 && flag_unsafe_math_optimizations \
  && ix86_pre_reload_split ()) && (TARGET_AVX512VL))
#define HAVE_fma_fcmaddc_v16hf_fma_zero ((TARGET_AVX512FP16 && flag_unsafe_math_optimizations \
  && ix86_pre_reload_split ()) && (TARGET_AVX512VL))
#define HAVE_fma_fmaddc_v8hf_fma_zero ((TARGET_AVX512FP16 && flag_unsafe_math_optimizations \
  && ix86_pre_reload_split ()) && (TARGET_AVX512VL))
#define HAVE_fma_fcmaddc_v8hf_fma_zero ((TARGET_AVX512FP16 && flag_unsafe_math_optimizations \
  && ix86_pre_reload_split ()) && (TARGET_AVX512VL))
#define HAVE_fma_fmaddc_v16sf_pair ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_fma_fcmaddc_v16sf_pair ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_fma_fmaddc_v8sf_pair ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_fma_fcmaddc_v8sf_pair ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_fma_fmaddc_v4sf_pair ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_fma_fcmaddc_v4sf_pair ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_fma_v32hf_fmaddc_bcst ((TARGET_AVX512FP16 && ix86_pre_reload_split ()) && (TARGET_EVEX512))
#define HAVE_fma_v16hf_fmaddc_bcst ((TARGET_AVX512FP16 && ix86_pre_reload_split ()) && (TARGET_AVX512VL))
#define HAVE_fma_v8hf_fmaddc_bcst ((TARGET_AVX512FP16 && ix86_pre_reload_split ()) && (TARGET_AVX512VL))
#define HAVE_fma_v32hf_fcmaddc_bcst ((TARGET_AVX512FP16 && ix86_pre_reload_split ()) && (TARGET_EVEX512))
#define HAVE_fma_v16hf_fcmaddc_bcst ((TARGET_AVX512FP16 && ix86_pre_reload_split ()) && (TARGET_AVX512VL))
#define HAVE_fma_v8hf_fcmaddc_bcst ((TARGET_AVX512FP16 && ix86_pre_reload_split ()) && (TARGET_AVX512VL))
#define HAVE_avx512bw_fmaddc_v32hf_mask ((TARGET_AVX512FP16 && 1) && (TARGET_EVEX512))
#define HAVE_avx512bw_fmaddc_v32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512bw_fcmaddc_v32hf_mask ((TARGET_AVX512FP16 && 1) && (TARGET_EVEX512))
#define HAVE_avx512bw_fcmaddc_v32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmaddc_v16hf_mask ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fcmaddc_v16hf_mask ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fmaddc_v8hf_mask ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fcmaddc_v8hf_mask ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512bw_fmulc_v32hf ((TARGET_AVX512FP16 && 1) && (TARGET_EVEX512))
#define HAVE_avx512bw_fmulc_v32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_EVEX512)))
#define HAVE_avx512bw_fmulc_v32hf_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512bw_fmulc_v32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_avx512bw_fcmulc_v32hf ((TARGET_AVX512FP16 && 1) && (TARGET_EVEX512))
#define HAVE_avx512bw_fcmulc_v32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_EVEX512)))
#define HAVE_avx512bw_fcmulc_v32hf_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512bw_fcmulc_v32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_avx512vl_fmulc_v16hf ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmulc_v16hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fcmulc_v16hf ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fcmulc_v16hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fmulc_v8hf ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fmulc_v8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fcmulc_v8hf ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fcmulc_v8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fma_fmaddcsh_v8hf (TARGET_AVX512FP16)
#define HAVE_avx512fp16_fma_fmaddcsh_v8hf_maskz ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_fma_fmaddcsh_v8hf_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_fma_fmaddcsh_v8hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_fma_fcmaddcsh_v8hf (TARGET_AVX512FP16)
#define HAVE_avx512fp16_fma_fcmaddcsh_v8hf_maskz ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_fma_fcmaddcsh_v8hf_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_fma_fcmaddcsh_v8hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_fmaddcsh_v8hf_mask (TARGET_AVX512FP16)
#define HAVE_avx512fp16_fmaddcsh_v8hf_mask_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_fcmaddcsh_v8hf_mask (TARGET_AVX512FP16)
#define HAVE_avx512fp16_fcmaddcsh_v8hf_mask_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_fmulcsh_v8hf (TARGET_AVX512FP16)
#define HAVE_avx512fp16_fmulcsh_v8hf_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_fmulcsh_v8hf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_fmulcsh_v8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_fcmulcsh_v8hf (TARGET_AVX512FP16)
#define HAVE_avx512fp16_fcmulcsh_v8hf_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_fcmulcsh_v8hf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_fcmulcsh_v8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vcvtph2uw_v32hi ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_vcvtph2uw_v32hi_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtph2uw_v32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtph2uw_v32hi_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_vcvtph2w_v32hi ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_vcvtph2w_v32hi_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtph2w_v32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtph2w_v32hi_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_vcvtph2udq_v16si ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_vcvtph2udq_v16si_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtph2udq_v16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtph2udq_v16si_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_vcvtph2dq_v16si ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_vcvtph2dq_v16si_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtph2dq_v16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtph2dq_v16si_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_vcvtph2uqq_v8di ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_vcvtph2uqq_v8di_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtph2uqq_v8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtph2uqq_v8di_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_vcvtph2qq_v8di ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_vcvtph2qq_v8di_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtph2qq_v8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtph2qq_v8di_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_vcvtph2uw_v16hi ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtph2uw_v16hi_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2uw_v16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2uw_v16hi_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtph2w_v16hi ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtph2w_v16hi_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2w_v16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2w_v16hi_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtph2udq_v8si ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtph2udq_v8si_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2udq_v8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2udq_v8si_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtph2dq_v8si ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtph2dq_v8si_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2dq_v8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2dq_v8si_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtph2uqq_v4di ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtph2uqq_v4di_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2uqq_v4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2uqq_v4di_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtph2qq_v4di ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtph2qq_v4di_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2qq_v4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2qq_v4di_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtph2uw_v8hi ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtph2uw_v8hi_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2uw_v8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2uw_v8hi_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtph2w_v8hi ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtph2w_v8hi_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2w_v8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2w_v8hi_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtph2udq_v4si ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtph2udq_v4si_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2udq_v4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2udq_v4si_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtph2dq_v4si ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtph2dq_v4si_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2dq_v4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2dq_v4si_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtph2uqq_v2di ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtph2uqq_v2di_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2uqq_v2di_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2uqq_v2di_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtph2qq_v2di ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtph2qq_v2di_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2qq_v2di_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtph2qq_v2di_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtw2ph_v8hi ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtw2ph_v8hi_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtw2ph_v8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtw2ph_v8hi_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtuw2ph_v8hi ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtuw2ph_v8hi_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtuw2ph_v8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtuw2ph_v8hi_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtw2ph_v16hi ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtw2ph_v16hi_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtw2ph_v16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtw2ph_v16hi_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtuw2ph_v16hi ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtuw2ph_v16hi_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtuw2ph_v16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtuw2ph_v16hi_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtw2ph_v32hi ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_vcvtw2ph_v32hi_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtw2ph_v32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtw2ph_v32hi_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_vcvtuw2ph_v32hi ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_vcvtuw2ph_v32hi_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtuw2ph_v32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtuw2ph_v32hi_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_vcvtdq2ph_v8si ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtdq2ph_v8si_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtdq2ph_v8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtdq2ph_v8si_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtudq2ph_v8si ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtudq2ph_v8si_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtudq2ph_v8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtudq2ph_v8si_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtdq2ph_v16si ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_vcvtdq2ph_v16si_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtdq2ph_v16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtdq2ph_v16si_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_vcvtudq2ph_v16si ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_vcvtudq2ph_v16si_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtudq2ph_v16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtudq2ph_v16si_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_vcvtqq2ph_v8di ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_vcvtqq2ph_v8di_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtqq2ph_v8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtqq2ph_v8di_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_vcvtuqq2ph_v8di ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_vcvtuqq2ph_v8di_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtuqq2ph_v8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtuqq2ph_v8di_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_vcvtsh2usi (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vcvtsh2usi_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vcvtsh2si (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vcvtsh2si_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vcvtsh2usi_2 (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vcvtsh2si_2 (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vcvtsi2sh (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vcvtsi2sh_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vcvtusi2sh (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vcvtusi2sh_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_unspec_avx512fp16_fix_truncv8hi2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_unspec_avx512fp16_fix_truncv8hi2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_unspec_avx512fp16_fix_truncv8hi2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_unspec_avx512fp16_fix_truncv8hi2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_unspec_avx512fp16_fixuns_truncv8hi2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_unspec_avx512fp16_fixuns_truncv8hi2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_unspec_avx512fp16_fixuns_truncv8hi2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_unspec_avx512fp16_fixuns_truncv8hi2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_unspec_avx512fp16_fix_truncv16hi2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_unspec_avx512fp16_fix_truncv16hi2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_unspec_avx512fp16_fix_truncv16hi2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_unspec_avx512fp16_fix_truncv16hi2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_unspec_avx512fp16_fixuns_truncv16hi2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_unspec_avx512fp16_fixuns_truncv16hi2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_unspec_avx512fp16_fixuns_truncv16hi2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_unspec_avx512fp16_fixuns_truncv16hi2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_unspec_avx512fp16_fix_truncv32hi2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_unspec_avx512fp16_fix_truncv32hi2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_unspec_avx512fp16_fix_truncv32hi2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_unspec_avx512fp16_fix_truncv32hi2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_unspec_avx512fp16_fixuns_truncv32hi2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_unspec_avx512fp16_fixuns_truncv32hi2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_unspec_avx512fp16_fixuns_truncv32hi2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_unspec_avx512fp16_fixuns_truncv32hi2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_unspec_avx512fp16_fix_truncv8si2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_unspec_avx512fp16_fix_truncv8si2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_unspec_avx512fp16_fix_truncv8si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_unspec_avx512fp16_fix_truncv8si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_unspec_avx512fp16_fixuns_truncv8si2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_unspec_avx512fp16_fixuns_truncv8si2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_unspec_avx512fp16_fixuns_truncv8si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_unspec_avx512fp16_fixuns_truncv8si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_unspec_avx512fp16_fix_truncv16si2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_unspec_avx512fp16_fix_truncv16si2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_unspec_avx512fp16_fix_truncv16si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_unspec_avx512fp16_fix_truncv16si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_unspec_avx512fp16_fixuns_truncv16si2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_unspec_avx512fp16_fixuns_truncv16si2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_unspec_avx512fp16_fixuns_truncv16si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_unspec_avx512fp16_fixuns_truncv16si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_unspec_avx512fp16_fix_truncv8di2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_unspec_avx512fp16_fix_truncv8di2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_unspec_avx512fp16_fix_truncv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_unspec_avx512fp16_fix_truncv8di2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_unspec_avx512fp16_fixuns_truncv8di2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_unspec_avx512fp16_fixuns_truncv8di2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_unspec_avx512fp16_fixuns_truncv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_unspec_avx512fp16_fixuns_truncv8di2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_fix_truncv8hi2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fix_truncv8hi2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fix_truncv8hi2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fix_truncv8hi2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_fixuns_truncv8hi2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fixuns_truncv8hi2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fixuns_truncv8hi2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fixuns_truncv8hi2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_fix_truncv16hi2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fix_truncv16hi2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fix_truncv16hi2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fix_truncv16hi2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_fixuns_truncv16hi2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fixuns_truncv16hi2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fixuns_truncv16hi2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fixuns_truncv16hi2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_fix_truncv32hi2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_fix_truncv32hi2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_fix_truncv32hi2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_fix_truncv32hi2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_fixuns_truncv32hi2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_fixuns_truncv32hi2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_fixuns_truncv32hi2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_fixuns_truncv32hi2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_fix_truncv8si2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fix_truncv8si2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fix_truncv8si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fix_truncv8si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_fixuns_truncv8si2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fixuns_truncv8si2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fixuns_truncv8si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fixuns_truncv8si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_fix_truncv16si2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_fix_truncv16si2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_fix_truncv16si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_fix_truncv16si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_fixuns_truncv16si2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_fixuns_truncv16si2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_fixuns_truncv16si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_fixuns_truncv16si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_fix_truncv8di2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_fix_truncv8di2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_fix_truncv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_fix_truncv8di2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_fixuns_truncv8di2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_fixuns_truncv8di2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_fixuns_truncv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_fixuns_truncv8di2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_unspec_avx512fp16_fix_truncv4si2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_unspec_avx512fp16_fix_truncv4si2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_unspec_avx512fp16_fixuns_truncv4si2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_unspec_avx512fp16_fixuns_truncv4si2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_unspec_avx512fp16_fix_truncv4di2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_unspec_avx512fp16_fix_truncv4di2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_unspec_avx512fp16_fixuns_truncv4di2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_unspec_avx512fp16_fixuns_truncv4di2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fix_truncv4si2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_fix_truncv4si2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fixuns_truncv4si2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_fixuns_truncv4si2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fix_truncv4di2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_fix_truncv4di2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fixuns_truncv4di2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_fixuns_truncv4di2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_unspec_avx512fp16_fix_truncv2di2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_unspec_avx512fp16_fix_truncv2di2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_unspec_avx512fp16_fixuns_truncv2di2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_unspec_avx512fp16_fixuns_truncv2di2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fix_truncv2di2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_fix_truncv2di2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fixuns_truncv2di2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_fixuns_truncv2di2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_unspec_avx512fp16_fix_truncsi2 (TARGET_AVX512FP16)
#define HAVE_unspec_avx512fp16_fix_truncsi2_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_unspec_avx512fp16_fixuns_truncsi2 (TARGET_AVX512FP16)
#define HAVE_unspec_avx512fp16_fixuns_truncsi2_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_fix_truncsi2 (TARGET_AVX512FP16)
#define HAVE_avx512fp16_fix_truncsi2_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_fixuns_truncsi2 (TARGET_AVX512FP16)
#define HAVE_avx512fp16_fixuns_truncsi2_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_fix_truncsi2_mem (TARGET_AVX512FP16)
#define HAVE_avx512fp16_fixuns_truncsi2_mem (TARGET_AVX512FP16)
#define HAVE_avx512fp16_float_extend_phv8df2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_float_extend_phv8df2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_float_extend_phv8df2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_float_extend_phv8df2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_float_extend_phv16sf2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_float_extend_phv16sf2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_float_extend_phv16sf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_float_extend_phv16sf2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_float_extend_phv8sf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_float_extend_phv8sf2_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_float_extend_phv8sf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_float_extend_phv8sf2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_float_extend_phv4df2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_float_extend_phv4df2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_float_extend_phv4sf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_float_extend_phv4sf2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_float_extend_phv2df2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_float_extend_phv2df2_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtpd2ph_v8df ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_vcvtpd2ph_v8df_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtpd2ph_v8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtpd2ph_v8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_vcvtps2ph_v16sf ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_vcvtps2ph_v16sf_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtps2ph_v16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_vcvtps2ph_v16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_EVEX512))))
#define HAVE_avx512fp16_vcvtps2ph_v8sf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vcvtps2ph_v8sf_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtps2ph_v8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vcvtps2ph_v8sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512FP16) && (TARGET_AVX512VL))))
#define HAVE_avx512fp16_vcvtsh2sd (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vcvtsh2sd_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vcvtsh2sd_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vcvtsh2sd_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vcvtsh2ss (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vcvtsh2ss_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vcvtsh2ss_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vcvtsh2ss_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vcvtsh2sd_mem (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vcvtsh2sd_mask_mem ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vcvtsh2ss_mem (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vcvtsh2ss_mask_mem ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vcvtsd2sh (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vcvtsd2sh_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vcvtsd2sh_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vcvtsd2sh_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vcvtss2sh (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vcvtss2sh_round ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vcvtss2sh_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vcvtss2sh_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vcvtss2sh_mem (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vcvtss2sh_mask_mem ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vcvtsd2sh_mem (TARGET_AVX512FP16)
#define HAVE_avx512fp16_vcvtsd2sh_mask_mem ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_sse_cvtpi2ps ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSE)
#define HAVE_sse_cvtps2pi ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSE)
#define HAVE_unspec_sse_cvttps2pi ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSE)
#define HAVE_sse_cvttps2pi ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSE)
#define HAVE_sse_cvtsi2ss (TARGET_SSE)
#define HAVE_sse_cvtsi2ss_round ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_cvtss2si (TARGET_SSE)
#define HAVE_sse_cvtss2si_round ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_cvtss2si_2 (TARGET_SSE)
#define HAVE_unspec_sse_cvttss2si (TARGET_SSE)
#define HAVE_unspec_sse_cvttss2si_round ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_cvttss2si (TARGET_SSE)
#define HAVE_sse_cvttss2si_round ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_cvtusi2ss32 (TARGET_AVX512F && 1)
#define HAVE_cvtusi2ss32_round ((TARGET_AVX512F) && (TARGET_AVX512F && (V4SFmode == V4SFmode)))
#define HAVE_cvtusi2sd32 ((TARGET_AVX512F && 1) && (TARGET_SSE2))
#define HAVE_floatv16siv16sf2 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_floatv16siv16sf2_round ((TARGET_AVX512F) && ((TARGET_SSE2 && 1 && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_floatv16siv16sf2_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_floatv16siv16sf2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL) && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_floatv8siv8sf2 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX))
#define HAVE_floatv8siv8sf2_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX)))
#define HAVE_floatv4siv4sf2 (TARGET_SSE2 && 1 && 1)
#define HAVE_floatv4siv4sf2_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL) && 1))
#define HAVE_floatunsv16siv16sf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_floatunsv16siv16sf2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512))))
#define HAVE_floatunsv8siv8sf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_floatunsv8siv8sf2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_floatunsv4siv4sf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_floatunsv4siv4sf2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx_fix_notruncv8sfv8si ((TARGET_SSE2 && 1) && (TARGET_AVX))
#define HAVE_avx_fix_notruncv8sfv8si_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX)))
#define HAVE_sse2_fix_notruncv4sfv4si (TARGET_SSE2 && 1)
#define HAVE_sse2_fix_notruncv4sfv4si_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL)))
#define HAVE_avx512f_fix_notruncv16sfv16si (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_fix_notruncv16sfv16si_round ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_fix_notruncv16sfv16si_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_fix_notruncv16sfv16si_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_fixuns_notruncv16sfv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_fixuns_notruncv16sfv16si_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512))))
#define HAVE_avx512vl_fixuns_notruncv8sfv8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fixuns_notruncv8sfv8si_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512vl_fixuns_notruncv4sfv4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fixuns_notruncv4sfv4si_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512dq_cvtps2qqv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_avx512dq_cvtps2qqv8di_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DImode == V16SFmode \
							      || V8DImode == V8DFmode \
							      || V8DImode == V8DImode \
							      || V8DImode == V16SImode \
							      || V8DImode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_avx512dq_cvtps2qqv4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512dq_cvtps2qqv2di_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_AVX512VL))
#define HAVE_avx512dq_cvtps2uqqv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_avx512dq_cvtps2uqqv8di_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DImode == V16SFmode \
							      || V8DImode == V8DFmode \
							      || V8DImode == V8DImode \
							      || V8DImode == V16SImode \
							      || V8DImode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_avx512dq_cvtps2uqqv4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512dq_cvtps2uqqv2di_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_AVX512VL))
#define HAVE_unspec_fix_truncv16sfv16si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_unspec_fix_truncv16sfv16si2_round ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_unspec_fix_truncv16sfv16si2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_unspec_fix_truncv16sfv16si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_unspec_fixuns_truncv16sfv16si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_unspec_fixuns_truncv16sfv16si2_round ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_unspec_fixuns_truncv16sfv16si2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_unspec_fixuns_truncv16sfv16si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_fix_truncv16sfv16si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fix_truncv16sfv16si2_round ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_fix_truncv16sfv16si2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_fix_truncv16sfv16si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_fixuns_truncv16sfv16si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fixuns_truncv16sfv16si2_round ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_fixuns_truncv16sfv16si2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_fixuns_truncv16sfv16si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_unspec_fix_truncv8sfv8si2 (TARGET_AVX && 1)
#define HAVE_unspec_fix_truncv8sfv8si2_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512VL))
#define HAVE_fix_truncv8sfv8si2 (TARGET_AVX && 1)
#define HAVE_fix_truncv8sfv8si2_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512VL))
#define HAVE_unspec_fix_truncv4sfv4si2 (TARGET_SSE2 && 1)
#define HAVE_unspec_fix_truncv4sfv4si2_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL))
#define HAVE_fix_truncv4sfv4si2 (TARGET_SSE2 && 1)
#define HAVE_fix_truncv4sfv4si2_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL))
#define HAVE_sse2_cvtpi2pd (TARGET_SSE2)
#define HAVE_sse2_cvtpd2pi (TARGET_SSE2)
#define HAVE_unspec_sse2_cvttpd2pi (TARGET_SSE2)
#define HAVE_sse2_cvttpd2pi (TARGET_SSE2)
#define HAVE_sse2_cvtsi2sd (TARGET_SSE2)
#define HAVE_avx512f_vcvtss2usi (TARGET_AVX512F)
#define HAVE_avx512f_vcvtss2usi_round (TARGET_AVX512F)
#define HAVE_unspec_avx512f_vcvttss2usi (TARGET_AVX512F)
#define HAVE_unspec_avx512f_vcvttss2usi_round (TARGET_AVX512F)
#define HAVE_avx512f_vcvttss2usi (TARGET_AVX512F)
#define HAVE_avx512f_vcvttss2usi_round (TARGET_AVX512F)
#define HAVE_avx512f_vcvtsd2usi (TARGET_AVX512F)
#define HAVE_avx512f_vcvtsd2usi_round (TARGET_AVX512F)
#define HAVE_unspec_avx512f_vcvttsd2usi (TARGET_AVX512F)
#define HAVE_unspec_avx512f_vcvttsd2usi_round (TARGET_AVX512F)
#define HAVE_avx512f_vcvttsd2usi (TARGET_AVX512F)
#define HAVE_avx512f_vcvttsd2usi_round (TARGET_AVX512F)
#define HAVE_sse2_cvtsd2si (TARGET_SSE2)
#define HAVE_sse2_cvtsd2si_round ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_sse2_cvtsd2si_2 (TARGET_SSE2)
#define HAVE_unspec_sse2_cvttsd2si (TARGET_SSE2)
#define HAVE_unspec_sse2_cvttsd2si_round ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_sse2_cvttsd2si (TARGET_SSE2)
#define HAVE_sse2_cvttsd2si_round ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_floatv8siv8df2 ((TARGET_AVX && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_floatv8siv8df2_mask ((TARGET_AVX512F) && ((TARGET_AVX && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_floatv4siv4df2 (TARGET_AVX && 1)
#define HAVE_floatv4siv4df2_mask ((TARGET_AVX512F) && (TARGET_AVX && (32 == 64 || TARGET_AVX512VL)))
#define HAVE_floatv8div8df2 ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_floatv8div8df2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512)))
#define HAVE_floatv8div8df2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512)))
#define HAVE_floatv8div8df2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512))))
#define HAVE_floatunsv8div8df2 ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_floatunsv8div8df2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512)))
#define HAVE_floatunsv8div8df2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512)))
#define HAVE_floatunsv8div8df2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512))))
#define HAVE_floatv4div4df2 ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_floatv4div4df2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL)))
#define HAVE_floatv4div4df2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL)))
#define HAVE_floatv4div4df2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL))))
#define HAVE_floatunsv4div4df2 ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_floatunsv4div4df2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL)))
#define HAVE_floatunsv4div4df2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL)))
#define HAVE_floatunsv4div4df2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL))))
#define HAVE_floatv2div2df2 ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_floatv2div2df2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL)))
#define HAVE_floatv2div2df2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL)))
#define HAVE_floatv2div2df2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL))))
#define HAVE_floatunsv2div2df2 ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_floatunsv2div2df2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL)))
#define HAVE_floatunsv2div2df2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL)))
#define HAVE_floatunsv2div2df2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL))))
#define HAVE_floatv8div8sf2 ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_floatv8div8sf2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DImode == V16SFmode \
							      || V8DImode == V8DFmode \
							      || V8DImode == V8DImode \
							      || V8DImode == V16SImode \
							      || V8DImode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_floatv8div8sf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_floatv8div8sf2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DImode == V16SFmode \
							      || V8DImode == V8DFmode \
							      || V8DImode == V8DImode \
							      || V8DImode == V16SImode \
							      || V8DImode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_floatunsv8div8sf2 ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_floatunsv8div8sf2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DImode == V16SFmode \
							      || V8DImode == V8DFmode \
							      || V8DImode == V8DImode \
							      || V8DImode == V16SImode \
							      || V8DImode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_floatunsv8div8sf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_floatunsv8div8sf2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DImode == V16SFmode \
							      || V8DImode == V8DFmode \
							      || V8DImode == V8DImode \
							      || V8DImode == V16SImode \
							      || V8DImode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_floatv4div4sf2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_floatv4div4sf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_floatunsv4div4sf2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_floatunsv4div4sf2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_floatunsv8siv8df2 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_floatunsv8siv8df2_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_floatunsv4siv4df2 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_floatunsv4siv4df2_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_floatunsv2siv2df2_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_cvtdq2pd512_2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx_cvtdq2pd256_2 (TARGET_AVX)
#define HAVE_sse2_cvtdq2pd (TARGET_SSE2 && 1)
#define HAVE_sse2_cvtdq2pd_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL))
#define HAVE_avx512f_cvtpd2dq512 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_cvtpd2dq512_round ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_cvtpd2dq512_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_cvtpd2dq512_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx_cvtpd2dq256 (TARGET_AVX && 1)
#define HAVE_avx_cvtpd2dq256_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512VL))
#define HAVE_sse2_cvtpd2dq (TARGET_SSE2)
#define HAVE_sse2_cvtpd2dq_mask (TARGET_AVX512VL)
#define HAVE_fixuns_notruncv8dfv8si2 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_fixuns_notruncv8dfv8si2_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_fixuns_notruncv8dfv8si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_fixuns_notruncv8dfv8si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512))))
#define HAVE_fixuns_notruncv4dfv4si2 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_fixuns_notruncv4dfv4si2_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_fixuns_notruncv4dfv4si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_fixuns_notruncv4dfv4si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_fixuns_notruncv2dfv2si2 (TARGET_AVX512VL)
#define HAVE_fixuns_notruncv2dfv2si2_mask (TARGET_AVX512VL)
#define HAVE_unspec_fix_truncv8dfv8si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_unspec_fix_truncv8dfv8si2_round ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_unspec_fix_truncv8dfv8si2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_unspec_fix_truncv8dfv8si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_unspec_fixuns_truncv8dfv8si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_unspec_fixuns_truncv8dfv8si2_round ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_unspec_fixuns_truncv8dfv8si2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_unspec_fixuns_truncv8dfv8si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_fix_truncv8dfv8si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fix_truncv8dfv8si2_round ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_fix_truncv8dfv8si2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_fix_truncv8dfv8si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_fixuns_truncv8dfv8si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fixuns_truncv8dfv8si2_round ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_fixuns_truncv8dfv8si2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_fixuns_truncv8dfv8si2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_unspec_fixuns_truncv2dfv2si2_mask (TARGET_AVX512VL)
#define HAVE_fixuns_truncv2dfv2si2_mask (TARGET_AVX512VL)
#define HAVE_unspec_fix_truncv4dfv4si2 (TARGET_AVX || (TARGET_AVX512VL && TARGET_AVX512F))
#define HAVE_unspec_fix_truncv4dfv4si2_mask ((TARGET_AVX512F) && (TARGET_AVX || (TARGET_AVX512VL && TARGET_AVX512F)))
#define HAVE_fix_truncv4dfv4si2 (TARGET_AVX || (TARGET_AVX512VL && TARGET_AVX512F))
#define HAVE_fix_truncv4dfv4si2_mask ((TARGET_AVX512F) && (TARGET_AVX || (TARGET_AVX512VL && TARGET_AVX512F)))
#define HAVE_unspec_fixuns_truncv4dfv4si2 (TARGET_AVX512VL && TARGET_AVX512F)
#define HAVE_unspec_fixuns_truncv4dfv4si2_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512F))
#define HAVE_fixuns_truncv4dfv4si2 (TARGET_AVX512VL && TARGET_AVX512F)
#define HAVE_fixuns_truncv4dfv4si2_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512F))
#define HAVE_unspec_fix_truncv8dfv8di2 ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_unspec_fix_truncv8dfv8di2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_unspec_fix_truncv8dfv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_unspec_fix_truncv8dfv8di2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_unspec_fixuns_truncv8dfv8di2 ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_unspec_fixuns_truncv8dfv8di2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_unspec_fixuns_truncv8dfv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_unspec_fixuns_truncv8dfv8di2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_unspec_fix_truncv4dfv4di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_unspec_fix_truncv4dfv4di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_unspec_fixuns_truncv4dfv4di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_unspec_fixuns_truncv4dfv4di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_unspec_fix_truncv2dfv2di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_unspec_fix_truncv2dfv2di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_unspec_fixuns_truncv2dfv2di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_unspec_fixuns_truncv2dfv2di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_fix_truncv8dfv8di2 ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_fix_truncv8dfv8di2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fix_truncv8dfv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_fix_truncv8dfv8di2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_fixuns_truncv8dfv8di2 ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_fixuns_truncv8dfv8di2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fixuns_truncv8dfv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_fixuns_truncv8dfv8di2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_fix_truncv4dfv4di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_fix_truncv4dfv4di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_fixuns_truncv4dfv4di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_fixuns_truncv4dfv4di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_fix_truncv2dfv2di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_fix_truncv2dfv2di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_fixuns_truncv2dfv2di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_fixuns_truncv2dfv2di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_fix_notruncv8dfv8di2 ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_fix_notruncv8dfv8di2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fix_notruncv8dfv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_fix_notruncv8dfv8di2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_fix_notruncv4dfv4di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_fix_notruncv4dfv4di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_fix_notruncv2dfv2di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_fix_notruncv2dfv2di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_fixuns_notruncv8dfv8di2 ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_fixuns_notruncv8dfv8di2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fixuns_notruncv8dfv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_fixuns_notruncv8dfv8di2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_fixuns_notruncv4dfv4di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_fixuns_notruncv4dfv4di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_fixuns_notruncv2dfv2di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_fixuns_notruncv2dfv2di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_unspec_fix_truncv8sfv8di2 ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_unspec_fix_truncv8sfv8di2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DImode == V16SFmode \
									      || V8DImode == V8DFmode \
									      || V8DImode == V8DImode \
									      || V8DImode == V16SImode \
									      || V8DImode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_unspec_fix_truncv8sfv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_unspec_fix_truncv8sfv8di2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DImode == V16SFmode \
									      || V8DImode == V8DFmode \
									      || V8DImode == V8DImode \
									      || V8DImode == V16SImode \
									      || V8DImode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_unspec_fixuns_truncv8sfv8di2 ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_unspec_fixuns_truncv8sfv8di2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DImode == V16SFmode \
									      || V8DImode == V8DFmode \
									      || V8DImode == V8DImode \
									      || V8DImode == V16SImode \
									      || V8DImode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_unspec_fixuns_truncv8sfv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_unspec_fixuns_truncv8sfv8di2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DImode == V16SFmode \
									      || V8DImode == V8DFmode \
									      || V8DImode == V8DImode \
									      || V8DImode == V16SImode \
									      || V8DImode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_unspec_fix_truncv4sfv4di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_unspec_fix_truncv4sfv4di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_unspec_fixuns_truncv4sfv4di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_unspec_fixuns_truncv4sfv4di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_fix_truncv8sfv8di2 ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_fix_truncv8sfv8di2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DImode == V16SFmode \
									      || V8DImode == V8DFmode \
									      || V8DImode == V8DImode \
									      || V8DImode == V16SImode \
									      || V8DImode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fix_truncv8sfv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_fix_truncv8sfv8di2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DImode == V16SFmode \
									      || V8DImode == V8DFmode \
									      || V8DImode == V8DImode \
									      || V8DImode == V16SImode \
									      || V8DImode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_fixuns_truncv8sfv8di2 ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_fixuns_truncv8sfv8di2_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DImode == V16SFmode \
									      || V8DImode == V8DFmode \
									      || V8DImode == V8DImode \
									      || V8DImode == V16SImode \
									      || V8DImode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_fixuns_truncv8sfv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_fixuns_truncv8sfv8di2_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DImode == V16SFmode \
									      || V8DImode == V8DFmode \
									      || V8DImode == V8DImode \
									      || V8DImode == V16SImode \
									      || V8DImode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_fix_truncv4sfv4di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_fix_truncv4sfv4di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_fixuns_truncv4sfv4di2 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_fixuns_truncv4sfv4di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_unspec_avx512dq_fix_truncv2sfv2di2 (TARGET_AVX512DQ && TARGET_AVX512VL)
#define HAVE_unspec_avx512dq_fix_truncv2sfv2di2_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_AVX512VL))
#define HAVE_unspec_avx512dq_fixuns_truncv2sfv2di2 (TARGET_AVX512DQ && TARGET_AVX512VL)
#define HAVE_unspec_avx512dq_fixuns_truncv2sfv2di2_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_AVX512VL))
#define HAVE_avx512dq_fix_truncv2sfv2di2 (TARGET_AVX512DQ && TARGET_AVX512VL)
#define HAVE_avx512dq_fix_truncv2sfv2di2_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_AVX512VL))
#define HAVE_avx512dq_fixuns_truncv2sfv2di2 (TARGET_AVX512DQ && TARGET_AVX512VL)
#define HAVE_avx512dq_fixuns_truncv2sfv2di2_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_AVX512VL))
#define HAVE_unspec_fixuns_truncv8sfv8si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512VL) && (TARGET_AVX)))
#define HAVE_unspec_fixuns_truncv4sfv4si2_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_fixuns_truncv8sfv8si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512VL) && (TARGET_AVX)))
#define HAVE_fixuns_truncv4sfv4si2_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_unspec_sse2_cvttpd2dq (TARGET_SSE2 && 1)
#define HAVE_unspec_sse2_cvttpd2dq_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL))
#define HAVE_sse2_cvttpd2dq (TARGET_SSE2)
#define HAVE_sse2_cvttpd2dq_mask (TARGET_AVX512VL)
#define HAVE_sse2_cvtsd2ss (TARGET_SSE2)
#define HAVE_sse2_cvtsd2ss_round ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_sse2_cvtsd2ss_mask ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_sse2_cvtsd2ss_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_sse2_cvtss2sd (TARGET_SSE2)
#define HAVE_sse2_cvtss2sd_round ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_sse2_cvtss2sd_mask ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_sse2_cvtss2sd_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_cvtpd2ps512_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_cvtpd2ps512_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx_cvtpd2ps256 (TARGET_AVX && 1)
#define HAVE_avx_cvtpd2ps256_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512VL))
#define HAVE_avx512f_cvtps2pd512 ((TARGET_AVX && 1 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_cvtps2pd512_round ((TARGET_AVX512F) && ((TARGET_AVX && 1 && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_cvtps2pd512_mask ((TARGET_AVX512F) && ((TARGET_AVX && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_cvtps2pd512_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX && (64 == 64 || TARGET_AVX512VL) && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_avx_cvtps2pd256 (TARGET_AVX && 1 && 1)
#define HAVE_avx_cvtps2pd256_mask ((TARGET_AVX512F) && (TARGET_AVX && (32 == 64 || TARGET_AVX512VL) && 1))
#define HAVE_vec_unpacks_lo_v16sf (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512bw_cvtb2maskv64qi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_cvtb2maskv16qi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cvtb2maskv32qi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_cvtw2maskv32hi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_cvtw2maskv16hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cvtw2maskv8hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512f_cvtd2maskv16si ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_avx512vl_cvtd2maskv8si ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cvtd2maskv4si ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_avx512f_cvtq2maskv8di ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_avx512vl_cvtq2maskv4di ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cvtq2maskv2di ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_sse2_cvtps2pd (TARGET_SSE2 && 1)
#define HAVE_sse2_cvtps2pd_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL))
#define HAVE_sse2_cvtps2pd_1 (TARGET_SSE2 && 1)
#define HAVE_sse2_cvtps2pd_mask_1 ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL))
#define HAVE_sse_movhlps (TARGET_SSE && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_sse_movlhps (TARGET_SSE && ix86_binary_operator_ok (UNKNOWN, V4SFmode, operands))
#define HAVE_sse_movlhps_v8hi (TARGET_SSE && ix86_binary_operator_ok (UNKNOWN, V8HImode, operands))
#define HAVE_sse_movlhps_v8hf (TARGET_SSE && ix86_binary_operator_ok (UNKNOWN, V8HFmode, operands))
#define HAVE_sse_movlhps_v8bf (TARGET_SSE && ix86_binary_operator_ok (UNKNOWN, V8BFmode, operands))
#define HAVE_avx512f_unpckhps512_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_unpckhps256 (TARGET_AVX && 1)
#define HAVE_avx_unpckhps256_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512VL))
#define HAVE_vec_interleave_highv4sf (TARGET_SSE && 1)
#define HAVE_vec_interleave_highv4sf_mask ((TARGET_AVX512F) && (TARGET_SSE && TARGET_AVX512VL))
#define HAVE_avx512f_unpcklps512_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_unpcklps256 (TARGET_AVX && 1)
#define HAVE_avx_unpcklps256_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512VL))
#define HAVE_unpcklps128_mask (TARGET_AVX512VL)
#define HAVE_vec_interleave_lowv4sf (TARGET_SSE)
#define HAVE_avx_movshdup256 (TARGET_AVX && 1)
#define HAVE_avx_movshdup256_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512VL))
#define HAVE_sse3_movshdup (TARGET_SSE3 && 1)
#define HAVE_sse3_movshdup_mask ((TARGET_AVX512F) && (TARGET_SSE3 && TARGET_AVX512VL))
#define HAVE_avx512f_movshdup512_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_movsldup256 (TARGET_AVX && 1)
#define HAVE_avx_movsldup256_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512VL))
#define HAVE_sse3_movsldup (TARGET_SSE3 && 1)
#define HAVE_sse3_movsldup_mask ((TARGET_AVX512F) && (TARGET_SSE3 && TARGET_AVX512VL))
#define HAVE_avx512f_movsldup512_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_shufps256_1 (TARGET_AVX \
   && 1 \
   && (INTVAL (operands[3]) == (INTVAL (operands[7]) - 4) \
       && INTVAL (operands[4]) == (INTVAL (operands[8]) - 4) \
       && INTVAL (operands[5]) == (INTVAL (operands[9]) - 4) \
       && INTVAL (operands[6]) == (INTVAL (operands[10]) - 4)))
#define HAVE_avx_shufps256_1_mask ((TARGET_AVX512F) && (TARGET_AVX \
   && TARGET_AVX512VL \
   && (INTVAL (operands[3]) == (INTVAL (operands[7]) - 4) \
       && INTVAL (operands[4]) == (INTVAL (operands[8]) - 4) \
       && INTVAL (operands[5]) == (INTVAL (operands[9]) - 4) \
       && INTVAL (operands[6]) == (INTVAL (operands[10]) - 4))))
#define HAVE_sse_shufps_v4sf_mask (TARGET_AVX512VL)
#define HAVE_sse_shufps_v4si (TARGET_SSE)
#define HAVE_sse_shufps_v4sf (TARGET_SSE)
#define HAVE_sse_storehps (TARGET_SSE && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_sse_loadhps (TARGET_SSE)
#define HAVE_sse_storelps (TARGET_SSE && TARGET_ALIGN_VECTOR_INSN \
   && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_sse_storelps_unalign (TARGET_SSE && TARGET_NO_ALIGN_VECTOR_INSN \
   && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_sse_loadlps (TARGET_SSE)
#define HAVE_sse_movss_v4si (TARGET_SSE)
#define HAVE_sse_movss_v4sf (TARGET_SSE)
#define HAVE_avx2_vec_dupv8sf ((TARGET_AVX2) && (TARGET_AVX))
#define HAVE_avx2_vec_dupv4sf (TARGET_AVX2)
#define HAVE_avx2_vec_dupv8sf_1 (TARGET_AVX2)
#define HAVE_avx512f_vec_dupv16sf_1 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vec_dupv8df_1 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_setv4si_0 (TARGET_SSE)
#define HAVE_vec_setv4sf_0 (TARGET_SSE)
#define HAVE_vec_setv8hi_0 (TARGET_SSE2)
#define HAVE_vec_setv8hf_0 (TARGET_SSE2)
#define HAVE_vec_setv8bf_0 (TARGET_SSE2)
#define HAVE_vec_setv16hi_0 (TARGET_AVX512FP16)
#define HAVE_vec_setv32hi_0 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_vec_setv16hf_0 (TARGET_AVX512FP16)
#define HAVE_vec_setv32hf_0 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_vec_setv16bf_0 (TARGET_AVX512FP16)
#define HAVE_vec_setv32bf_0 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_avx512fp16_movv8hi (TARGET_AVX512FP16 \
  || (TARGET_AVX10_2 && const0_operand (operands[1], V8HImode)))
#define HAVE_avx512fp16_movv8hf (TARGET_AVX512FP16 \
  || (TARGET_AVX10_2 && const0_operand (operands[1], V8HFmode)))
#define HAVE_avx512fp16_movv8bf (TARGET_AVX512FP16 \
  || (TARGET_AVX10_2 && const0_operand (operands[1], V8BFmode)))
#define HAVE_vec_setv8si_0 (TARGET_AVX)
#define HAVE_vec_setv8sf_0 (TARGET_AVX)
#define HAVE_vec_setv16si_0 ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_setv16sf_0 ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_sse4_1_insertps_v4si (TARGET_SSE4_1)
#define HAVE_sse4_1_insertps_v4sf (TARGET_SSE4_1)
#define HAVE_vec_setv2df_0 (TARGET_SSE2)
#define HAVE_avx512dq_vextractf64x2_1_mask ((TARGET_AVX512DQ \
   && INTVAL (operands[2]) % 2 == 0 \
   && INTVAL (operands[2]) == INTVAL (operands[3]) - 1 \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[4]))) && (TARGET_EVEX512))
#define HAVE_avx512dq_vextracti64x2_1_mask ((TARGET_AVX512DQ \
   && INTVAL (operands[2]) % 2 == 0 \
   && INTVAL (operands[2]) == INTVAL (operands[3]) - 1 \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[4]))) && (TARGET_EVEX512))
#define HAVE_avx512f_vextractf32x4_1_mask ((TARGET_AVX512F \
   && INTVAL (operands[2]) % 4 == 0 \
   && INTVAL (operands[2]) == INTVAL (operands[3]) - 1 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && INTVAL (operands[4]) == INTVAL (operands[5]) - 1 \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[6]))) && (TARGET_EVEX512))
#define HAVE_avx512f_vextracti32x4_1_mask ((TARGET_AVX512F \
   && INTVAL (operands[2]) % 4 == 0 \
   && INTVAL (operands[2]) == INTVAL (operands[3]) - 1 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && INTVAL (operands[4]) == INTVAL (operands[5]) - 1 \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[6]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_lo_v8df_mask ((TARGET_AVX512F \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_lo_v8di_mask ((TARGET_AVX512F \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_lo_v8df ((TARGET_AVX512F && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_lo_v8di ((TARGET_AVX512F && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_hi_v8df_mask ((TARGET_AVX512F \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_hi_v8di_mask ((TARGET_AVX512F \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_hi_v8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_extract_hi_v8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_extract_hi_v16sf_mask ((TARGET_AVX512DQ \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_hi_v16si_mask ((TARGET_AVX512DQ \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_hi_v16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_extract_hi_v16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_extract_lo_v16sf_mask ((TARGET_AVX512DQ \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_lo_v16si_mask ((TARGET_AVX512DQ \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_lo_v16sf ((TARGET_AVX512F \
   && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_lo_v16si ((TARGET_AVX512F \
   && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_lo_v4di_mask (TARGET_AVX512DQ \
   && TARGET_AVX512VL \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2])))
#define HAVE_vec_extract_lo_v4df_mask (TARGET_AVX512DQ \
   && TARGET_AVX512VL \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2])))
#define HAVE_vec_extract_lo_v4di (TARGET_AVX \
   && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_vec_extract_lo_v4df (TARGET_AVX \
   && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_vec_extract_hi_v4di_mask (TARGET_AVX512DQ \
   && TARGET_AVX512VL \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2])))
#define HAVE_vec_extract_hi_v4df_mask (TARGET_AVX512DQ \
   && TARGET_AVX512VL \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2])))
#define HAVE_vec_extract_hi_v4di (TARGET_AVX)
#define HAVE_vec_extract_hi_v4df (TARGET_AVX)
#define HAVE_vec_extract_lo_v8si_mask (TARGET_AVX512VL \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2])))
#define HAVE_vec_extract_lo_v8sf_mask (TARGET_AVX512VL \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2])))
#define HAVE_vec_extract_lo_v8si (TARGET_AVX \
   && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_vec_extract_lo_v8sf (TARGET_AVX \
   && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_vec_extract_hi_v8si_mask (TARGET_AVX512VL \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2])))
#define HAVE_vec_extract_hi_v8sf_mask (TARGET_AVX512VL \
   && (!MEM_P (operands[0]) || rtx_equal_p (operands[0], operands[2])))
#define HAVE_vec_extract_hi_v8si (TARGET_AVX)
#define HAVE_vec_extract_hi_v8sf (TARGET_AVX)
#define HAVE_vec_extract_lo_v32hi ((TARGET_AVX512F && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_lo_v32hf ((TARGET_AVX512F && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_lo_v32bf ((TARGET_AVX512F && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_EVEX512))
#define HAVE_vec_extract_hi_v32hi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_extract_hi_v32hf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_extract_hi_v32bf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_extract_lo_v16hi (TARGET_AVX && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_vec_extract_lo_v16hf (TARGET_AVX && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_vec_extract_lo_v16bf (TARGET_AVX && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_vec_extract_hi_v16hi (TARGET_AVX)
#define HAVE_vec_extract_hi_v16hf (TARGET_AVX)
#define HAVE_vec_extract_hi_v16bf (TARGET_AVX)
#define HAVE_vec_extract_lo_v64qi (TARGET_AVX512F && TARGET_EVEX512 \
   && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_vec_extract_hi_v64qi (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_vec_extract_lo_v32qi (TARGET_AVX && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_vec_extract_hi_v32qi (TARGET_AVX)
#define HAVE_avx512f_unpckhpd512_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_unpckhpd256 (TARGET_AVX && 1)
#define HAVE_avx_unpckhpd256_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512VL))
#define HAVE_avx512vl_unpckhpd128_mask (TARGET_AVX512VL)
#define HAVE_avx512f_movddup512 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_movddup512_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_unpcklpd512 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_unpcklpd512_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_movddup256 (TARGET_AVX && 1)
#define HAVE_avx_movddup256_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512VL))
#define HAVE_avx_unpcklpd256 (TARGET_AVX && 1)
#define HAVE_avx_unpcklpd256_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512VL))
#define HAVE_avx512vl_unpcklpd128_mask (TARGET_AVX512VL)
#define HAVE_avx512f_vmscalefv8hf ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_vmscalefv8hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vmscalefv8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vmscalefv8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16))))
#define HAVE_avx512f_vmscalefv4sf (TARGET_AVX512F)
#define HAVE_avx512f_vmscalefv4sf_round (TARGET_AVX512F)
#define HAVE_avx512f_vmscalefv4sf_mask (TARGET_AVX512F)
#define HAVE_avx512f_vmscalefv4sf_mask_round (TARGET_AVX512F)
#define HAVE_avx512f_vmscalefv2df ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_vmscalefv2df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_vmscalefv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_vmscalefv2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2))))
#define HAVE_avx512bw_scalefv32hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_scalefv32hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512bw_scalefv32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512bw_scalefv32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_avx512vl_scalefv16hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_scalefv16hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512vl_scalefv16hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512vl_scalefv16hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))))
#define HAVE_avx512fp16_scalefv8hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_scalefv8hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_scalefv8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_scalefv8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))))
#define HAVE_avx512f_scalefv16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_scalefv16sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_scalefv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_scalefv16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512))))
#define HAVE_avx512vl_scalefv8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scalefv8sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_scalefv8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_scalefv8sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512vl_scalefv4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scalefv4sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_scalefv4sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_scalefv4sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512f_scalefv8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_scalefv8df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_scalefv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_scalefv8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512))))
#define HAVE_avx512vl_scalefv4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scalefv4df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_scalefv4df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_scalefv4df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512vl_scalefv2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scalefv2df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_scalefv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_scalefv2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512f_vternlogv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vternlogv16si_maskz_1 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_vternlogv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vternlogv8si_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vternlogv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vternlogv4si_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_vternlogv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vternlogv8di_maskz_1 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_vternlogv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vternlogv4di_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vternlogv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vternlogv2di_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512bw_getexpv32hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_getexpv32hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512bw_getexpv32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512bw_getexpv32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_avx512vl_getexpv16hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_getexpv16hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512vl_getexpv16hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512vl_getexpv16hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))))
#define HAVE_avx512fp16_getexpv8hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_getexpv8hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_getexpv8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_getexpv8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))))
#define HAVE_avx512f_getexpv16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_getexpv16sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_getexpv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_getexpv16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512))))
#define HAVE_avx512vl_getexpv8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_getexpv8sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getexpv8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getexpv8sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512vl_getexpv4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_getexpv4sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getexpv4sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getexpv4sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512f_getexpv8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_getexpv8df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_getexpv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_getexpv8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512))))
#define HAVE_avx512vl_getexpv4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_getexpv4df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getexpv4df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getexpv4df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512vl_getexpv2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_getexpv2df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getexpv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getexpv2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512f_sgetexpv8hf ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_sgetexpv8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_sgetexpv8hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_sgetexpv8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16))))
#define HAVE_avx512f_sgetexpv4sf (TARGET_AVX512F)
#define HAVE_avx512f_sgetexpv4sf_mask (TARGET_AVX512F)
#define HAVE_avx512f_sgetexpv4sf_round (TARGET_AVX512F)
#define HAVE_avx512f_sgetexpv4sf_mask_round (TARGET_AVX512F)
#define HAVE_avx512f_sgetexpv2df ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_sgetexpv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_sgetexpv2df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_sgetexpv2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2))))
#define HAVE_avx512f_alignv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_alignv8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_alignv4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_alignv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_alignv4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_alignv2di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fixupimmv16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fixupimmv16sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_fixupimmv16sf_maskz_1 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fixupimmv16sf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fixupimmv8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv8sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fixupimmv8sf_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv8sf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fixupimmv4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv4sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fixupimmv4sf_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv4sf_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fixupimmv8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fixupimmv8df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_fixupimmv8df_maskz_1 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fixupimmv8df_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fixupimmv4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv4df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fixupimmv4df_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv4df_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fixupimmv2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv2df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fixupimmv2df_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv2df_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fixupimmv16sf_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fixupimmv16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fixupimmv8sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv8sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fixupimmv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv4sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fixupimmv8df_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fixupimmv8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fixupimmv4df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv4df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fixupimmv2df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_sfixupimmv4sf (TARGET_AVX512F)
#define HAVE_avx512f_sfixupimmv4sf_round (TARGET_AVX512F)
#define HAVE_avx512f_sfixupimmv4sf_maskz_1 (TARGET_AVX512F)
#define HAVE_avx512f_sfixupimmv4sf_maskz_1_round (TARGET_AVX512F)
#define HAVE_avx512f_sfixupimmv2df ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_sfixupimmv2df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_sfixupimmv2df_maskz_1 ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_sfixupimmv2df_maskz_1_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2))))
#define HAVE_avx512f_sfixupimmv4sf_mask (TARGET_AVX512F)
#define HAVE_avx512f_sfixupimmv4sf_mask_round (TARGET_AVX512F)
#define HAVE_avx512f_sfixupimmv2df_mask ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_sfixupimmv2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512bw_rndscalev32hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_rndscalev32hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512bw_rndscalev32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512bw_rndscalev32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_avx512vl_rndscalev16hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_rndscalev16hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512vl_rndscalev16hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512vl_rndscalev16hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))))
#define HAVE_avx512fp16_rndscalev8hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_rndscalev8hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_rndscalev8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_rndscalev8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))))
#define HAVE_avx512f_rndscalev16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_rndscalev16sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_rndscalev16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_rndscalev16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512))))
#define HAVE_avx512vl_rndscalev8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rndscalev8sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rndscalev8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rndscalev8sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512vl_rndscalev4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rndscalev4sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rndscalev4sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rndscalev4sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512f_rndscalev8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_rndscalev8df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_rndscalev8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_rndscalev8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512))))
#define HAVE_avx512vl_rndscalev4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rndscalev4df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rndscalev4df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rndscalev4df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512vl_rndscalev2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rndscalev2df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rndscalev2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rndscalev2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512f_rndscalev8hf ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_rndscalev8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_rndscalev8hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_rndscalev8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16))))
#define HAVE_avx512f_rndscalev4sf (TARGET_AVX512F)
#define HAVE_avx512f_rndscalev4sf_mask (TARGET_AVX512F)
#define HAVE_avx512f_rndscalev4sf_round (TARGET_AVX512F)
#define HAVE_avx512f_rndscalev4sf_mask_round (TARGET_AVX512F)
#define HAVE_avx512f_rndscalev2df ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_rndscalev2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_rndscalev2df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_rndscalev2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2))))
#define HAVE_avx512f_shufps512_1 (TARGET_AVX512F && TARGET_EVEX512 \
   && (INTVAL (operands[3]) == (INTVAL (operands[7]) - 4) \
       && INTVAL (operands[4]) == (INTVAL (operands[8]) - 4) \
       && INTVAL (operands[5]) == (INTVAL (operands[9]) - 4) \
       && INTVAL (operands[6]) == (INTVAL (operands[10]) - 4) \
       && INTVAL (operands[3]) == (INTVAL (operands[11]) - 8) \
       && INTVAL (operands[4]) == (INTVAL (operands[12]) - 8) \
       && INTVAL (operands[5]) == (INTVAL (operands[13]) - 8) \
       && INTVAL (operands[6]) == (INTVAL (operands[14]) - 8) \
       && INTVAL (operands[3]) == (INTVAL (operands[15]) - 12) \
       && INTVAL (operands[4]) == (INTVAL (operands[16]) - 12) \
       && INTVAL (operands[5]) == (INTVAL (operands[17]) - 12) \
       && INTVAL (operands[6]) == (INTVAL (operands[18]) - 12)))
#define HAVE_avx512f_shufps512_1_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512 \
   && (INTVAL (operands[3]) == (INTVAL (operands[7]) - 4) \
       && INTVAL (operands[4]) == (INTVAL (operands[8]) - 4) \
       && INTVAL (operands[5]) == (INTVAL (operands[9]) - 4) \
       && INTVAL (operands[6]) == (INTVAL (operands[10]) - 4) \
       && INTVAL (operands[3]) == (INTVAL (operands[11]) - 8) \
       && INTVAL (operands[4]) == (INTVAL (operands[12]) - 8) \
       && INTVAL (operands[5]) == (INTVAL (operands[13]) - 8) \
       && INTVAL (operands[6]) == (INTVAL (operands[14]) - 8) \
       && INTVAL (operands[3]) == (INTVAL (operands[15]) - 12) \
       && INTVAL (operands[4]) == (INTVAL (operands[16]) - 12) \
       && INTVAL (operands[5]) == (INTVAL (operands[17]) - 12) \
       && INTVAL (operands[6]) == (INTVAL (operands[18]) - 12))))
#define HAVE_avx512f_shufpd512_1 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_shufpd512_1_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_shufpd256_1 (TARGET_AVX && 1)
#define HAVE_avx_shufpd256_1_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512VL))
#define HAVE_sse2_shufpd_v2df_mask (TARGET_AVX512VL)
#define HAVE_avx2_interleave_highv4di (TARGET_AVX2 && 1)
#define HAVE_avx2_interleave_highv4di_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_avx512f_interleave_highv8di_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_interleave_highv2di (TARGET_SSE2 && 1)
#define HAVE_vec_interleave_highv2di_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL))
#define HAVE_avx2_interleave_lowv4di (TARGET_AVX2 && 1)
#define HAVE_avx2_interleave_lowv4di_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_avx512f_interleave_lowv8di_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_interleave_lowv2di (TARGET_SSE2 && 1)
#define HAVE_vec_interleave_lowv2di_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL))
#define HAVE_sse2_shufpd_v2di (TARGET_SSE2)
#define HAVE_sse2_shufpd_v2df (TARGET_SSE2)
#define HAVE_sse2_storehpd (TARGET_SSE2 && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_sse2_storelpd (TARGET_SSE2 && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_sse2_loadhpd (TARGET_SSE2 && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_sse2_loadlpd (TARGET_SSE2 && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_sse2_movsd_v2di (TARGET_SSE2)
#define HAVE_sse2_movsd_v2df (TARGET_SSE2)
#define HAVE_vec_dupv2df (TARGET_SSE2)
#define HAVE_vec_dupv2df_mask ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_vec_concatv2df (TARGET_SSE && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_vec_setv8df_0 ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_setv4df_0 (TARGET_AVX)
#define HAVE_avx512f_ss_truncatev16siv16qi2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_truncatev16siv16qi2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_us_truncatev16siv16qi2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_ss_truncatev16siv16hi2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_truncatev16siv16hi2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_us_truncatev16siv16hi2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_ss_truncatev8div8si2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_truncatev8div8si2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_us_truncatev8div8si2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_ss_truncatev8div8hi2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_truncatev8div8hi2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_us_truncatev8div8hi2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512bw_ss_truncatev32hiv32qi2 (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_truncatev32hiv32qi2 (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_us_truncatev32hiv32qi2 (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_ss_truncatev32hiv32qi2_mask (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_truncatev32hiv32qi2_mask (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_us_truncatev32hiv32qi2_mask (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512vl_ss_truncatev4div4si2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4div4si2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4div4si2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev8siv8hi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev8siv8hi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev8siv8hi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev16hiv16qi2_mask ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_truncatev16hiv16qi2_mask ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_us_truncatev16hiv16qi2_mask ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_ss_truncatev4div4qi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4div4qi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4div4qi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev2div2qi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev2div2qi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev2div2qi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev8siv8qi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev8siv8qi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev8siv8qi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev4siv4qi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4siv4qi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4siv4qi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev8hiv8qi2 ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_truncatev8hiv8qi2 ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_us_truncatev8hiv8qi2 ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_ss_truncatev2div2qi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev2div2qi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev2div2qi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev2div2qi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev2div2qi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev2div2qi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev4siv4qi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4siv4qi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4siv4qi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev4div4qi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4div4qi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4div4qi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev4siv4qi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4siv4qi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4siv4qi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev4div4qi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4div4qi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4div4qi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev8hiv8qi2_mask ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_truncatev8hiv8qi2_mask ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_us_truncatev8hiv8qi2_mask ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_ss_truncatev8siv8qi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev8siv8qi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev8siv8qi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev8hiv8qi2_mask_store_1 ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_truncatev8hiv8qi2_mask_store_1 ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_us_truncatev8hiv8qi2_mask_store_1 ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_ss_truncatev8siv8qi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev8siv8qi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev8siv8qi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev4div4hi2 ((TARGET_AVX512VL) && (TARGET_AVX2))
#define HAVE_avx512vl_truncatev4div4hi2 ((TARGET_AVX512VL) && (TARGET_AVX2))
#define HAVE_avx512vl_us_truncatev4div4hi2 ((TARGET_AVX512VL) && (TARGET_AVX2))
#define HAVE_avx512vl_ss_truncatev2div2hi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev2div2hi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev2div2hi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev4siv4hi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4siv4hi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4siv4hi2 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev4siv4hi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4siv4hi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4siv4hi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev4div4hi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4div4hi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4div4hi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev4siv4hi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4siv4hi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4siv4hi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev4div4hi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4div4hi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4div4hi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev2div2hi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev2div2hi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev2div2hi2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev2div2hi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev2div2hi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev2div2hi2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev2div2si2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev2div2si2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev2div2si2 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev2div2si2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev2div2si2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev2div2si2_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev2div2si2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev2div2si2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev2div2si2_mask_store_1 (TARGET_AVX512VL)
#define HAVE_avx512f_ss_truncatev8div16qi2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_truncatev8div16qi2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_us_truncatev8div16qi2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_ss_truncatev8div16qi2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_truncatev8div16qi2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_us_truncatev8div16qi2_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_ss_truncatev8div16qi2_mask_store_1 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_truncatev8div16qi2_mask_store_1 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_us_truncatev8div16qi2_mask_store_1 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512bw_pmaddwd512v32hi ((TARGET_AVX512BW && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_pmaddwd512v32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_avx512bw_pmaddwd512v16hi ((TARGET_AVX512BW && 1) && (TARGET_AVX2))
#define HAVE_avx512bw_pmaddwd512v16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX2)))
#define HAVE_avx512bw_pmaddwd512v8hi (TARGET_AVX512BW && 1)
#define HAVE_avx512bw_pmaddwd512v8hi_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && (16 == 64 || TARGET_AVX512VL)))
#define HAVE_ashrv16hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512VL) && (TARGET_AVX512BW)))
#define HAVE_ashrv8hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512VL) && (TARGET_AVX512BW)))
#define HAVE_ashrv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_ashrv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_ashrv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_ashrv16hi3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_ashrv8hi3 (TARGET_SSE2)
#define HAVE_ashrv8si3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_ashrv4si3 (TARGET_SSE2)
#define HAVE_ashrv32hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_ashrv4di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_ashrv16si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_ashrv8di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_ashlv16hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512VL) && (TARGET_AVX512BW)))
#define HAVE_lshrv16hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512VL) && (TARGET_AVX512BW)))
#define HAVE_ashlv8hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512VL) && (TARGET_AVX512BW)))
#define HAVE_lshrv8hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512VL) && (TARGET_AVX512BW)))
#define HAVE_ashlv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_lshrv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_ashlv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_lshrv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_ashlv4di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_lshrv4di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_ashlv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_lshrv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_ashlv16hi3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_lshrv16hi3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_ashlv8hi3 (TARGET_SSE2)
#define HAVE_lshrv8hi3 (TARGET_SSE2)
#define HAVE_ashlv8si3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_lshrv8si3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_ashlv4si3 (TARGET_SSE2)
#define HAVE_lshrv4si3 (TARGET_SSE2)
#define HAVE_ashlv4di3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_lshrv4di3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_ashlv2di3 (TARGET_SSE2)
#define HAVE_lshrv2di3 (TARGET_SSE2)
#define HAVE_ashlv32hi3 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_ashlv32hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_lshrv32hi3 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_lshrv32hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_ashlv16si3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_ashlv16si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_lshrv16si3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_lshrv16si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_ashlv8di3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_ashlv8di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_lshrv8di3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_lshrv8di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512bw_ashlv4ti3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_lshrv4ti3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_ashlv2ti3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_lshrv2ti3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_ashlv1ti3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_lshrv1ti3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx2_ashlv2ti3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_avx2_lshrv2ti3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_sse2_ashlv1ti3 (TARGET_SSE2)
#define HAVE_sse2_lshrv1ti3 (TARGET_SSE2)
#define HAVE_avx512f_rolvv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_rolvv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_rorvv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_rorvv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_rolvv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rolvv8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rorvv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rorvv8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rolvv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rolvv4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rorvv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rorvv4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_rolvv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_rolvv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_rorvv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_rorvv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_rolvv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rolvv4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rorvv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rorvv4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rolvv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rolvv2di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rorvv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rorvv2di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_rolv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_rolv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_rorv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_rorv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_rolv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rolv8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rorv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rorv8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rolv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rolv4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rorv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rorv4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_rolv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_rolv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_rorv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_rorv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_rolv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rolv4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rorv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rorv4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rolv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rolv2di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_rorv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_rorv2di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_sse4_2_gtv2di3 (TARGET_SSE4_2)
#define HAVE_avx2_gtv32qi3 (TARGET_AVX2)
#define HAVE_avx2_gtv16hi3 (TARGET_AVX2)
#define HAVE_avx2_gtv8si3 (TARGET_AVX2)
#define HAVE_avx2_gtv4di3 (TARGET_AVX2)
#define HAVE_one_cmplv16si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512F \
   && (64 == 64 || TARGET_AVX512VL || TARGET_EVEX512) \
   && (!true \
       || SImode == SImode \
       || SImode == DImode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_one_cmplv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512F \
   && (64 == 64 || TARGET_AVX512VL || TARGET_EVEX512) \
   && (!true \
       || DImode == SImode \
       || DImode == DImode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_one_cmplv8si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512F \
   && (32 == 64 || TARGET_AVX512VL || TARGET_EVEX512) \
   && (!true \
       || SImode == SImode \
       || SImode == DImode)) && (TARGET_AVX)))
#define HAVE_one_cmplv4si2_mask ((TARGET_AVX512F) && (TARGET_AVX512F \
   && (16 == 64 || TARGET_AVX512VL || TARGET_EVEX512) \
   && (!true \
       || SImode == SImode \
       || SImode == DImode)))
#define HAVE_one_cmplv4di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512F \
   && (32 == 64 || TARGET_AVX512VL || TARGET_EVEX512) \
   && (!true \
       || DImode == SImode \
       || DImode == DImode)) && (TARGET_AVX)))
#define HAVE_one_cmplv2di2_mask ((TARGET_AVX512F) && (TARGET_AVX512F \
   && (16 == 64 || TARGET_AVX512VL || TARGET_EVEX512) \
   && (!true \
       || DImode == SImode \
       || DImode == DImode)))
#define HAVE_andv1ti3 (TARGET_SSE2)
#define HAVE_iorv1ti3 (TARGET_SSE2)
#define HAVE_xorv1ti3 (TARGET_SSE2)
#define HAVE_avx512bw_testmv64qi3 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_testmv64qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_avx512vl_testmv32qi3 ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx512vl_testmv32qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW)))
#define HAVE_avx512vl_testmv16qi3 ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx512vl_testmv16qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW)))
#define HAVE_avx512bw_testmv32hi3 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_testmv32hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_avx512vl_testmv16hi3 ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx512vl_testmv16hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW)))
#define HAVE_avx512vl_testmv8hi3 ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx512vl_testmv8hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW)))
#define HAVE_avx512f_testmv16si3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_testmv16si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_testmv8si3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_testmv8si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_testmv4si3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_testmv4si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_testmv8di3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_testmv8di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_testmv4di3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_testmv4di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_testmv2di3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_testmv2di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_testnmv64qi3 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_testnmv64qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_avx512vl_testnmv32qi3 ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx512vl_testnmv32qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW)))
#define HAVE_avx512vl_testnmv16qi3 ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx512vl_testnmv16qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW)))
#define HAVE_avx512bw_testnmv32hi3 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_testnmv32hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_avx512vl_testnmv16hi3 ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx512vl_testnmv16hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW)))
#define HAVE_avx512vl_testnmv8hi3 ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx512vl_testnmv8hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW)))
#define HAVE_avx512f_testnmv16si3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_testnmv16si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_testnmv8si3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_testnmv8si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_testnmv4si3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_testnmv4si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_testnmv8di3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_testnmv8di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_testnmv4di3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_testnmv4di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_testnmv2di3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_testnmv2di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_sse2_packsswb (TARGET_SSE2 && 1 && 1)
#define HAVE_sse2_packsswb_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx2_packsswb (TARGET_AVX2 && 1 && 1)
#define HAVE_avx2_packsswb_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx512bw_packsswb (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_packsswb_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_sse2_packssdw (TARGET_SSE2 && 1 && 1)
#define HAVE_sse2_packssdw_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx2_packssdw (TARGET_AVX2 && 1 && 1)
#define HAVE_avx2_packssdw_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx512bw_packssdw (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_packssdw_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_packuswb ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_packuswb_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_avx2_packuswb ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX2))
#define HAVE_avx2_packuswb_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_sse2_packuswb (TARGET_SSE2 && 1 && 1)
#define HAVE_sse2_packuswb_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_avx512bw_interleave_highv64qi (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_interleave_highv64qi_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx2_interleave_highv32qi (TARGET_AVX2 && 1 && 1)
#define HAVE_avx2_interleave_highv32qi_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_vec_interleave_highv16qi (TARGET_SSE2 && 1 && 1)
#define HAVE_vec_interleave_highv16qi_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx512bw_interleave_lowv64qi (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_interleave_lowv64qi_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx2_interleave_lowv32qi (TARGET_AVX2 && 1 && 1)
#define HAVE_avx2_interleave_lowv32qi_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_vec_interleave_lowv16qi (TARGET_SSE2 && 1 && 1)
#define HAVE_vec_interleave_lowv16qi_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx512bw_interleave_highv32hi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_interleave_highv32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512bw_interleave_highv32hf ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_interleave_highv32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512bw_interleave_highv32bf ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_interleave_highv32bf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx2_interleave_highv16hi (TARGET_AVX2 && 1 && 1)
#define HAVE_avx2_interleave_highv16hi_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx2_interleave_highv16hf (TARGET_AVX2 && 1 && 1)
#define HAVE_avx2_interleave_highv16hf_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx2_interleave_highv16bf (TARGET_AVX2 && 1 && 1)
#define HAVE_avx2_interleave_highv16bf_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_vec_interleave_highv8hi (TARGET_SSE2 && 1 && 1)
#define HAVE_vec_interleave_highv8hi_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_vec_interleave_highv8hf (TARGET_SSE2 && 1 && 1)
#define HAVE_vec_interleave_highv8hf_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_vec_interleave_highv8bf (TARGET_SSE2 && 1 && 1)
#define HAVE_vec_interleave_highv8bf_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx512bw_interleave_lowv32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512bw_interleave_lowv32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512bw_interleave_lowv32bf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx2_interleave_lowv16hi (TARGET_AVX2 && 1 && 1)
#define HAVE_avx2_interleave_lowv16hi_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx2_interleave_lowv16hf (TARGET_AVX2 && 1 && 1)
#define HAVE_avx2_interleave_lowv16hf_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx2_interleave_lowv16bf (TARGET_AVX2 && 1 && 1)
#define HAVE_avx2_interleave_lowv16bf_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_vec_interleave_lowv8hi (TARGET_SSE2 && 1 && 1)
#define HAVE_vec_interleave_lowv8hi_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_vec_interleave_lowv8hf (TARGET_SSE2 && 1 && 1)
#define HAVE_vec_interleave_lowv8hf_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_vec_interleave_lowv8bf (TARGET_SSE2 && 1 && 1)
#define HAVE_vec_interleave_lowv8bf_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_avx2_interleave_highv8si (TARGET_AVX2 && 1)
#define HAVE_avx2_interleave_highv8si_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_avx512f_interleave_highv16si_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_interleave_highv4si (TARGET_SSE2 && 1)
#define HAVE_vec_interleave_highv4si_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL))
#define HAVE_avx2_interleave_lowv8si (TARGET_AVX2 && 1)
#define HAVE_avx2_interleave_lowv8si_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_avx512f_interleave_lowv16si_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_interleave_lowv4si (TARGET_SSE2 && 1)
#define HAVE_vec_interleave_lowv4si_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL))
#define HAVE_sse4_1_pinsrb ((TARGET_SSE2 \
   && ((unsigned) exact_log2 (INTVAL (operands[3])) \
       < GET_MODE_NUNITS (V16QImode))) && (TARGET_SSE4_1))
#define HAVE_sse2_pinsrw (TARGET_SSE2 \
   && ((unsigned) exact_log2 (INTVAL (operands[3])) \
       < GET_MODE_NUNITS (V8HImode)))
#define HAVE_sse2_pinsrph (TARGET_SSE2 \
   && ((unsigned) exact_log2 (INTVAL (operands[3])) \
       < GET_MODE_NUNITS (V8HFmode)))
#define HAVE_sse2_pinsrbf (TARGET_SSE2 \
   && ((unsigned) exact_log2 (INTVAL (operands[3])) \
       < GET_MODE_NUNITS (V8BFmode)))
#define HAVE_sse4_1_pinsrd ((TARGET_SSE2 \
   && ((unsigned) exact_log2 (INTVAL (operands[3])) \
       < GET_MODE_NUNITS (V4SImode))) && (TARGET_SSE4_1))
#define HAVE_sse4_1_pinsrq ((TARGET_SSE2 \
   && ((unsigned) exact_log2 (INTVAL (operands[3])) \
       < GET_MODE_NUNITS (V2DImode))) && (TARGET_SSE4_1 && TARGET_64BIT))
#define HAVE_avx512dq_vinsertf64x2_1_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_EVEX512)))
#define HAVE_avx512dq_vinserti64x2_1_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_EVEX512)))
#define HAVE_avx512f_vinsertf32x4_1_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_vinserti32x4_1_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_vec_set_lo_v16sf ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_vec_set_lo_v16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512)))
#define HAVE_vec_set_lo_v16si ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_vec_set_lo_v16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512)))
#define HAVE_vec_set_hi_v16sf ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_vec_set_hi_v16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512)))
#define HAVE_vec_set_hi_v16si ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_vec_set_hi_v16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512)))
#define HAVE_vec_set_lo_v8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_set_lo_v8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_vec_set_lo_v8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_set_lo_v8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_vec_set_hi_v8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_set_hi_v8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_vec_set_hi_v8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_set_hi_v8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512dq_shuf_i64x2_1_mask ((TARGET_AVX512F) && (TARGET_AVX512VL \
   && (INTVAL (operands[3]) & 1) == 0 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && (INTVAL (operands[5]) & 1) == 0 \
   && INTVAL (operands[5]) == INTVAL (operands[6]) - 1))
#define HAVE_avx512dq_shuf_f64x2_1_mask ((TARGET_AVX512F) && (TARGET_AVX512VL \
   && (INTVAL (operands[3]) & 1) == 0 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && (INTVAL (operands[5]) & 1) == 0 \
   && INTVAL (operands[5]) == INTVAL (operands[6]) - 1))
#define HAVE_avx512f_shuf_f64x2_1 ((TARGET_AVX512F \
   && (INTVAL (operands[3]) & 1) == 0 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && (INTVAL (operands[5]) & 1) == 0 \
   && INTVAL (operands[5]) == INTVAL (operands[6]) - 1 \
   && (INTVAL (operands[7]) & 1) == 0 \
   && INTVAL (operands[7]) == INTVAL (operands[8]) - 1 \
   && (INTVAL (operands[9]) & 1) == 0 \
   && INTVAL (operands[9]) == INTVAL (operands[10]) - 1) && (TARGET_EVEX512))
#define HAVE_avx512f_shuf_f64x2_1_mask ((TARGET_AVX512F) && ((TARGET_AVX512F \
   && (INTVAL (operands[3]) & 1) == 0 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && (INTVAL (operands[5]) & 1) == 0 \
   && INTVAL (operands[5]) == INTVAL (operands[6]) - 1 \
   && (INTVAL (operands[7]) & 1) == 0 \
   && INTVAL (operands[7]) == INTVAL (operands[8]) - 1 \
   && (INTVAL (operands[9]) & 1) == 0 \
   && INTVAL (operands[9]) == INTVAL (operands[10]) - 1) && (TARGET_EVEX512)))
#define HAVE_avx512f_shuf_i64x2_1 ((TARGET_AVX512F \
   && (INTVAL (operands[3]) & 1) == 0 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && (INTVAL (operands[5]) & 1) == 0 \
   && INTVAL (operands[5]) == INTVAL (operands[6]) - 1 \
   && (INTVAL (operands[7]) & 1) == 0 \
   && INTVAL (operands[7]) == INTVAL (operands[8]) - 1 \
   && (INTVAL (operands[9]) & 1) == 0 \
   && INTVAL (operands[9]) == INTVAL (operands[10]) - 1) && (TARGET_EVEX512))
#define HAVE_avx512f_shuf_i64x2_1_mask ((TARGET_AVX512F) && ((TARGET_AVX512F \
   && (INTVAL (operands[3]) & 1) == 0 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && (INTVAL (operands[5]) & 1) == 0 \
   && INTVAL (operands[5]) == INTVAL (operands[6]) - 1 \
   && (INTVAL (operands[7]) & 1) == 0 \
   && INTVAL (operands[7]) == INTVAL (operands[8]) - 1 \
   && (INTVAL (operands[9]) & 1) == 0 \
   && INTVAL (operands[9]) == INTVAL (operands[10]) - 1) && (TARGET_EVEX512)))
#define HAVE_avx512vl_shuf_i32x4_1 (TARGET_AVX512VL \
   && (INTVAL (operands[3]) & 3) == 0 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && INTVAL (operands[3]) == INTVAL (operands[5]) - 2 \
   && INTVAL (operands[3]) == INTVAL (operands[6]) - 3 \
   && (INTVAL (operands[7]) & 3) == 0 \
   && INTVAL (operands[7]) == INTVAL (operands[8]) - 1 \
   && INTVAL (operands[7]) == INTVAL (operands[9]) - 2 \
   && INTVAL (operands[7]) == INTVAL (operands[10]) - 3)
#define HAVE_avx512vl_shuf_i32x4_1_mask ((TARGET_AVX512F) && (TARGET_AVX512VL \
   && (INTVAL (operands[3]) & 3) == 0 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && INTVAL (operands[3]) == INTVAL (operands[5]) - 2 \
   && INTVAL (operands[3]) == INTVAL (operands[6]) - 3 \
   && (INTVAL (operands[7]) & 3) == 0 \
   && INTVAL (operands[7]) == INTVAL (operands[8]) - 1 \
   && INTVAL (operands[7]) == INTVAL (operands[9]) - 2 \
   && INTVAL (operands[7]) == INTVAL (operands[10]) - 3))
#define HAVE_avx512vl_shuf_f32x4_1 (TARGET_AVX512VL \
   && (INTVAL (operands[3]) & 3) == 0 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && INTVAL (operands[3]) == INTVAL (operands[5]) - 2 \
   && INTVAL (operands[3]) == INTVAL (operands[6]) - 3 \
   && (INTVAL (operands[7]) & 3) == 0 \
   && INTVAL (operands[7]) == INTVAL (operands[8]) - 1 \
   && INTVAL (operands[7]) == INTVAL (operands[9]) - 2 \
   && INTVAL (operands[7]) == INTVAL (operands[10]) - 3)
#define HAVE_avx512vl_shuf_f32x4_1_mask ((TARGET_AVX512F) && (TARGET_AVX512VL \
   && (INTVAL (operands[3]) & 3) == 0 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && INTVAL (operands[3]) == INTVAL (operands[5]) - 2 \
   && INTVAL (operands[3]) == INTVAL (operands[6]) - 3 \
   && (INTVAL (operands[7]) & 3) == 0 \
   && INTVAL (operands[7]) == INTVAL (operands[8]) - 1 \
   && INTVAL (operands[7]) == INTVAL (operands[9]) - 2 \
   && INTVAL (operands[7]) == INTVAL (operands[10]) - 3))
#define HAVE_avx512f_shuf_f32x4_1 ((TARGET_AVX512F \
   && (INTVAL (operands[3]) & 3) == 0 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && INTVAL (operands[3]) == INTVAL (operands[5]) - 2 \
   && INTVAL (operands[3]) == INTVAL (operands[6]) - 3 \
   && (INTVAL (operands[7]) & 3) == 0 \
   && INTVAL (operands[7]) == INTVAL (operands[8]) - 1 \
   && INTVAL (operands[7]) == INTVAL (operands[9]) - 2 \
   && INTVAL (operands[7]) == INTVAL (operands[10]) - 3 \
   && (INTVAL (operands[11]) & 3) == 0 \
   && INTVAL (operands[11]) == INTVAL (operands[12]) - 1 \
   && INTVAL (operands[11]) == INTVAL (operands[13]) - 2 \
   && INTVAL (operands[11]) == INTVAL (operands[14]) - 3 \
   && (INTVAL (operands[15]) & 3) == 0 \
   && INTVAL (operands[15]) == INTVAL (operands[16]) - 1 \
   && INTVAL (operands[15]) == INTVAL (operands[17]) - 2 \
   && INTVAL (operands[15]) == INTVAL (operands[18]) - 3) && (TARGET_EVEX512))
#define HAVE_avx512f_shuf_f32x4_1_mask ((TARGET_AVX512F) && ((TARGET_AVX512F \
   && (INTVAL (operands[3]) & 3) == 0 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && INTVAL (operands[3]) == INTVAL (operands[5]) - 2 \
   && INTVAL (operands[3]) == INTVAL (operands[6]) - 3 \
   && (INTVAL (operands[7]) & 3) == 0 \
   && INTVAL (operands[7]) == INTVAL (operands[8]) - 1 \
   && INTVAL (operands[7]) == INTVAL (operands[9]) - 2 \
   && INTVAL (operands[7]) == INTVAL (operands[10]) - 3 \
   && (INTVAL (operands[11]) & 3) == 0 \
   && INTVAL (operands[11]) == INTVAL (operands[12]) - 1 \
   && INTVAL (operands[11]) == INTVAL (operands[13]) - 2 \
   && INTVAL (operands[11]) == INTVAL (operands[14]) - 3 \
   && (INTVAL (operands[15]) & 3) == 0 \
   && INTVAL (operands[15]) == INTVAL (operands[16]) - 1 \
   && INTVAL (operands[15]) == INTVAL (operands[17]) - 2 \
   && INTVAL (operands[15]) == INTVAL (operands[18]) - 3) && (TARGET_EVEX512)))
#define HAVE_avx512f_shuf_i32x4_1 ((TARGET_AVX512F \
   && (INTVAL (operands[3]) & 3) == 0 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && INTVAL (operands[3]) == INTVAL (operands[5]) - 2 \
   && INTVAL (operands[3]) == INTVAL (operands[6]) - 3 \
   && (INTVAL (operands[7]) & 3) == 0 \
   && INTVAL (operands[7]) == INTVAL (operands[8]) - 1 \
   && INTVAL (operands[7]) == INTVAL (operands[9]) - 2 \
   && INTVAL (operands[7]) == INTVAL (operands[10]) - 3 \
   && (INTVAL (operands[11]) & 3) == 0 \
   && INTVAL (operands[11]) == INTVAL (operands[12]) - 1 \
   && INTVAL (operands[11]) == INTVAL (operands[13]) - 2 \
   && INTVAL (operands[11]) == INTVAL (operands[14]) - 3 \
   && (INTVAL (operands[15]) & 3) == 0 \
   && INTVAL (operands[15]) == INTVAL (operands[16]) - 1 \
   && INTVAL (operands[15]) == INTVAL (operands[17]) - 2 \
   && INTVAL (operands[15]) == INTVAL (operands[18]) - 3) && (TARGET_EVEX512))
#define HAVE_avx512f_shuf_i32x4_1_mask ((TARGET_AVX512F) && ((TARGET_AVX512F \
   && (INTVAL (operands[3]) & 3) == 0 \
   && INTVAL (operands[3]) == INTVAL (operands[4]) - 1 \
   && INTVAL (operands[3]) == INTVAL (operands[5]) - 2 \
   && INTVAL (operands[3]) == INTVAL (operands[6]) - 3 \
   && (INTVAL (operands[7]) & 3) == 0 \
   && INTVAL (operands[7]) == INTVAL (operands[8]) - 1 \
   && INTVAL (operands[7]) == INTVAL (operands[9]) - 2 \
   && INTVAL (operands[7]) == INTVAL (operands[10]) - 3 \
   && (INTVAL (operands[11]) & 3) == 0 \
   && INTVAL (operands[11]) == INTVAL (operands[12]) - 1 \
   && INTVAL (operands[11]) == INTVAL (operands[13]) - 2 \
   && INTVAL (operands[11]) == INTVAL (operands[14]) - 3 \
   && (INTVAL (operands[15]) & 3) == 0 \
   && INTVAL (operands[15]) == INTVAL (operands[16]) - 1 \
   && INTVAL (operands[15]) == INTVAL (operands[17]) - 2 \
   && INTVAL (operands[15]) == INTVAL (operands[18]) - 3) && (TARGET_EVEX512)))
#define HAVE_avx512f_pshufd_1 (TARGET_AVX512F && TARGET_EVEX512 \
   && INTVAL (operands[2]) + 4 == INTVAL (operands[6]) \
   && INTVAL (operands[3]) + 4 == INTVAL (operands[7]) \
   && INTVAL (operands[4]) + 4 == INTVAL (operands[8]) \
   && INTVAL (operands[5]) + 4 == INTVAL (operands[9]) \
   && INTVAL (operands[2]) + 8 == INTVAL (operands[10]) \
   && INTVAL (operands[3]) + 8 == INTVAL (operands[11]) \
   && INTVAL (operands[4]) + 8 == INTVAL (operands[12]) \
   && INTVAL (operands[5]) + 8 == INTVAL (operands[13]) \
   && INTVAL (operands[2]) + 12 == INTVAL (operands[14]) \
   && INTVAL (operands[3]) + 12 == INTVAL (operands[15]) \
   && INTVAL (operands[4]) + 12 == INTVAL (operands[16]) \
   && INTVAL (operands[5]) + 12 == INTVAL (operands[17]))
#define HAVE_avx512f_pshufd_1_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512 \
   && INTVAL (operands[2]) + 4 == INTVAL (operands[6]) \
   && INTVAL (operands[3]) + 4 == INTVAL (operands[7]) \
   && INTVAL (operands[4]) + 4 == INTVAL (operands[8]) \
   && INTVAL (operands[5]) + 4 == INTVAL (operands[9]) \
   && INTVAL (operands[2]) + 8 == INTVAL (operands[10]) \
   && INTVAL (operands[3]) + 8 == INTVAL (operands[11]) \
   && INTVAL (operands[4]) + 8 == INTVAL (operands[12]) \
   && INTVAL (operands[5]) + 8 == INTVAL (operands[13]) \
   && INTVAL (operands[2]) + 12 == INTVAL (operands[14]) \
   && INTVAL (operands[3]) + 12 == INTVAL (operands[15]) \
   && INTVAL (operands[4]) + 12 == INTVAL (operands[16]) \
   && INTVAL (operands[5]) + 12 == INTVAL (operands[17])))
#define HAVE_avx2_pshufd_1 (TARGET_AVX2 \
   && 1 \
   && INTVAL (operands[2]) + 4 == INTVAL (operands[6]) \
   && INTVAL (operands[3]) + 4 == INTVAL (operands[7]) \
   && INTVAL (operands[4]) + 4 == INTVAL (operands[8]) \
   && INTVAL (operands[5]) + 4 == INTVAL (operands[9]))
#define HAVE_avx2_pshufd_1_mask ((TARGET_AVX512F) && (TARGET_AVX2 \
   && TARGET_AVX512VL \
   && INTVAL (operands[2]) + 4 == INTVAL (operands[6]) \
   && INTVAL (operands[3]) + 4 == INTVAL (operands[7]) \
   && INTVAL (operands[4]) + 4 == INTVAL (operands[8]) \
   && INTVAL (operands[5]) + 4 == INTVAL (operands[9])))
#define HAVE_sse2_pshufd_1 (TARGET_SSE2 && 1)
#define HAVE_sse2_pshufd_1_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL))
#define HAVE_avx512bw_pshuflwv32hi_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx2_pshuflw_1 (TARGET_AVX2 \
   && 1 && 1 \
   && INTVAL (operands[2]) + 8 == INTVAL (operands[6]) \
   && INTVAL (operands[3]) + 8 == INTVAL (operands[7]) \
   && INTVAL (operands[4]) + 8 == INTVAL (operands[8]) \
   && INTVAL (operands[5]) + 8 == INTVAL (operands[9]))
#define HAVE_avx2_pshuflw_1_mask ((TARGET_AVX512F) && (TARGET_AVX2 \
   && TARGET_AVX512BW && TARGET_AVX512VL \
   && INTVAL (operands[2]) + 8 == INTVAL (operands[6]) \
   && INTVAL (operands[3]) + 8 == INTVAL (operands[7]) \
   && INTVAL (operands[4]) + 8 == INTVAL (operands[8]) \
   && INTVAL (operands[5]) + 8 == INTVAL (operands[9])))
#define HAVE_sse2_pshuflw_1 (TARGET_SSE2 && 1 && 1)
#define HAVE_sse2_pshuflw_1_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_avx512bw_pshufhwv32hi_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx2_pshufhw_1 (TARGET_AVX2 \
   && 1 && 1 \
   && INTVAL (operands[2]) + 8 == INTVAL (operands[6]) \
   && INTVAL (operands[3]) + 8 == INTVAL (operands[7]) \
   && INTVAL (operands[4]) + 8 == INTVAL (operands[8]) \
   && INTVAL (operands[5]) + 8 == INTVAL (operands[9]))
#define HAVE_avx2_pshufhw_1_mask ((TARGET_AVX512F) && (TARGET_AVX2 \
   && TARGET_AVX512BW && TARGET_AVX512VL \
   && INTVAL (operands[2]) + 8 == INTVAL (operands[6]) \
   && INTVAL (operands[3]) + 8 == INTVAL (operands[7]) \
   && INTVAL (operands[4]) + 8 == INTVAL (operands[8]) \
   && INTVAL (operands[5]) + 8 == INTVAL (operands[9])))
#define HAVE_sse2_pshufhw_1 (TARGET_SSE2 && 1 && 1)
#define HAVE_sse2_pshufhw_1_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_sse2_loadld (TARGET_SSE)
#define HAVE_vec_concatv2di (TARGET_SSE)
#define HAVE_vec_setv8di_0 ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_setv4di_0 (TARGET_AVX)
#define HAVE_avx_movmskps256 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_sse_movmskps (TARGET_SSE)
#define HAVE_avx_movmskpd256 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_sse2_movmskpd ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_avx2_pmovmskb ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_sse2_pmovmskb (TARGET_SSE2)
#define HAVE_sse_ldmxcsr (TARGET_SSE)
#define HAVE_sse_stmxcsr (TARGET_SSE)
#define HAVE_sse2_clflush (TARGET_SSE2)
#define HAVE_sse3_mwait (TARGET_MWAIT)
#define HAVE_sse3_monitor_si ((TARGET_MWAIT) && (Pmode == SImode))
#define HAVE_sse3_monitor_di ((TARGET_MWAIT) && (Pmode == DImode))
#define HAVE_avx2_phaddwv16hi3 (TARGET_AVX2)
#define HAVE_avx2_phaddswv16hi3 (TARGET_AVX2)
#define HAVE_avx2_phsubwv16hi3 (TARGET_AVX2)
#define HAVE_avx2_phsubswv16hi3 (TARGET_AVX2)
#define HAVE_ssse3_phaddwv8hi3 (TARGET_SSSE3)
#define HAVE_ssse3_phaddswv8hi3 (TARGET_SSSE3)
#define HAVE_ssse3_phsubwv8hi3 (TARGET_SSSE3)
#define HAVE_ssse3_phsubswv8hi3 (TARGET_SSSE3)
#define HAVE_ssse3_phaddwv4hi3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_ssse3_phaddswv4hi3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_ssse3_phsubwv4hi3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_ssse3_phsubswv4hi3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_avx2_phadddv8si3 (TARGET_AVX2)
#define HAVE_avx2_phsubdv8si3 (TARGET_AVX2)
#define HAVE_ssse3_phadddv4si3 (TARGET_SSSE3)
#define HAVE_ssse3_phsubdv4si3 (TARGET_SSSE3)
#define HAVE_ssse3_phadddv2si3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_ssse3_phsubdv2si3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_avx2_pmaddubsw256 (TARGET_AVX2)
#define HAVE_avx512bw_pmaddubsw512v8hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_pmaddubsw512v8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_pmaddubsw512v16hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_pmaddubsw512v16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_pmaddubsw512v32hi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_pmaddubsw512v32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512bw_umulhrswv32hi3 (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_umulhrswv32hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_ssse3_pmaddubsw128 (TARGET_SSSE3)
#define HAVE_ssse3_pmaddubsw ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_avx512bw_pshufbv64qi3 ((TARGET_SSSE3 && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_pshufbv64qi3_mask ((TARGET_AVX512F) && ((TARGET_SSSE3 && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_avx2_pshufbv32qi3 ((TARGET_SSSE3 && 1 && 1) && (TARGET_AVX2))
#define HAVE_avx2_pshufbv32qi3_mask ((TARGET_AVX512F) && ((TARGET_SSSE3 && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_ssse3_pshufbv16qi3 (TARGET_SSSE3 && 1 && 1)
#define HAVE_ssse3_pshufbv16qi3_mask ((TARGET_AVX512F) && (TARGET_SSSE3 && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_avx2_psignv32qi3 ((TARGET_SSSE3) && (TARGET_AVX2))
#define HAVE_ssse3_psignv16qi3 (TARGET_SSSE3)
#define HAVE_avx2_psignv16hi3 ((TARGET_SSSE3) && (TARGET_AVX2))
#define HAVE_ssse3_psignv8hi3 (TARGET_SSSE3)
#define HAVE_avx2_psignv8si3 ((TARGET_SSSE3) && (TARGET_AVX2))
#define HAVE_ssse3_psignv4si3 (TARGET_SSSE3)
#define HAVE_ssse3_psignv8qi3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_ssse3_psignv4hi3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_ssse3_psignv2si3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_avx512bw_palignrv64qi_mask ((TARGET_AVX512BW && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx2_palignrv32qi_mask ((TARGET_AVX512BW && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX2))
#define HAVE_ssse3_palignrv16qi_mask (TARGET_AVX512BW && (16 == 64 || TARGET_AVX512VL))
#define HAVE_avx512bw_palignrv4ti ((TARGET_SSSE3) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx2_palignrv2ti ((TARGET_SSSE3) && (TARGET_AVX2))
#define HAVE_ssse3_palignrv1ti (TARGET_SSSE3)
#define HAVE_ssse3_palignrdi ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_absv16si2_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_absv8si2_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_absv4si2_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_absv8di2_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_absv4di2_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_absv2di2_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_absv64qi2_mask ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_absv16qi2_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_absv32qi2_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_absv32hi2_mask ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_absv16hi2_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_absv8hi2_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_sse4a_movntsf (TARGET_SSE4A)
#define HAVE_sse4a_movntdf (TARGET_SSE4A)
#define HAVE_sse4a_vmmovntv4sf (TARGET_SSE4A)
#define HAVE_sse4a_vmmovntv2df ((TARGET_SSE4A) && (TARGET_SSE2))
#define HAVE_sse4a_extrqi (TARGET_SSE4A)
#define HAVE_sse4a_extrq (TARGET_SSE4A)
#define HAVE_sse4a_insertqi (TARGET_SSE4A)
#define HAVE_sse4a_insertq (TARGET_SSE4A)
#define HAVE_avx_blendps256 ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_sse4_1_blendps (TARGET_SSE4_1)
#define HAVE_avx_blendpd256 ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_sse4_1_blendpd ((TARGET_SSE4_1) && (TARGET_SSE2))
#define HAVE_avx_blendvps256 ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_sse4_1_blendvps (TARGET_SSE4_1)
#define HAVE_avx_blendvpd256 ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_sse4_1_blendvpd ((TARGET_SSE4_1) && (TARGET_SSE2))
#define HAVE_sse4_1_blendvss (TARGET_SSE4_1)
#define HAVE_sse4_1_blendvsd (TARGET_SSE4_1)
#define HAVE_avx_dpps256 ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_sse4_1_dpps (TARGET_SSE4_1)
#define HAVE_avx_dppd256 ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_sse4_1_dppd ((TARGET_SSE4_1) && (TARGET_SSE2))
#define HAVE_avx512f_movntdqa ((TARGET_SSE4_1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx2_movntdqa ((TARGET_SSE4_1) && (TARGET_AVX2))
#define HAVE_sse4_1_movntdqa (TARGET_SSE4_1)
#define HAVE_avx2_mpsadbw ((TARGET_SSE4_1) && (TARGET_AVX2))
#define HAVE_sse4_1_mpsadbw (TARGET_SSE4_1)
#define HAVE_avx10_2_mpsadbw (TARGET_AVX10_2)
#define HAVE_avx10_2_mpsadbw_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx2_mpsadbw_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2) && (TARGET_AVX)))
#define HAVE_sse4_1_mpsadbw_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx512bw_packusdw ((TARGET_SSE4_1 && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_packusdw_mask ((TARGET_AVX512F) && ((TARGET_SSE4_1 && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_avx2_packusdw ((TARGET_SSE4_1 && 1 && 1) && (TARGET_AVX2))
#define HAVE_avx2_packusdw_mask ((TARGET_AVX512F) && ((TARGET_SSE4_1 && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_sse4_1_packusdw (TARGET_SSE4_1 && 1 && 1)
#define HAVE_sse4_1_packusdw_mask ((TARGET_AVX512F) && (TARGET_SSE4_1 && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_avx2_pblendvb ((TARGET_SSE4_1) && (TARGET_AVX2))
#define HAVE_sse4_1_pblendvb (TARGET_SSE4_1)
#define HAVE_sse4_1_pblendw (TARGET_SSE4_1)
#define HAVE_sse4_1_pblendph (TARGET_SSE4_1)
#define HAVE_sse4_1_pblendbf (TARGET_SSE4_1)
#define HAVE_avx2_pblenddv8si (TARGET_AVX2)
#define HAVE_avx2_pblenddv4si (TARGET_AVX2)
#define HAVE_sse4_1_phminposuw (TARGET_SSE4_1)
#define HAVE_avx2_sign_extendv16qiv16hi2 (TARGET_AVX2 && 1 && 1)
#define HAVE_avx2_sign_extendv16qiv16hi2_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_avx2_zero_extendv16qiv16hi2 (TARGET_AVX2 && 1 && 1)
#define HAVE_avx2_zero_extendv16qiv16hi2_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_avx512bw_sign_extendv32qiv32hi2 (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_sign_extendv32qiv32hi2_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_zero_extendv32qiv32hi2 (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_zero_extendv32qiv32hi2_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_sse4_1_sign_extendv8qiv8hi2 (TARGET_SSE4_1 && 1 && 1)
#define HAVE_sse4_1_sign_extendv8qiv8hi2_mask ((TARGET_AVX512F) && (TARGET_SSE4_1 && TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_sse4_1_zero_extendv8qiv8hi2 (TARGET_SSE4_1 && 1 && 1)
#define HAVE_sse4_1_zero_extendv8qiv8hi2_mask ((TARGET_AVX512F) && (TARGET_SSE4_1 && TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_avx512f_sign_extendv16qiv16si2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_zero_extendv16qiv16si2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx2_sign_extendv8qiv8si2 (TARGET_AVX2 && 1)
#define HAVE_avx2_sign_extendv8qiv8si2_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_avx2_zero_extendv8qiv8si2 (TARGET_AVX2 && 1)
#define HAVE_avx2_zero_extendv8qiv8si2_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_sse4_1_sign_extendv4qiv4si2 (TARGET_SSE4_1 && 1)
#define HAVE_sse4_1_sign_extendv4qiv4si2_mask ((TARGET_AVX512F) && (TARGET_SSE4_1 && TARGET_AVX512VL))
#define HAVE_sse4_1_zero_extendv4qiv4si2 (TARGET_SSE4_1 && 1)
#define HAVE_sse4_1_zero_extendv4qiv4si2_mask ((TARGET_AVX512F) && (TARGET_SSE4_1 && TARGET_AVX512VL))
#define HAVE_avx512f_sign_extendv16hiv16si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_sign_extendv16hiv16si2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_zero_extendv16hiv16si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_zero_extendv16hiv16si2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_zero_extendv16hiv16si2_1 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx2_sign_extendv8hiv8si2 (TARGET_AVX2 && 1)
#define HAVE_avx2_sign_extendv8hiv8si2_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_avx2_zero_extendv8hiv8si2 (TARGET_AVX2 && 1)
#define HAVE_avx2_zero_extendv8hiv8si2_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_avx2_zero_extendv8hiv8si2_1 (TARGET_AVX2)
#define HAVE_sse4_1_sign_extendv4hiv4si2 (TARGET_SSE4_1 && 1)
#define HAVE_sse4_1_sign_extendv4hiv4si2_mask ((TARGET_AVX512F) && (TARGET_SSE4_1 && TARGET_AVX512VL))
#define HAVE_sse4_1_zero_extendv4hiv4si2 (TARGET_SSE4_1 && 1)
#define HAVE_sse4_1_zero_extendv4hiv4si2_mask ((TARGET_AVX512F) && (TARGET_SSE4_1 && TARGET_AVX512VL))
#define HAVE_avx512f_sign_extendv8qiv8di2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_sign_extendv8qiv8di2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_zero_extendv8qiv8di2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_zero_extendv8qiv8di2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx2_sign_extendv4qiv4di2 (TARGET_AVX2 && 1)
#define HAVE_avx2_sign_extendv4qiv4di2_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_avx2_zero_extendv4qiv4di2 (TARGET_AVX2 && 1)
#define HAVE_avx2_zero_extendv4qiv4di2_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_sse4_1_sign_extendv2qiv2di2 (TARGET_SSE4_1 && 1)
#define HAVE_sse4_1_sign_extendv2qiv2di2_mask ((TARGET_AVX512F) && (TARGET_SSE4_1 && TARGET_AVX512VL))
#define HAVE_sse4_1_zero_extendv2qiv2di2 (TARGET_SSE4_1 && 1)
#define HAVE_sse4_1_zero_extendv2qiv2di2_mask ((TARGET_AVX512F) && (TARGET_SSE4_1 && TARGET_AVX512VL))
#define HAVE_avx512f_sign_extendv8hiv8di2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_sign_extendv8hiv8di2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_zero_extendv8hiv8di2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_zero_extendv8hiv8di2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx2_sign_extendv4hiv4di2 (TARGET_AVX2 && 1)
#define HAVE_avx2_sign_extendv4hiv4di2_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_avx2_zero_extendv4hiv4di2 (TARGET_AVX2 && 1)
#define HAVE_avx2_zero_extendv4hiv4di2_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_sse4_1_sign_extendv2hiv2di2 (TARGET_SSE4_1 && 1)
#define HAVE_sse4_1_sign_extendv2hiv2di2_mask ((TARGET_AVX512F) && (TARGET_SSE4_1 && TARGET_AVX512VL))
#define HAVE_sse4_1_zero_extendv2hiv2di2 (TARGET_SSE4_1 && 1)
#define HAVE_sse4_1_zero_extendv2hiv2di2_mask ((TARGET_AVX512F) && (TARGET_SSE4_1 && TARGET_AVX512VL))
#define HAVE_avx512f_sign_extendv8siv8di2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_sign_extendv8siv8di2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_zero_extendv8siv8di2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_zero_extendv8siv8di2_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx2_sign_extendv4siv4di2 (TARGET_AVX2 && 1)
#define HAVE_avx2_sign_extendv4siv4di2_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_avx2_zero_extendv4siv4di2 (TARGET_AVX2 && 1)
#define HAVE_avx2_zero_extendv4siv4di2_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_sse4_1_sign_extendv2siv2di2 (TARGET_SSE4_1 && 1)
#define HAVE_sse4_1_sign_extendv2siv2di2_mask ((TARGET_AVX512F) && (TARGET_SSE4_1 && TARGET_AVX512VL))
#define HAVE_sse4_1_zero_extendv2siv2di2 (TARGET_SSE4_1 && 1)
#define HAVE_sse4_1_zero_extendv2siv2di2_mask ((TARGET_AVX512F) && (TARGET_SSE4_1 && TARGET_AVX512VL))
#define HAVE_avx_vtestps256 (TARGET_AVX)
#define HAVE_avx_vtestps (TARGET_AVX)
#define HAVE_avx_vtestpd256 (TARGET_AVX)
#define HAVE_avx_vtestpd ((TARGET_AVX) && (TARGET_SSE2))
#define HAVE_ptesttf2 (TARGET_SSE4_1)
#define HAVE_avx_roundps256 ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_sse4_1_roundps (TARGET_SSE4_1)
#define HAVE_avx_roundpd256 ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_sse4_1_roundpd ((TARGET_SSE4_1) && (TARGET_SSE2))
#define HAVE_sse4_1_roundsh ((TARGET_SSE4_1) && (TARGET_AVX512FP16))
#define HAVE_sse4_1_roundss (TARGET_SSE4_1)
#define HAVE_sse4_1_roundsd ((TARGET_SSE4_1) && (TARGET_SSE2))
#define HAVE_sse4_2_pcmpestr (TARGET_SSE4_2 \
   && ix86_pre_reload_split ())
#define HAVE_sse4_2_pcmpestri (TARGET_SSE4_2)
#define HAVE_sse4_2_pcmpestrm (TARGET_SSE4_2)
#define HAVE_sse4_2_pcmpestr_cconly (TARGET_SSE4_2)
#define HAVE_sse4_2_pcmpistr (TARGET_SSE4_2 \
   && ix86_pre_reload_split ())
#define HAVE_sse4_2_pcmpistri (TARGET_SSE4_2)
#define HAVE_sse4_2_pcmpistrm (TARGET_SSE4_2)
#define HAVE_sse4_2_pcmpistr_cconly (TARGET_SSE4_2)
#define HAVE_xop_pmacsww (TARGET_XOP)
#define HAVE_xop_pmacssww (TARGET_XOP)
#define HAVE_xop_pmacsdd (TARGET_XOP)
#define HAVE_xop_pmacssdd (TARGET_XOP)
#define HAVE_xop_pmacsdql (TARGET_XOP)
#define HAVE_xop_pmacssdql (TARGET_XOP)
#define HAVE_xop_pmacsdqh (TARGET_XOP)
#define HAVE_xop_pmacssdqh (TARGET_XOP)
#define HAVE_xop_pmacswd (TARGET_XOP)
#define HAVE_xop_pmacsswd (TARGET_XOP)
#define HAVE_xop_pmadcswd (TARGET_XOP)
#define HAVE_xop_pmadcsswd (TARGET_XOP)
#define HAVE_xop_pcmov_v32qi256 (TARGET_XOP)
#define HAVE_xop_pcmov_v16qi (TARGET_XOP)
#define HAVE_xop_pcmov_v16hi256 (TARGET_XOP)
#define HAVE_xop_pcmov_v8hi (TARGET_XOP)
#define HAVE_xop_pcmov_v8si256 (TARGET_XOP)
#define HAVE_xop_pcmov_v4si (TARGET_XOP)
#define HAVE_xop_pcmov_v4di256 (TARGET_XOP)
#define HAVE_xop_pcmov_v2di (TARGET_XOP)
#define HAVE_xop_pcmov_v2ti256 (TARGET_XOP)
#define HAVE_xop_pcmov_v1ti (TARGET_XOP)
#define HAVE_xop_pcmov_v16hf256 (TARGET_XOP)
#define HAVE_xop_pcmov_v8hf (TARGET_XOP)
#define HAVE_xop_pcmov_v8sf256 (TARGET_XOP)
#define HAVE_xop_pcmov_v4sf (TARGET_XOP)
#define HAVE_xop_pcmov_v4df256 (TARGET_XOP)
#define HAVE_xop_pcmov_v2df (TARGET_XOP)
#define HAVE_xop_phaddbw (TARGET_XOP)
#define HAVE_xop_phaddubw (TARGET_XOP)
#define HAVE_xop_phaddbd (TARGET_XOP)
#define HAVE_xop_phaddubd (TARGET_XOP)
#define HAVE_xop_phaddbq (TARGET_XOP)
#define HAVE_xop_phaddubq (TARGET_XOP)
#define HAVE_xop_phaddwd (TARGET_XOP)
#define HAVE_xop_phadduwd (TARGET_XOP)
#define HAVE_xop_phaddwq (TARGET_XOP)
#define HAVE_xop_phadduwq (TARGET_XOP)
#define HAVE_xop_phadddq (TARGET_XOP)
#define HAVE_xop_phaddudq (TARGET_XOP)
#define HAVE_xop_phsubbw (TARGET_XOP)
#define HAVE_xop_phsubwd (TARGET_XOP)
#define HAVE_xop_phsubdq (TARGET_XOP)
#define HAVE_xop_pperm (TARGET_XOP && !(MEM_P (operands[2]) && MEM_P (operands[3])))
#define HAVE_xop_pperm_pack_v2di_v4si (TARGET_XOP && !(MEM_P (operands[2]) && MEM_P (operands[3])))
#define HAVE_xop_pperm_pack_v4si_v8hi (TARGET_XOP && !(MEM_P (operands[2]) && MEM_P (operands[3])))
#define HAVE_xop_pperm_pack_v8hi_v16qi (TARGET_XOP && !(MEM_P (operands[2]) && MEM_P (operands[3])))
#define HAVE_xop_rotlv16qi3 (TARGET_XOP)
#define HAVE_xop_rotlv8hi3 (TARGET_XOP)
#define HAVE_xop_rotlv4si3 (TARGET_XOP)
#define HAVE_xop_rotlv2di3 (TARGET_XOP)
#define HAVE_xop_rotrv16qi3 (TARGET_XOP)
#define HAVE_xop_rotrv8hi3 (TARGET_XOP)
#define HAVE_xop_rotrv4si3 (TARGET_XOP)
#define HAVE_xop_rotrv2di3 (TARGET_XOP)
#define HAVE_xop_vrotlv16qi3 (TARGET_XOP && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_xop_vrotlv8hi3 (TARGET_XOP && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_xop_vrotlv4si3 (TARGET_XOP && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_xop_vrotlv2di3 (TARGET_XOP && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_xop_shav16qi3 (TARGET_XOP && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_xop_shav8hi3 (TARGET_XOP && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_xop_shav4si3 (TARGET_XOP && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_xop_shav2di3 (TARGET_XOP && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_xop_shlv16qi3 (TARGET_XOP && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_xop_shlv8hi3 (TARGET_XOP && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_xop_shlv4si3 (TARGET_XOP && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_xop_shlv2di3 (TARGET_XOP && !(MEM_P (operands[1]) && MEM_P (operands[2])))
#define HAVE_xop_frczsf2 (TARGET_XOP)
#define HAVE_xop_frczdf2 (TARGET_XOP)
#define HAVE_xop_frczv4sf2 (TARGET_XOP)
#define HAVE_xop_frczv2df2 (TARGET_XOP)
#define HAVE_xop_frczv8sf2 (TARGET_XOP)
#define HAVE_xop_frczv4df2 (TARGET_XOP)
#define HAVE_xop_maskcmpv16qi3 (TARGET_XOP)
#define HAVE_xop_maskcmpv8hi3 (TARGET_XOP)
#define HAVE_xop_maskcmpv4si3 (TARGET_XOP)
#define HAVE_xop_maskcmpv2di3 (TARGET_XOP)
#define HAVE_xop_maskcmp_unsv16qi3 (TARGET_XOP)
#define HAVE_xop_maskcmp_unsv8hi3 (TARGET_XOP)
#define HAVE_xop_maskcmp_unsv4si3 (TARGET_XOP)
#define HAVE_xop_maskcmp_unsv2di3 (TARGET_XOP)
#define HAVE_xop_maskcmp_uns2v16qi3 (TARGET_XOP)
#define HAVE_xop_maskcmp_uns2v8hi3 (TARGET_XOP)
#define HAVE_xop_maskcmp_uns2v4si3 (TARGET_XOP)
#define HAVE_xop_maskcmp_uns2v2di3 (TARGET_XOP)
#define HAVE_xop_pcom_tfv16qi3 (TARGET_XOP)
#define HAVE_xop_pcom_tfv8hi3 (TARGET_XOP)
#define HAVE_xop_pcom_tfv4si3 (TARGET_XOP)
#define HAVE_xop_pcom_tfv2di3 (TARGET_XOP)
#define HAVE_xop_vpermil2v8sf3 ((TARGET_XOP) && (TARGET_AVX))
#define HAVE_xop_vpermil2v4sf3 (TARGET_XOP)
#define HAVE_xop_vpermil2v4df3 ((TARGET_XOP) && (TARGET_AVX))
#define HAVE_xop_vpermil2v2df3 ((TARGET_XOP) && (TARGET_SSE2))
#define HAVE_aesenc (TARGET_AES || (TARGET_VAES && TARGET_AVX512VL))
#define HAVE_aesenclast (TARGET_AES || (TARGET_VAES && TARGET_AVX512VL))
#define HAVE_aesdec (TARGET_AES || (TARGET_VAES && TARGET_AVX512VL))
#define HAVE_aesdeclast (TARGET_AES || (TARGET_VAES && TARGET_AVX512VL))
#define HAVE_aesimc (TARGET_AES)
#define HAVE_aeskeygenassist (TARGET_AES)
#define HAVE_pclmulqdq (TARGET_PCLMUL)
#define HAVE_avx_vzeroupper_callee_abi (TARGET_AVX)
#define HAVE_avx2_pbroadcastv16si ((TARGET_AVX2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx2_pbroadcastv8di ((TARGET_AVX2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx2_pbroadcastv64qi ((TARGET_AVX2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx2_pbroadcastv32qi ((TARGET_AVX2) && (TARGET_AVX))
#define HAVE_avx2_pbroadcastv16qi (TARGET_AVX2)
#define HAVE_avx2_pbroadcastv32hi ((TARGET_AVX2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx2_pbroadcastv16hi ((TARGET_AVX2) && (TARGET_AVX))
#define HAVE_avx2_pbroadcastv8hi (TARGET_AVX2)
#define HAVE_avx2_pbroadcastv8si ((TARGET_AVX2) && (TARGET_AVX))
#define HAVE_avx2_pbroadcastv4si (TARGET_AVX2)
#define HAVE_avx2_pbroadcastv4di ((TARGET_AVX2) && (TARGET_AVX))
#define HAVE_avx2_pbroadcastv2di (TARGET_AVX2)
#define HAVE_avx2_pbroadcastv32hf ((TARGET_AVX2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx2_pbroadcastv16hf ((TARGET_AVX2) && (TARGET_AVX))
#define HAVE_avx2_pbroadcastv8hf (TARGET_AVX2)
#define HAVE_avx2_pbroadcastv32bf ((TARGET_AVX2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx2_pbroadcastv16bf ((TARGET_AVX2) && (TARGET_AVX))
#define HAVE_avx2_pbroadcastv8bf (TARGET_AVX2)
#define HAVE_avx2_pbroadcastv32qi_1 (TARGET_AVX2)
#define HAVE_avx2_pbroadcastv16hi_1 (TARGET_AVX2)
#define HAVE_avx2_pbroadcastv8si_1 (TARGET_AVX2)
#define HAVE_avx2_pbroadcastv4di_1 (TARGET_AVX2)
#define HAVE_avx2_pbroadcastv16hf_1 (TARGET_AVX2)
#define HAVE_avx2_pbroadcastv16bf_1 (TARGET_AVX2)
#define HAVE_avx2_permvarv8si (TARGET_AVX2 && 1)
#define HAVE_avx2_permvarv8si_mask ((TARGET_AVX512F) && (TARGET_AVX2 && (32 == 64 || TARGET_AVX512VL)))
#define HAVE_avx2_permvarv8sf (TARGET_AVX2 && 1)
#define HAVE_avx2_permvarv8sf_mask ((TARGET_AVX512F) && (TARGET_AVX2 && (32 == 64 || TARGET_AVX512VL)))
#define HAVE_avx512f_permvarv16si ((TARGET_AVX2 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_permvarv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX2 && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_permvarv16sf ((TARGET_AVX2 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_permvarv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX2 && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_permvarv8di ((TARGET_AVX2 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_permvarv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX2 && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_permvarv8df ((TARGET_AVX2 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_permvarv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX2 && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx2_permvarv4di ((TARGET_AVX2 && 1) && (TARGET_AVX512VL))
#define HAVE_avx2_permvarv4di_mask ((TARGET_AVX512F) && ((TARGET_AVX2 && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512VL)))
#define HAVE_avx2_permvarv4df ((TARGET_AVX2 && 1) && (TARGET_AVX512VL))
#define HAVE_avx2_permvarv4df_mask ((TARGET_AVX512F) && ((TARGET_AVX2 && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_permvarv64qi ((TARGET_AVX512VBMI && 1) && (TARGET_EVEX512))
#define HAVE_avx512bw_permvarv64qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI && (64 == 64 || TARGET_AVX512VL)) && (TARGET_EVEX512)))
#define HAVE_avx512vl_permvarv16qi ((TARGET_AVX512VBMI && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_permvarv16qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI && (16 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_permvarv32qi ((TARGET_AVX512VBMI && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_permvarv32qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_permvarv8hi ((TARGET_AVX512BW && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_permvarv8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW && (16 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_permvarv16hi ((TARGET_AVX512BW && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_permvarv16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_permvarv32hi ((TARGET_AVX512BW && 1) && (TARGET_EVEX512))
#define HAVE_avx512bw_permvarv32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW && (64 == 64 || TARGET_AVX512VL)) && (TARGET_EVEX512)))
#define HAVE_avx512fp16_permvarv8hf ((TARGET_AVX512BW && 1) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_permvarv8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW && (16 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_permvarv16hf ((TARGET_AVX512BW && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_permvarv16hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_permvarv32hf ((TARGET_AVX512BW && 1) && (TARGET_EVEX512))
#define HAVE_avx512bw_permvarv32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW && (64 == 64 || TARGET_AVX512VL)) && (TARGET_EVEX512)))
#define HAVE_avx512vl_permvarv8bf ((TARGET_AVX512BW && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_permvarv8bf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW && (16 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_permvarv16bf ((TARGET_AVX512BW && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_permvarv16bf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_permvarv32bf ((TARGET_AVX512BW && 1) && (TARGET_EVEX512))
#define HAVE_avx512bw_permvarv32bf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW && (64 == 64 || TARGET_AVX512VL)) && (TARGET_EVEX512)))
#define HAVE_avx2_permv4di_1 (TARGET_AVX2 && 1)
#define HAVE_avx2_permv4di_1_mask ((TARGET_AVX512F) && (TARGET_AVX2 && (32 == 64 || TARGET_AVX512VL)))
#define HAVE_avx2_permv4df_1 (TARGET_AVX2 && 1)
#define HAVE_avx2_permv4df_1_mask ((TARGET_AVX512F) && (TARGET_AVX2 && (32 == 64 || TARGET_AVX512VL)))
#define HAVE_avx512f_permv8df_1 ((TARGET_AVX512F && 1 \
   && (INTVAL (operands[2]) == (INTVAL (operands[6]) - 4) \
       && INTVAL (operands[3]) == (INTVAL (operands[7]) - 4) \
       && INTVAL (operands[4]) == (INTVAL (operands[8]) - 4) \
       && INTVAL (operands[5]) == (INTVAL (operands[9]) - 4))) && (TARGET_EVEX512))
#define HAVE_avx512f_permv8df_1_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) \
   && (INTVAL (operands[2]) == (INTVAL (operands[6]) - 4) \
       && INTVAL (operands[3]) == (INTVAL (operands[7]) - 4) \
       && INTVAL (operands[4]) == (INTVAL (operands[8]) - 4) \
       && INTVAL (operands[5]) == (INTVAL (operands[9]) - 4))) && (TARGET_EVEX512)))
#define HAVE_avx512f_permv8di_1 ((TARGET_AVX512F && 1 \
   && (INTVAL (operands[2]) == (INTVAL (operands[6]) - 4) \
       && INTVAL (operands[3]) == (INTVAL (operands[7]) - 4) \
       && INTVAL (operands[4]) == (INTVAL (operands[8]) - 4) \
       && INTVAL (operands[5]) == (INTVAL (operands[9]) - 4))) && (TARGET_EVEX512))
#define HAVE_avx512f_permv8di_1_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && (64 == 64 || TARGET_AVX512VL) \
   && (INTVAL (operands[2]) == (INTVAL (operands[6]) - 4) \
       && INTVAL (operands[3]) == (INTVAL (operands[7]) - 4) \
       && INTVAL (operands[4]) == (INTVAL (operands[8]) - 4) \
       && INTVAL (operands[5]) == (INTVAL (operands[9]) - 4))) && (TARGET_EVEX512)))
#define HAVE_avx2_permv2ti (TARGET_AVX2)
#define HAVE_avx2_vec_dupv4df (TARGET_AVX2)
#define HAVE_avx512f_vec_dupv16si_1 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vec_dupv8di_1 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512bw_vec_dupv32hi_1 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_vec_dupv64qi_1 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_vec_dupv32hf_1 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_vec_dupv32bf_1 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512f_vec_dupv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vec_dupv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dupv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vec_dupv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_vec_dupv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vec_dupv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dupv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vec_dupv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv2di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_vec_dupv16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vec_dupv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dupv8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vec_dupv4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv4sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_vec_dupv8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vec_dupv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dupv4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv4df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vec_dupv2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_vec_dupv64qi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_vec_dupv64qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dupv16qi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv16qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vec_dupv32qi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv32qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_vec_dupv32hi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_vec_dupv32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dupv16hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vec_dupv8hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_vec_dupv32hf ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_vec_dupv32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dupv16hf ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv16hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vec_dupv8hf ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vec_dupv8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_vec_dupv32bf ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_vec_dupv32bf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dupv16bf ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv16bf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vec_dupv8bf ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vec_dupv8bf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512f_broadcastv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_broadcastv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_broadcastv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_broadcastv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512bw_vec_dup_gprv64qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dup_gprv16qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vec_dup_gprv32qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_vec_dup_gprv32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dup_gprv16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vec_dup_gprv8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_vec_dup_gprv32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dup_gprv16hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_vec_dup_gprv8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_vec_dup_gprv32bf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dup_gprv16bf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vec_dup_gprv8bf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512f_vec_dup_gprv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dup_gprv8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vec_dup_gprv4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_vec_dup_gprv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dup_gprv4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vec_dup_gprv2di_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_vec_dup_gprv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dup_gprv8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vec_dup_gprv4sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_vec_dup_gprv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vec_dup_gprv4df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vec_dup_gprv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_vec_dupv4sf (TARGET_SSE)
#define HAVE_avx2_vbroadcasti128_v32qi (TARGET_AVX2)
#define HAVE_avx2_vbroadcasti128_v16hi (TARGET_AVX2)
#define HAVE_avx2_vbroadcasti128_v8si (TARGET_AVX2)
#define HAVE_avx2_vbroadcasti128_v4di (TARGET_AVX2)
#define HAVE_avx2_lddqu_inserti_to_bcasti (TARGET_AVX2 && ix86_pre_reload_split ())
#define HAVE_vec_dupv8si (TARGET_AVX)
#define HAVE_vec_dupv8sf (TARGET_AVX)
#define HAVE_vec_dupv4di (TARGET_AVX)
#define HAVE_vec_dupv4df (TARGET_AVX)
#define HAVE_avx_vbroadcastf128_v32qi (TARGET_AVX)
#define HAVE_avx_vbroadcastf128_v16hi (TARGET_AVX)
#define HAVE_avx_vbroadcastf128_v8si (TARGET_AVX)
#define HAVE_avx_vbroadcastf128_v4di (TARGET_AVX)
#define HAVE_avx_vbroadcastf128_v8sf (TARGET_AVX)
#define HAVE_avx_vbroadcastf128_v4df (TARGET_AVX)
#define HAVE_avx_vbroadcastf128_v16hf (TARGET_AVX)
#define HAVE_avx_vbroadcastf128_v16bf (TARGET_AVX)
#define HAVE_avx512dq_broadcastv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512)))
#define HAVE_avx512dq_broadcastv8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL)))
#define HAVE_avx512dq_broadcastv4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL)))
#define HAVE_avx512dq_broadcastv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512)))
#define HAVE_avx512dq_broadcastv8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_broadcastv8si_mask_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_broadcastv8sf_mask_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512dq_broadcastv16sf_mask_1 ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512)))
#define HAVE_avx512dq_broadcastv16si_mask_1 ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512)))
#define HAVE_avx512dq_broadcastv8di_mask_1 ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512)))
#define HAVE_avx512dq_broadcastv8df_mask_1 ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_EVEX512)))
#define HAVE_avx512dq_broadcastv4di_mask_1 ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL)))
#define HAVE_avx512dq_broadcastv4df_mask_1 ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_AVX512VL)))
#define HAVE_avx512cd_maskb_vec_dupv8di ((TARGET_AVX512CD) && (TARGET_EVEX512))
#define HAVE_avx512cd_maskb_vec_dupv4di ((TARGET_AVX512CD) && (TARGET_AVX512VL))
#define HAVE_avx512cd_maskb_vec_dupv2di ((TARGET_AVX512CD) && (TARGET_AVX512VL))
#define HAVE_avx512cd_maskw_vec_dupv16si ((TARGET_AVX512CD) && (TARGET_EVEX512))
#define HAVE_avx512cd_maskw_vec_dupv8si ((TARGET_AVX512CD) && (TARGET_AVX512VL))
#define HAVE_avx512cd_maskw_vec_dupv4si ((TARGET_AVX512CD) && (TARGET_AVX512VL))
#define HAVE_avx512f_vpermilvarv16sf3 ((TARGET_AVX && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_vpermilvarv16sf3_mask ((TARGET_AVX512F) && ((TARGET_AVX && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx_vpermilvarv8sf3 ((TARGET_AVX && 1) && (TARGET_AVX))
#define HAVE_avx_vpermilvarv8sf3_mask ((TARGET_AVX512F) && ((TARGET_AVX && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX)))
#define HAVE_avx_vpermilvarv4sf3 (TARGET_AVX && 1)
#define HAVE_avx_vpermilvarv4sf3_mask ((TARGET_AVX512F) && (TARGET_AVX && (16 == 64 || TARGET_AVX512VL)))
#define HAVE_avx512f_vpermilvarv8df3 ((TARGET_AVX && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_vpermilvarv8df3_mask ((TARGET_AVX512F) && ((TARGET_AVX && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx_vpermilvarv4df3 ((TARGET_AVX && 1) && (TARGET_AVX))
#define HAVE_avx_vpermilvarv4df3_mask ((TARGET_AVX512F) && ((TARGET_AVX && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX)))
#define HAVE_avx_vpermilvarv2df3 ((TARGET_AVX && 1) && (TARGET_SSE2))
#define HAVE_avx_vpermilvarv2df3_mask ((TARGET_AVX512F) && ((TARGET_AVX && (16 == 64 || TARGET_AVX512VL)) && (TARGET_SSE2)))
#define HAVE_avx512f_vpermt2varv16si3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermt2varv16si3_maskz_1 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermt2varv16sf3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermt2varv16sf3_maskz_1 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermt2varv8di3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermt2varv8di3_maskz_1 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermt2varv8df3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermt2varv8df3_maskz_1 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_vpermt2varv8si3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv8si3_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv8sf3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv8sf3_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4di3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4di3_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4df3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4df3_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4si3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4si3_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4sf3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4sf3_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv2di3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv2di3_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv2df3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv2df3_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512bw_vpermt2varv32hi3 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_vpermt2varv32hi3_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512vl_vpermt2varv16hi3 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv16hi3_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv8hi3 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv8hi3_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_avx512bw_vpermt2varv64qi3 ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_EVEX512))
#define HAVE_avx512bw_vpermt2varv64qi3_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_EVEX512))
#define HAVE_avx512vl_vpermt2varv32qi3 ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv32qi3_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv16qi3 ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv16qi3_maskz_1 ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_AVX512VL))
#define HAVE_avx512bw_vpermt2varv32hf3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_vpermt2varv32hf3_maskz_1 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_vpermt2varv16hf3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv16hf3_maskz_1 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vpermt2varv8hf3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_vpermt2varv8hf3_maskz_1 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_vpermt2varv32bf3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_vpermt2varv32bf3_maskz_1 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_vpermt2varv16bf3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv16bf3_maskz_1 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv8bf3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv8bf3_maskz_1 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512f_vpermt2varv16si3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermt2varv16sf3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermt2varv8di3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermt2varv8df3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_vpermt2varv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv8sf3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4df3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4sf3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv2df3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512bw_vpermt2varv32hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512vl_vpermt2varv16hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv8hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_avx512bw_vpermt2varv64qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_EVEX512))
#define HAVE_avx512vl_vpermt2varv32qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv16qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_AVX512VL))
#define HAVE_vec_set_lo_v4di (TARGET_AVX && 1)
#define HAVE_vec_set_lo_v4di_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512DQ))
#define HAVE_vec_set_lo_v4df (TARGET_AVX && 1)
#define HAVE_vec_set_lo_v4df_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512DQ))
#define HAVE_vec_set_hi_v4di (TARGET_AVX && 1)
#define HAVE_vec_set_hi_v4di_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512DQ))
#define HAVE_vec_set_hi_v4df (TARGET_AVX && 1)
#define HAVE_vec_set_hi_v4df_mask ((TARGET_AVX512F) && (TARGET_AVX && TARGET_AVX512DQ))
#define HAVE_vec_set_lo_v8si (TARGET_AVX)
#define HAVE_vec_set_lo_v8si_mask ((TARGET_AVX512F) && (TARGET_AVX))
#define HAVE_vec_set_lo_v8sf (TARGET_AVX)
#define HAVE_vec_set_lo_v8sf_mask ((TARGET_AVX512F) && (TARGET_AVX))
#define HAVE_vec_set_hi_v8si (TARGET_AVX)
#define HAVE_vec_set_hi_v8si_mask ((TARGET_AVX512F) && (TARGET_AVX))
#define HAVE_vec_set_hi_v8sf (TARGET_AVX)
#define HAVE_vec_set_hi_v8sf_mask ((TARGET_AVX512F) && (TARGET_AVX))
#define HAVE_vec_set_lo_v16hi (TARGET_AVX)
#define HAVE_vec_set_lo_v16hf (TARGET_AVX)
#define HAVE_vec_set_lo_v16bf (TARGET_AVX)
#define HAVE_vec_set_hi_v16hi (TARGET_AVX)
#define HAVE_vec_set_hi_v16hf (TARGET_AVX)
#define HAVE_vec_set_hi_v16bf (TARGET_AVX)
#define HAVE_vec_set_lo_v32qi (TARGET_AVX)
#define HAVE_vec_set_hi_v32qi (TARGET_AVX)
#define HAVE_avx_maskloadps (TARGET_AVX)
#define HAVE_avx_maskloadpd (TARGET_AVX)
#define HAVE_avx2_maskloadq256 (TARGET_AVX)
#define HAVE_avx2_maskloadq (TARGET_AVX)
#define HAVE_avx_maskloadps256 (TARGET_AVX)
#define HAVE_avx_maskloadpd256 (TARGET_AVX)
#define HAVE_avx2_maskloadd256 (TARGET_AVX)
#define HAVE_avx2_maskloadd (TARGET_AVX)
#define HAVE_avx_maskstoreps (TARGET_AVX)
#define HAVE_avx_maskstorepd (TARGET_AVX)
#define HAVE_avx2_maskstoreq256 (TARGET_AVX)
#define HAVE_avx2_maskstoreq (TARGET_AVX)
#define HAVE_avx_maskstoreps256 (TARGET_AVX)
#define HAVE_avx_maskstorepd256 (TARGET_AVX)
#define HAVE_avx2_maskstored256 (TARGET_AVX)
#define HAVE_avx2_maskstored (TARGET_AVX)
#define HAVE_avx512f_storev16si_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_storev8si_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_storev4si_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_storev8di_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_storev4di_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_storev2di_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_storev16sf_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_storev8sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_storev4sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_storev8df_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_storev4df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_storev2df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512bw_storev64qi_mask ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_storev16qi_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_storev32qi_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_storev32hi_mask ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_storev16hi_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_storev8hi_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_storev32hf_mask ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_storev16hf_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_storev8hf_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_storev32bf_mask ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_storev16bf_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_storev8bf_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx_si256_si (TARGET_AVX && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_avx_ps256_ps (TARGET_AVX && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_avx_pd256_pd (TARGET_AVX && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_avx2_ashrvv4si (TARGET_AVX2 && 1)
#define HAVE_avx2_ashrvv4si_mask ((TARGET_AVX512F) && (TARGET_AVX2 && (16 == 64 || TARGET_AVX512VL)))
#define HAVE_avx2_ashrvv8si (TARGET_AVX2 && 1)
#define HAVE_avx2_ashrvv8si_mask ((TARGET_AVX512F) && (TARGET_AVX2 && (32 == 64 || TARGET_AVX512VL)))
#define HAVE_avx512f_ashrvv16si ((TARGET_AVX2 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_ashrvv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX2 && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx2_ashrvv2di ((TARGET_AVX2 && 1) && (TARGET_AVX512VL))
#define HAVE_avx2_ashrvv2di_mask ((TARGET_AVX512F) && ((TARGET_AVX2 && (16 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512VL)))
#define HAVE_avx2_ashrvv4di ((TARGET_AVX2 && 1) && (TARGET_AVX512VL))
#define HAVE_avx2_ashrvv4di_mask ((TARGET_AVX512F) && ((TARGET_AVX2 && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512VL)))
#define HAVE_avx512f_ashrvv8di ((TARGET_AVX2 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_ashrvv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX2 && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512vl_ashrvv8hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_ashrvv8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_ashrvv16hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_ashrvv16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_ashrvv32hi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_ashrvv32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512f_ashlvv16si ((TARGET_AVX2 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_ashlvv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX2 && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_lshrvv16si ((TARGET_AVX2 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_lshrvv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX2 && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx2_ashlvv8si (TARGET_AVX2 && 1)
#define HAVE_avx2_ashlvv8si_mask ((TARGET_AVX512F) && (TARGET_AVX2 && (32 == 64 || TARGET_AVX512VL)))
#define HAVE_avx2_lshrvv8si (TARGET_AVX2 && 1)
#define HAVE_avx2_lshrvv8si_mask ((TARGET_AVX512F) && (TARGET_AVX2 && (32 == 64 || TARGET_AVX512VL)))
#define HAVE_avx2_ashlvv4si (TARGET_AVX2 && 1)
#define HAVE_avx2_ashlvv4si_mask ((TARGET_AVX512F) && (TARGET_AVX2 && (16 == 64 || TARGET_AVX512VL)))
#define HAVE_avx2_lshrvv4si (TARGET_AVX2 && 1)
#define HAVE_avx2_lshrvv4si_mask ((TARGET_AVX512F) && (TARGET_AVX2 && (16 == 64 || TARGET_AVX512VL)))
#define HAVE_avx512f_ashlvv8di ((TARGET_AVX2 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_ashlvv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX2 && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_lshrvv8di ((TARGET_AVX2 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_lshrvv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX2 && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx2_ashlvv4di (TARGET_AVX2 && 1)
#define HAVE_avx2_ashlvv4di_mask ((TARGET_AVX512F) && (TARGET_AVX2 && (32 == 64 || TARGET_AVX512VL)))
#define HAVE_avx2_lshrvv4di (TARGET_AVX2 && 1)
#define HAVE_avx2_lshrvv4di_mask ((TARGET_AVX512F) && (TARGET_AVX2 && (32 == 64 || TARGET_AVX512VL)))
#define HAVE_avx2_ashlvv2di (TARGET_AVX2 && 1)
#define HAVE_avx2_ashlvv2di_mask ((TARGET_AVX512F) && (TARGET_AVX2 && (16 == 64 || TARGET_AVX512VL)))
#define HAVE_avx2_lshrvv2di (TARGET_AVX2 && 1)
#define HAVE_avx2_lshrvv2di_mask ((TARGET_AVX512F) && (TARGET_AVX2 && (16 == 64 || TARGET_AVX512VL)))
#define HAVE_avx512vl_ashlvv8hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_ashlvv8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_lshrvv8hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_lshrvv8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_ashlvv16hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_ashlvv16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_lshrvv16hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_lshrvv16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_ashlvv32hi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_ashlvv32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512bw_lshrvv32hi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_lshrvv32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx_vec_concatv32qi (TARGET_AVX \
   && (operands[2] == CONST0_RTX (V16QImode) \
       || !MEM_P (operands[1])))
#define HAVE_avx_vec_concatv16hi (TARGET_AVX \
   && (operands[2] == CONST0_RTX (V8HImode) \
       || !MEM_P (operands[1])))
#define HAVE_avx_vec_concatv16hf (TARGET_AVX \
   && (operands[2] == CONST0_RTX (V8HFmode) \
       || !MEM_P (operands[1])))
#define HAVE_avx_vec_concatv16bf (TARGET_AVX \
   && (operands[2] == CONST0_RTX (V8BFmode) \
       || !MEM_P (operands[1])))
#define HAVE_avx_vec_concatv8si (TARGET_AVX \
   && (operands[2] == CONST0_RTX (V4SImode) \
       || !MEM_P (operands[1])))
#define HAVE_avx_vec_concatv4di (TARGET_AVX \
   && (operands[2] == CONST0_RTX (V2DImode) \
       || !MEM_P (operands[1])))
#define HAVE_avx_vec_concatv8sf (TARGET_AVX \
   && (operands[2] == CONST0_RTX (V4SFmode) \
       || !MEM_P (operands[1])))
#define HAVE_avx_vec_concatv4df (TARGET_AVX \
   && (operands[2] == CONST0_RTX (V2DFmode) \
       || !MEM_P (operands[1])))
#define HAVE_avx_vec_concatv64qi ((TARGET_AVX \
   && (operands[2] == CONST0_RTX (V32QImode) \
       || !MEM_P (operands[1]))) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_vec_concatv32hi ((TARGET_AVX \
   && (operands[2] == CONST0_RTX (V16HImode) \
       || !MEM_P (operands[1]))) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_vec_concatv32hf ((TARGET_AVX \
   && (operands[2] == CONST0_RTX (V16HFmode) \
       || !MEM_P (operands[1]))) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_vec_concatv32bf ((TARGET_AVX \
   && (operands[2] == CONST0_RTX (V16BFmode) \
       || !MEM_P (operands[1]))) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_vec_concatv16si ((TARGET_AVX \
   && (operands[2] == CONST0_RTX (V8SImode) \
       || !MEM_P (operands[1]))) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_vec_concatv8di ((TARGET_AVX \
   && (operands[2] == CONST0_RTX (V4DImode) \
       || !MEM_P (operands[1]))) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_vec_concatv16sf ((TARGET_AVX \
   && (operands[2] == CONST0_RTX (V8SFmode) \
       || !MEM_P (operands[1]))) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_vec_concatv8df ((TARGET_AVX \
   && (operands[2] == CONST0_RTX (V4DFmode) \
       || !MEM_P (operands[1]))) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vcvtph2ps (TARGET_F16C || TARGET_AVX512VL)
#define HAVE_vcvtph2ps_mask ((TARGET_AVX512F) && (TARGET_F16C || TARGET_AVX512VL))
#define HAVE_vcvtph2ps256 (TARGET_F16C || TARGET_AVX512VL)
#define HAVE_vcvtph2ps256_mask ((TARGET_AVX512F) && (TARGET_F16C || TARGET_AVX512VL))
#define HAVE_avx512f_vcvtph2ps512_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_vcvtph2ps512_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_vcvtps2ph256 (TARGET_F16C || TARGET_AVX512VL)
#define HAVE_vcvtps2ph256_mask ((TARGET_AVX512F) && (TARGET_F16C || TARGET_AVX512VL))
#define HAVE_avx512f_vcvtps2ph512_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_vcvtps2ph512_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx512f_compressv16si_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_compressv16sf_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_compressv8di_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_compressv8df_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_compressv8si_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_compressv8sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_compressv4di_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_compressv4df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_compressv4si_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_compressv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_compressv2di_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_compressv2df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_compressv64qi_mask ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_compressv16qi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_compressv32qi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_compressv32hi_mask ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_compressv16hi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_compressv8hi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_avx512f_compressstorev16si_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_compressstorev16sf_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_compressstorev8di_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_compressstorev8df_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_compressstorev8si_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_compressstorev8sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_compressstorev4di_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_compressstorev4df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_compressstorev4si_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_compressstorev4sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_compressstorev2di_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_compressstorev2df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_compressstorev64qi_mask ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_compressstorev16qi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_compressstorev32qi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_compressstorev32hi_mask ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_compressstorev16hi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_compressstorev8hi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_expandv16si_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_expandv16sf_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_expandv8di_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_expandv8df_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_expandv8si_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_expandv8sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_expandv4di_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_expandv4df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_expandv4si_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_expandv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_expandv2di_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_expandv2df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_expandv64qi_mask ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_expandv16qi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_expandv32qi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_expandv32hi_mask ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_expandv16hi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_expandv8hi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_avx512dq_rangepv16sf ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_avx512dq_rangepv16sf_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512dq_rangepv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_avx512dq_rangepv16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_avx512dq_rangepv8sf ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_avx512dq_rangepv8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512dq_rangepv4sf ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_avx512dq_rangepv4sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512dq_rangepv8df ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_avx512dq_rangepv8df_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512dq_rangepv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512)))
#define HAVE_avx512dq_rangepv8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_EVEX512))))
#define HAVE_avx512dq_rangepv4df ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_avx512dq_rangepv4df_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512dq_rangepv2df ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_avx512dq_rangepv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512dq_rangesv4sf (TARGET_AVX512DQ)
#define HAVE_avx512dq_rangesv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ))
#define HAVE_avx512dq_rangesv4sf_round ((TARGET_AVX512F) && (TARGET_AVX512DQ))
#define HAVE_avx512dq_rangesv4sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512DQ)))
#define HAVE_avx512dq_rangesv2df ((TARGET_AVX512DQ) && (TARGET_SSE2))
#define HAVE_avx512dq_rangesv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_SSE2)))
#define HAVE_avx512dq_rangesv2df_round ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_SSE2)))
#define HAVE_avx512dq_rangesv2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512DQ) && (TARGET_SSE2))))
#define HAVE_avx512dq_fpclassv32hf ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512dq_fpclassv32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512dq_fpclassv16hf ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V16HFmode)) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512dq_fpclassv16hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V16HFmode)) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512dq_fpclassv8hf ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V8HFmode)) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512dq_fpclassv8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V8HFmode)) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512dq_fpclassv16sf ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V16SFmode)) && (TARGET_EVEX512))
#define HAVE_avx512dq_fpclassv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V16SFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512dq_fpclassv8sf ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V8SFmode)) && (TARGET_AVX512VL))
#define HAVE_avx512dq_fpclassv8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V8SFmode)) && (TARGET_AVX512VL)))
#define HAVE_avx512dq_fpclassv4sf ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V4SFmode)) && (TARGET_AVX512VL))
#define HAVE_avx512dq_fpclassv4sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V4SFmode)) && (TARGET_AVX512VL)))
#define HAVE_avx512dq_fpclassv8df ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V8DFmode)) && (TARGET_EVEX512))
#define HAVE_avx512dq_fpclassv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V8DFmode)) && (TARGET_EVEX512)))
#define HAVE_avx512dq_fpclassv4df ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V4DFmode)) && (TARGET_AVX512VL))
#define HAVE_avx512dq_fpclassv4df_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V4DFmode)) && (TARGET_AVX512VL)))
#define HAVE_avx512dq_fpclassv2df ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V2DFmode)) && (TARGET_AVX512VL))
#define HAVE_avx512dq_fpclassv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V2DFmode)) && (TARGET_AVX512VL)))
#define HAVE_avx512dq_vmfpclassv8hf ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V8HFmode)) && (TARGET_AVX512FP16))
#define HAVE_avx512dq_vmfpclassv8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V8HFmode)) && (TARGET_AVX512FP16)))
#define HAVE_avx512dq_vmfpclassv4sf (TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V4SFmode))
#define HAVE_avx512dq_vmfpclassv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V4SFmode)))
#define HAVE_avx512dq_vmfpclassv2df ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V2DFmode)) && (TARGET_SSE2))
#define HAVE_avx512dq_vmfpclassv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ || VALID_AVX512FP16_REG_MODE(V2DFmode)) && (TARGET_SSE2)))
#define HAVE_avx512bw_getmantv32hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_getmantv32hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512bw_getmantv32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512bw_getmantv32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_avx512vl_getmantv16hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_getmantv16hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512vl_getmantv16hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512vl_getmantv16hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))))
#define HAVE_avx512fp16_getmantv8hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_getmantv8hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_getmantv8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_getmantv8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))))
#define HAVE_avx512f_getmantv16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_getmantv16sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_getmantv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_getmantv16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512))))
#define HAVE_avx512vl_getmantv8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_getmantv8sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getmantv8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getmantv8sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512vl_getmantv4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_getmantv4sf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getmantv4sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getmantv4sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512f_getmantv8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_getmantv8df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_getmantv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512f_getmantv8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512))))
#define HAVE_avx512vl_getmantv4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_getmantv4df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getmantv4df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getmantv4df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512vl_getmantv2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_getmantv2df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getmantv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_getmantv2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL))))
#define HAVE_avx512f_vgetmantv8hf ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_vgetmantv8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vgetmantv8hf_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vgetmantv8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16))))
#define HAVE_avx512f_vgetmantv4sf (TARGET_AVX512F)
#define HAVE_avx512f_vgetmantv4sf_mask (TARGET_AVX512F)
#define HAVE_avx512f_vgetmantv4sf_round (TARGET_AVX512F)
#define HAVE_avx512f_vgetmantv4sf_mask_round (TARGET_AVX512F)
#define HAVE_avx512f_vgetmantv2df ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_vgetmantv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_vgetmantv2df_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_vgetmantv2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2))))
#define HAVE_avx512bw_dbpsadbwv8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_dbpsadbwv16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_dbpsadbwv32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_clzv16si2 ((TARGET_AVX512CD) && (TARGET_EVEX512))
#define HAVE_clzv16si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512CD) && (TARGET_EVEX512)))
#define HAVE_clzv8si2 ((TARGET_AVX512CD) && (TARGET_AVX512VL))
#define HAVE_clzv8si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512CD) && (TARGET_AVX512VL)))
#define HAVE_clzv4si2 ((TARGET_AVX512CD) && (TARGET_AVX512VL))
#define HAVE_clzv4si2_mask ((TARGET_AVX512F) && ((TARGET_AVX512CD) && (TARGET_AVX512VL)))
#define HAVE_clzv8di2 ((TARGET_AVX512CD) && (TARGET_EVEX512))
#define HAVE_clzv8di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512CD) && (TARGET_EVEX512)))
#define HAVE_clzv4di2 ((TARGET_AVX512CD) && (TARGET_AVX512VL))
#define HAVE_clzv4di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512CD) && (TARGET_AVX512VL)))
#define HAVE_clzv2di2 ((TARGET_AVX512CD) && (TARGET_AVX512VL))
#define HAVE_clzv2di2_mask ((TARGET_AVX512F) && ((TARGET_AVX512CD) && (TARGET_AVX512VL)))
#define HAVE_conflictv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512CD) && (TARGET_EVEX512)))
#define HAVE_conflictv8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512CD) && (TARGET_AVX512VL)))
#define HAVE_conflictv4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512CD) && (TARGET_AVX512VL)))
#define HAVE_conflictv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512CD) && (TARGET_EVEX512)))
#define HAVE_conflictv4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512CD) && (TARGET_AVX512VL)))
#define HAVE_conflictv2di_mask ((TARGET_AVX512F) && ((TARGET_AVX512CD) && (TARGET_AVX512VL)))
#define HAVE_sha1msg1 (TARGET_SHA)
#define HAVE_sha1msg2 (TARGET_SHA)
#define HAVE_sha1nexte (TARGET_SHA)
#define HAVE_sha1rnds4 (TARGET_SHA)
#define HAVE_sha256msg1 (TARGET_SHA)
#define HAVE_sha256msg2 (TARGET_SHA)
#define HAVE_sha256rnds2 (TARGET_SHA)
#define HAVE_vsm3msg1 (TARGET_SM3)
#define HAVE_vsm3msg2 (TARGET_SM3)
#define HAVE_vsm3rnds2 (TARGET_SM3)
#define HAVE_vsha512msg1 (TARGET_SHA512)
#define HAVE_vsha512msg2 (TARGET_SHA512)
#define HAVE_vsha512rnds2 (TARGET_SHA512)
#define HAVE_vsm4key4_v16si ((TARGET_SM4) && (TARGET_AVX10_2))
#define HAVE_vsm4key4_v8si (TARGET_SM4)
#define HAVE_vsm4key4_v4si (TARGET_SM4)
#define HAVE_vsm4rnds4_v16si ((TARGET_SM4) && (TARGET_AVX10_2))
#define HAVE_vsm4rnds4_v8si (TARGET_SM4)
#define HAVE_vsm4rnds4_v4si (TARGET_SM4)
#define HAVE_avx512f_si512_si ((TARGET_AVX512F && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_EVEX512))
#define HAVE_avx512f_ps512_ps ((TARGET_AVX512F && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_EVEX512))
#define HAVE_avx512f_pd512_pd ((TARGET_AVX512F && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_EVEX512))
#define HAVE_avx512f_si512_256si ((TARGET_AVX512F && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_EVEX512))
#define HAVE_avx512f_ps512_256ps ((TARGET_AVX512F && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_EVEX512))
#define HAVE_avx512f_pd512_256pd ((TARGET_AVX512F && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_EVEX512))
#define HAVE_vpmadd52luqv8di (TARGET_AVX512IFMA && TARGET_EVEX512)
#define HAVE_vpmadd52huqv8di (TARGET_AVX512IFMA && TARGET_EVEX512)
#define HAVE_vpmadd52luqv4di ((TARGET_AVXIFMA || (TARGET_AVX512IFMA && TARGET_AVX512VL)) && (TARGET_AVX2))
#define HAVE_vpmadd52huqv4di ((TARGET_AVXIFMA || (TARGET_AVX512IFMA && TARGET_AVX512VL)) && (TARGET_AVX2))
#define HAVE_vpmadd52luqv2di (TARGET_AVXIFMA || (TARGET_AVX512IFMA && TARGET_AVX512VL))
#define HAVE_vpmadd52huqv2di (TARGET_AVXIFMA || (TARGET_AVX512IFMA && TARGET_AVX512VL))
#define HAVE_vpmadd52luqv8di_maskz_1 ((TARGET_AVX512IFMA) && (TARGET_EVEX512))
#define HAVE_vpmadd52huqv8di_maskz_1 ((TARGET_AVX512IFMA) && (TARGET_EVEX512))
#define HAVE_vpmadd52luqv4di_maskz_1 ((TARGET_AVX512IFMA) && (TARGET_AVX512VL))
#define HAVE_vpmadd52huqv4di_maskz_1 ((TARGET_AVX512IFMA) && (TARGET_AVX512VL))
#define HAVE_vpmadd52luqv2di_maskz_1 ((TARGET_AVX512IFMA) && (TARGET_AVX512VL))
#define HAVE_vpmadd52huqv2di_maskz_1 ((TARGET_AVX512IFMA) && (TARGET_AVX512VL))
#define HAVE_vpmadd52luqv8di_mask ((TARGET_AVX512IFMA) && (TARGET_EVEX512))
#define HAVE_vpmadd52huqv8di_mask ((TARGET_AVX512IFMA) && (TARGET_EVEX512))
#define HAVE_vpmadd52luqv4di_mask ((TARGET_AVX512IFMA) && (TARGET_AVX512VL))
#define HAVE_vpmadd52huqv4di_mask ((TARGET_AVX512IFMA) && (TARGET_AVX512VL))
#define HAVE_vpmadd52luqv2di_mask ((TARGET_AVX512IFMA) && (TARGET_AVX512VL))
#define HAVE_vpmadd52huqv2di_mask ((TARGET_AVX512IFMA) && (TARGET_AVX512VL))
#define HAVE_vpmultishiftqbv64qi ((TARGET_AVX512VBMI) && (TARGET_EVEX512))
#define HAVE_vpmultishiftqbv64qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI) && (TARGET_EVEX512)))
#define HAVE_vpmultishiftqbv16qi ((TARGET_AVX512VBMI) && (TARGET_AVX512VL))
#define HAVE_vpmultishiftqbv16qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI) && (TARGET_AVX512VL)))
#define HAVE_vpmultishiftqbv32qi ((TARGET_AVX512VBMI) && (TARGET_AVX512VL))
#define HAVE_vpmultishiftqbv32qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI) && (TARGET_AVX512VL)))
#define HAVE_vpopcountv16si ((TARGET_AVX512VPOPCNTDQ) && (TARGET_EVEX512))
#define HAVE_vpopcountv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512VPOPCNTDQ) && (TARGET_EVEX512)))
#define HAVE_vpopcountv8si ((TARGET_AVX512VPOPCNTDQ) && (TARGET_AVX512VL))
#define HAVE_vpopcountv8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512VPOPCNTDQ) && (TARGET_AVX512VL)))
#define HAVE_vpopcountv4si ((TARGET_AVX512VPOPCNTDQ) && (TARGET_AVX512VL))
#define HAVE_vpopcountv4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512VPOPCNTDQ) && (TARGET_AVX512VL)))
#define HAVE_vpopcountv8di ((TARGET_AVX512VPOPCNTDQ) && (TARGET_EVEX512))
#define HAVE_vpopcountv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512VPOPCNTDQ) && (TARGET_EVEX512)))
#define HAVE_vpopcountv4di ((TARGET_AVX512VPOPCNTDQ) && (TARGET_AVX512VL))
#define HAVE_vpopcountv4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512VPOPCNTDQ) && (TARGET_AVX512VL)))
#define HAVE_vpopcountv2di ((TARGET_AVX512VPOPCNTDQ) && (TARGET_AVX512VL))
#define HAVE_vpopcountv2di_mask ((TARGET_AVX512F) && ((TARGET_AVX512VPOPCNTDQ) && (TARGET_AVX512VL)))
#define HAVE_vpopcountv64qi ((TARGET_AVX512BITALG) && (TARGET_EVEX512))
#define HAVE_vpopcountv64qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BITALG) && (TARGET_EVEX512)))
#define HAVE_vpopcountv16qi ((TARGET_AVX512BITALG) && (TARGET_AVX512VL))
#define HAVE_vpopcountv16qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BITALG) && (TARGET_AVX512VL)))
#define HAVE_vpopcountv32qi ((TARGET_AVX512BITALG) && (TARGET_AVX512VL))
#define HAVE_vpopcountv32qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BITALG) && (TARGET_AVX512VL)))
#define HAVE_vpopcountv32hi ((TARGET_AVX512BITALG) && (TARGET_EVEX512))
#define HAVE_vpopcountv32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BITALG) && (TARGET_EVEX512)))
#define HAVE_vpopcountv16hi ((TARGET_AVX512BITALG) && (TARGET_AVX512VL))
#define HAVE_vpopcountv16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BITALG) && (TARGET_AVX512VL)))
#define HAVE_vpopcountv8hi ((TARGET_AVX512BITALG) && (TARGET_AVX512VL))
#define HAVE_vpopcountv8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BITALG) && (TARGET_AVX512VL)))
#define HAVE_vgf2p8affineinvqb_v64qi ((TARGET_GFNI) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vgf2p8affineinvqb_v64qi_mask ((TARGET_AVX512F) && ((TARGET_GFNI) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_vgf2p8affineinvqb_v32qi ((TARGET_GFNI) && (TARGET_AVX))
#define HAVE_vgf2p8affineinvqb_v32qi_mask ((TARGET_AVX512F) && ((TARGET_GFNI) && (TARGET_AVX)))
#define HAVE_vgf2p8affineinvqb_v16qi (TARGET_GFNI)
#define HAVE_vgf2p8affineinvqb_v16qi_mask ((TARGET_AVX512F) && (TARGET_GFNI))
#define HAVE_vgf2p8affineqb_v64qi ((TARGET_GFNI) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vgf2p8affineqb_v64qi_mask ((TARGET_AVX512F) && ((TARGET_GFNI) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_vgf2p8affineqb_v32qi ((TARGET_GFNI) && (TARGET_AVX))
#define HAVE_vgf2p8affineqb_v32qi_mask ((TARGET_AVX512F) && ((TARGET_GFNI) && (TARGET_AVX)))
#define HAVE_vgf2p8affineqb_v16qi (TARGET_GFNI)
#define HAVE_vgf2p8affineqb_v16qi_mask ((TARGET_AVX512F) && (TARGET_GFNI))
#define HAVE_vgf2p8mulb_v64qi ((TARGET_GFNI) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vgf2p8mulb_v64qi_mask ((TARGET_AVX512F) && ((TARGET_GFNI) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_vgf2p8mulb_v32qi ((TARGET_GFNI) && (TARGET_AVX))
#define HAVE_vgf2p8mulb_v32qi_mask ((TARGET_AVX512F) && ((TARGET_GFNI) && (TARGET_AVX)))
#define HAVE_vgf2p8mulb_v16qi (TARGET_GFNI)
#define HAVE_vgf2p8mulb_v16qi_mask ((TARGET_AVX512F) && (TARGET_GFNI))
#define HAVE_vpshrd_v32hi ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrd_v32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_EVEX512)))
#define HAVE_vpshrd_v16si ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrd_v16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_EVEX512)))
#define HAVE_vpshrd_v8di ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrd_v8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_EVEX512)))
#define HAVE_vpshrd_v16hi ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrd_v16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL)))
#define HAVE_vpshrd_v8si ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrd_v8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL)))
#define HAVE_vpshrd_v4di ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrd_v4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL)))
#define HAVE_vpshrd_v8hi ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrd_v8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL)))
#define HAVE_vpshrd_v4si ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrd_v4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL)))
#define HAVE_vpshrd_v2di ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrd_v2di_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL)))
#define HAVE_vpshld_v32hi ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshld_v32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_EVEX512)))
#define HAVE_vpshld_v16si ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshld_v16si_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_EVEX512)))
#define HAVE_vpshld_v8di ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshld_v8di_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_EVEX512)))
#define HAVE_vpshld_v16hi ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshld_v16hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL)))
#define HAVE_vpshld_v8si ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshld_v8si_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL)))
#define HAVE_vpshld_v4di ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshld_v4di_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL)))
#define HAVE_vpshld_v8hi ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshld_v8hi_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL)))
#define HAVE_vpshld_v4si ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshld_v4si_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL)))
#define HAVE_vpshld_v2di ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshld_v2di_mask ((TARGET_AVX512F) && ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL)))
#define HAVE_vpshrdv_v32hi ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrdv_v16si ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrdv_v8di ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrdv_v16hi ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v8si ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v4di ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v8hi ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v4si ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v2di ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v32hi_mask ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrdv_v16si_mask ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrdv_v8di_mask ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrdv_v16hi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v8si_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v4di_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v8hi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v4si_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v2di_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v32hi_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrdv_v16si_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrdv_v8di_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrdv_v16hi_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v8si_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v4di_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v8hi_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v4si_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v2di_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v32hi ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshldv_v16si ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshldv_v8di ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshldv_v16hi ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v8si ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v4di ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v8hi ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v4si ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v2di ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v32hi_mask ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshldv_v16si_mask ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshldv_v8di_mask ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshldv_v16hi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v8si_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v4di_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v8hi_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v4si_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v2di_mask ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v32hi_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshldv_v16si_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshldv_v8di_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshldv_v16hi_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v8si_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v4di_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v8hi_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v4si_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v2di_maskz_1 ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpdpbusd_v16si (TARGET_AVX512VNNI && TARGET_EVEX512)
#define HAVE_vpdpbusd_v8si ((TARGET_AVXVNNI || (TARGET_AVX512VNNI && TARGET_AVX512VL)) && (TARGET_AVX2))
#define HAVE_vpdpbusd_v4si (TARGET_AVXVNNI || (TARGET_AVX512VNNI && TARGET_AVX512VL))
#define HAVE_vpdpbusd_v16si_mask ((TARGET_AVX512VNNI) && (TARGET_EVEX512))
#define HAVE_vpdpbusd_v8si_mask ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpbusd_v4si_mask ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpbusd_v16si_maskz_1 ((TARGET_AVX512VNNI) && (TARGET_EVEX512))
#define HAVE_vpdpbusd_v8si_maskz_1 ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpbusd_v4si_maskz_1 ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpbusds_v16si (TARGET_AVX512VNNI && TARGET_EVEX512)
#define HAVE_vpdpbusds_v8si ((TARGET_AVXVNNI || (TARGET_AVX512VNNI && TARGET_AVX512VL)) && (TARGET_AVX2))
#define HAVE_vpdpbusds_v4si (TARGET_AVXVNNI || (TARGET_AVX512VNNI && TARGET_AVX512VL))
#define HAVE_vpdpbusds_v16si_mask ((TARGET_AVX512VNNI) && (TARGET_EVEX512))
#define HAVE_vpdpbusds_v8si_mask ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpbusds_v4si_mask ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpbusds_v16si_maskz_1 ((TARGET_AVX512VNNI) && (TARGET_EVEX512))
#define HAVE_vpdpbusds_v8si_maskz_1 ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpbusds_v4si_maskz_1 ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpwssd_v16si (TARGET_AVX512VNNI && TARGET_EVEX512)
#define HAVE_vpdpwssd_v8si ((TARGET_AVXVNNI || (TARGET_AVX512VNNI && TARGET_AVX512VL)) && (TARGET_AVX2))
#define HAVE_vpdpwssd_v4si (TARGET_AVXVNNI || (TARGET_AVX512VNNI && TARGET_AVX512VL))
#define HAVE_vpdpwssd_v16si_mask ((TARGET_AVX512VNNI) && (TARGET_EVEX512))
#define HAVE_vpdpwssd_v8si_mask ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpwssd_v4si_mask ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpwssd_v16si_maskz_1 ((TARGET_AVX512VNNI) && (TARGET_EVEX512))
#define HAVE_vpdpwssd_v8si_maskz_1 ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpwssd_v4si_maskz_1 ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpwssds_v16si (TARGET_AVX512VNNI && TARGET_EVEX512)
#define HAVE_vpdpwssds_v8si ((TARGET_AVXVNNI || (TARGET_AVX512VNNI && TARGET_AVX512VL)) && (TARGET_AVX2))
#define HAVE_vpdpwssds_v4si (TARGET_AVXVNNI || (TARGET_AVX512VNNI && TARGET_AVX512VL))
#define HAVE_vpdpwssds_v16si_mask ((TARGET_AVX512VNNI) && (TARGET_EVEX512))
#define HAVE_vpdpwssds_v8si_mask ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpwssds_v4si_mask ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpwssds_v16si_maskz_1 ((TARGET_AVX512VNNI) && (TARGET_EVEX512))
#define HAVE_vpdpwssds_v8si_maskz_1 ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpwssds_v4si_maskz_1 ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vaesdec_v32qi (TARGET_VAES)
#define HAVE_vaesdec_v16qi ((TARGET_VAES) && (TARGET_AVX512VL))
#define HAVE_vaesdec_v64qi ((TARGET_VAES) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vaesdeclast_v32qi (TARGET_VAES)
#define HAVE_vaesdeclast_v16qi ((TARGET_VAES) && (TARGET_AVX512VL))
#define HAVE_vaesdeclast_v64qi ((TARGET_VAES) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vaesenc_v32qi (TARGET_VAES)
#define HAVE_vaesenc_v16qi ((TARGET_VAES) && (TARGET_AVX512VL))
#define HAVE_vaesenc_v64qi ((TARGET_VAES) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vaesenclast_v32qi (TARGET_VAES)
#define HAVE_vaesenclast_v16qi ((TARGET_VAES) && (TARGET_AVX512VL))
#define HAVE_vaesenclast_v64qi ((TARGET_VAES) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vpclmulqdq_v8di ((TARGET_VPCLMULQDQ) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vpclmulqdq_v4di (TARGET_VPCLMULQDQ)
#define HAVE_vpclmulqdq_v2di ((TARGET_VPCLMULQDQ) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpshufbitqmbv64qi ((TARGET_AVX512BITALG) && (TARGET_EVEX512))
#define HAVE_avx512vl_vpshufbitqmbv64qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BITALG) && (TARGET_EVEX512)))
#define HAVE_avx512vl_vpshufbitqmbv16qi ((TARGET_AVX512BITALG) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpshufbitqmbv16qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BITALG) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_vpshufbitqmbv32qi ((TARGET_AVX512BITALG) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpshufbitqmbv32qi_mask ((TARGET_AVX512F) && ((TARGET_AVX512BITALG) && (TARGET_AVX512VL)))
#define HAVE_avx512vp2intersect_2intersectv8di ((TARGET_AVX512VP2INTERSECT) && (TARGET_EVEX512))
#define HAVE_avx512vp2intersect_2intersectv4di ((TARGET_AVX512VP2INTERSECT) && (TARGET_AVX512VL))
#define HAVE_avx512vp2intersect_2intersectv2di ((TARGET_AVX512VP2INTERSECT) && (TARGET_AVX512VL))
#define HAVE_avx512vp2intersect_2intersectv8si ((TARGET_AVX512VP2INTERSECT) && (TARGET_AVX512VL))
#define HAVE_avx512vp2intersect_2intersectv4si ((TARGET_AVX512VP2INTERSECT) && (TARGET_AVX512VL))
#define HAVE_avx512vp2intersect_2intersectv16si (TARGET_AVX512VP2INTERSECT && TARGET_EVEX512)
#define HAVE_avx512f_cvtne2ps2bf16_v32bf ((TARGET_AVX512BF16) && (TARGET_EVEX512))
#define HAVE_avx512f_cvtne2ps2bf16_v32bf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BF16) && (TARGET_EVEX512)))
#define HAVE_avx512f_cvtne2ps2bf16_v16bf ((TARGET_AVX512BF16) && (TARGET_AVX512VL))
#define HAVE_avx512f_cvtne2ps2bf16_v16bf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BF16) && (TARGET_AVX512VL)))
#define HAVE_avx512f_cvtne2ps2bf16_v8bf ((TARGET_AVX512BF16) && (TARGET_AVX512VL))
#define HAVE_avx512f_cvtne2ps2bf16_v8bf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BF16) && (TARGET_AVX512VL)))
#define HAVE_avx512f_cvtneps2bf16_v4sf_mask_1 (TARGET_AVX512BF16 && TARGET_AVX512VL)
#define HAVE_vcvtneps2bf16_v8sf (TARGET_AVXNECONVERT || (TARGET_AVX512BF16 && TARGET_AVX512VL))
#define HAVE_avx512f_cvtneps2bf16_v16sf ((TARGET_AVX512BF16) && (TARGET_EVEX512))
#define HAVE_avx512f_cvtneps2bf16_v16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BF16) && (TARGET_EVEX512)))
#define HAVE_avx512f_cvtneps2bf16_v8sf ((TARGET_AVX512BF16) && (TARGET_AVX512VL))
#define HAVE_avx512f_cvtneps2bf16_v8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX512BF16) && (TARGET_AVX512VL)))
#define HAVE_avx512f_dpbf16ps_v16sf ((TARGET_AVX512BF16) && (TARGET_EVEX512))
#define HAVE_avx512f_dpbf16ps_v16sf_maskz_1 ((TARGET_AVX512BF16) && (TARGET_EVEX512))
#define HAVE_avx512f_dpbf16ps_v8sf ((TARGET_AVX512BF16) && (TARGET_AVX512VL))
#define HAVE_avx512f_dpbf16ps_v8sf_maskz_1 ((TARGET_AVX512BF16) && (TARGET_AVX512VL))
#define HAVE_avx512f_dpbf16ps_v4sf ((TARGET_AVX512BF16) && (TARGET_AVX512VL))
#define HAVE_avx512f_dpbf16ps_v4sf_maskz_1 ((TARGET_AVX512BF16) && (TARGET_AVX512VL))
#define HAVE_avx512f_dpbf16ps_v16sf_mask ((TARGET_AVX512BF16) && (TARGET_EVEX512))
#define HAVE_avx512f_dpbf16ps_v8sf_mask ((TARGET_AVX512BF16) && (TARGET_AVX512VL))
#define HAVE_avx512f_dpbf16ps_v4sf_mask ((TARGET_AVX512BF16) && (TARGET_AVX512VL))
#define HAVE_loadiwkey (TARGET_KL)
#define HAVE_aesdec128klu8 (TARGET_KL)
#define HAVE_aesdec256klu8 (TARGET_KL)
#define HAVE_aesenc128klu8 (TARGET_KL)
#define HAVE_aesenc256klu8 (TARGET_KL)
#define HAVE_vpdpbssd_v8si ((TARGET_AVXVNNIINT8 || TARGET_AVX10_2) && (TARGET_AVX))
#define HAVE_vpdpbssds_v8si ((TARGET_AVXVNNIINT8 || TARGET_AVX10_2) && (TARGET_AVX))
#define HAVE_vpdpbsud_v8si ((TARGET_AVXVNNIINT8 || TARGET_AVX10_2) && (TARGET_AVX))
#define HAVE_vpdpbsuds_v8si ((TARGET_AVXVNNIINT8 || TARGET_AVX10_2) && (TARGET_AVX))
#define HAVE_vpdpbuud_v8si ((TARGET_AVXVNNIINT8 || TARGET_AVX10_2) && (TARGET_AVX))
#define HAVE_vpdpbuuds_v8si ((TARGET_AVXVNNIINT8 || TARGET_AVX10_2) && (TARGET_AVX))
#define HAVE_vpdpbssd_v4si (TARGET_AVXVNNIINT8 || TARGET_AVX10_2)
#define HAVE_vpdpbssds_v4si (TARGET_AVXVNNIINT8 || TARGET_AVX10_2)
#define HAVE_vpdpbsud_v4si (TARGET_AVXVNNIINT8 || TARGET_AVX10_2)
#define HAVE_vpdpbsuds_v4si (TARGET_AVXVNNIINT8 || TARGET_AVX10_2)
#define HAVE_vpdpbuud_v4si (TARGET_AVXVNNIINT8 || TARGET_AVX10_2)
#define HAVE_vpdpbuuds_v4si (TARGET_AVXVNNIINT8 || TARGET_AVX10_2)
#define HAVE_vpdpbssd_v16si (TARGET_AVX10_2)
#define HAVE_vpdpbssds_v16si (TARGET_AVX10_2)
#define HAVE_vpdpbsud_v16si (TARGET_AVX10_2)
#define HAVE_vpdpbsuds_v16si (TARGET_AVX10_2)
#define HAVE_vpdpbuud_v16si (TARGET_AVX10_2)
#define HAVE_vpdpbuuds_v16si (TARGET_AVX10_2)
#define HAVE_vpdpbssd_v16si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbssds_v16si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbsud_v16si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbsuds_v16si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbuud_v16si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbuuds_v16si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbssd_v8si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbssds_v8si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbsud_v8si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbsuds_v8si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbuud_v8si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbuuds_v8si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbssd_v4si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbssds_v4si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbsud_v4si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbsuds_v4si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbuud_v4si_mask (TARGET_AVX10_2)
#define HAVE_vpdpbuuds_v4si_mask (TARGET_AVX10_2)
#define HAVE_vbcstnebf162ps_v8sf ((TARGET_AVXNECONVERT) && (TARGET_AVX))
#define HAVE_vbcstnebf162ps_v4sf (TARGET_AVXNECONVERT)
#define HAVE_vbcstnesh2ps_v8sf ((TARGET_AVXNECONVERT) && (TARGET_AVX))
#define HAVE_vbcstnesh2ps_v4sf (TARGET_AVXNECONVERT)
#define HAVE_vcvtneeph2ps_v8hf (TARGET_AVXNECONVERT)
#define HAVE_vcvtneebf162ps_v8bf (TARGET_AVXNECONVERT)
#define HAVE_vcvtneeph2ps_v16hf (TARGET_AVXNECONVERT)
#define HAVE_vcvtneebf162ps_v16bf (TARGET_AVXNECONVERT)
#define HAVE_vcvtneoph2ps_v8hf (TARGET_AVXNECONVERT)
#define HAVE_vcvtneobf162ps_v8bf (TARGET_AVXNECONVERT)
#define HAVE_vcvtneoph2ps_v16hf (TARGET_AVXNECONVERT)
#define HAVE_vcvtneobf162ps_v16bf (TARGET_AVXNECONVERT)
#define HAVE_avx10_2_cvt2ps2phx_v32hf ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvt2ps2phx_v32hf_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvt2ps2phx_v32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvt2ps2phx_v32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_cvt2ps2phx_v16hf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvt2ps2phx_v16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvt2ps2phx_v8hf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvt2ps2phx_v8hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_vcvt2ph2bf8v32hf (TARGET_AVX10_2)
#define HAVE_vcvt2ph2bf8v32hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvt2ph2bf8sv32hf (TARGET_AVX10_2)
#define HAVE_vcvt2ph2bf8sv32hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvt2ph2hf8v32hf (TARGET_AVX10_2)
#define HAVE_vcvt2ph2hf8v32hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvt2ph2hf8sv32hf (TARGET_AVX10_2)
#define HAVE_vcvt2ph2hf8sv32hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvt2ph2bf8v16hf (TARGET_AVX10_2)
#define HAVE_vcvt2ph2bf8v16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvt2ph2bf8sv16hf (TARGET_AVX10_2)
#define HAVE_vcvt2ph2bf8sv16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvt2ph2hf8v16hf (TARGET_AVX10_2)
#define HAVE_vcvt2ph2hf8v16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvt2ph2hf8sv16hf (TARGET_AVX10_2)
#define HAVE_vcvt2ph2hf8sv16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvt2ph2bf8v8hf (TARGET_AVX10_2)
#define HAVE_vcvt2ph2bf8v8hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvt2ph2bf8sv8hf (TARGET_AVX10_2)
#define HAVE_vcvt2ph2bf8sv8hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvt2ph2hf8v8hf (TARGET_AVX10_2)
#define HAVE_vcvt2ph2hf8v8hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvt2ph2hf8sv8hf (TARGET_AVX10_2)
#define HAVE_vcvt2ph2hf8sv8hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtbiasph2bf8v32hf (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2bf8v32hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtbiasph2bf8sv32hf (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2bf8sv32hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtbiasph2hf8v32hf (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2hf8v32hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtbiasph2hf8sv32hf (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2hf8sv32hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtbiasph2bf8v16hf (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2bf8v16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtbiasph2bf8sv16hf (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2bf8sv16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtbiasph2hf8v16hf (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2hf8v16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtbiasph2hf8sv16hf (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2hf8sv16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtph2bf8v16hf (TARGET_AVX10_2)
#define HAVE_vcvtph2bf8v16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtph2bf8sv16hf (TARGET_AVX10_2)
#define HAVE_vcvtph2bf8sv16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtph2hf8v16hf (TARGET_AVX10_2)
#define HAVE_vcvtph2hf8v16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtph2hf8sv16hf (TARGET_AVX10_2)
#define HAVE_vcvtph2hf8sv16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtph2bf8v32hf (TARGET_AVX10_2)
#define HAVE_vcvtph2bf8v32hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtph2bf8sv32hf (TARGET_AVX10_2)
#define HAVE_vcvtph2bf8sv32hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtph2hf8v32hf (TARGET_AVX10_2)
#define HAVE_vcvtph2hf8v32hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvtph2hf8sv32hf (TARGET_AVX10_2)
#define HAVE_vcvtph2hf8sv32hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvthf82phv32hf (TARGET_AVX10_2)
#define HAVE_vcvthf82phv32hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvthf82phv16hf (TARGET_AVX10_2)
#define HAVE_vcvthf82phv16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vcvthf82phv8hf (TARGET_AVX10_2)
#define HAVE_vcvthf82phv8hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_vpdpwusd_v8si ((TARGET_AVXVNNIINT16 || TARGET_AVX10_2) && (TARGET_AVX))
#define HAVE_vpdpwusds_v8si ((TARGET_AVXVNNIINT16 || TARGET_AVX10_2) && (TARGET_AVX))
#define HAVE_vpdpwsud_v8si ((TARGET_AVXVNNIINT16 || TARGET_AVX10_2) && (TARGET_AVX))
#define HAVE_vpdpwsuds_v8si ((TARGET_AVXVNNIINT16 || TARGET_AVX10_2) && (TARGET_AVX))
#define HAVE_vpdpwuud_v8si ((TARGET_AVXVNNIINT16 || TARGET_AVX10_2) && (TARGET_AVX))
#define HAVE_vpdpwuuds_v8si ((TARGET_AVXVNNIINT16 || TARGET_AVX10_2) && (TARGET_AVX))
#define HAVE_vpdpwusd_v4si (TARGET_AVXVNNIINT16 || TARGET_AVX10_2)
#define HAVE_vpdpwusds_v4si (TARGET_AVXVNNIINT16 || TARGET_AVX10_2)
#define HAVE_vpdpwsud_v4si (TARGET_AVXVNNIINT16 || TARGET_AVX10_2)
#define HAVE_vpdpwsuds_v4si (TARGET_AVXVNNIINT16 || TARGET_AVX10_2)
#define HAVE_vpdpwuud_v4si (TARGET_AVXVNNIINT16 || TARGET_AVX10_2)
#define HAVE_vpdpwuuds_v4si (TARGET_AVXVNNIINT16 || TARGET_AVX10_2)
#define HAVE_vpdpwusd_v16si (TARGET_AVX10_2)
#define HAVE_vpdpwusds_v16si (TARGET_AVX10_2)
#define HAVE_vpdpwsud_v16si (TARGET_AVX10_2)
#define HAVE_vpdpwsuds_v16si (TARGET_AVX10_2)
#define HAVE_vpdpwuud_v16si (TARGET_AVX10_2)
#define HAVE_vpdpwuuds_v16si (TARGET_AVX10_2)
#define HAVE_vpdpwusd_v16si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwusds_v16si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwsud_v16si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwsuds_v16si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwuud_v16si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwuuds_v16si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwusd_v8si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwusds_v8si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwsud_v8si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwsuds_v8si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwuud_v8si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwuuds_v8si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwusd_v4si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwusds_v4si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwsud_v4si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwsuds_v4si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwuud_v4si_mask (TARGET_AVX10_2)
#define HAVE_vpdpwuuds_v4si_mask (TARGET_AVX10_2)
#define HAVE_vdpphps_v16sf (TARGET_AVX10_2)
#define HAVE_vdpphps_v8sf (TARGET_AVX10_2)
#define HAVE_vdpphps_v4sf (TARGET_AVX10_2)
#define HAVE_vdpphps_v16sf_mask (TARGET_AVX10_2)
#define HAVE_vdpphps_v8sf_mask (TARGET_AVX10_2)
#define HAVE_vdpphps_v4sf_mask (TARGET_AVX10_2)
#define HAVE_vdpphps_v16sf_maskz_1 (TARGET_AVX10_2)
#define HAVE_vdpphps_v8sf_maskz_1 (TARGET_AVX10_2)
#define HAVE_vdpphps_v4sf_maskz_1 (TARGET_AVX10_2)
#define HAVE_avx10_2_scalefbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_scalefbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_scalefbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_scalefbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_scalefbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_scalefbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_smaxbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_smaxbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_sminbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_sminbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_smaxbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_smaxbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_sminbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_sminbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_smaxbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_smaxbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_sminbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_sminbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_addbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_addbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_subbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_subbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_mulbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_mulbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_divbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_divbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_addbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_addbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_subbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_subbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_mulbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_mulbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_divbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_divbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_addbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_addbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_subbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_subbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_mulbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_mulbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_divbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_divbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_fmaddbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fmaddbf16_v32bf_maskz_1 (TARGET_AVX10_2)
#define HAVE_avx10_2_fmaddbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fmaddbf16_v16bf_maskz_1 (TARGET_AVX10_2)
#define HAVE_avx10_2_fmaddbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fmaddbf16_v8bf_maskz_1 (TARGET_AVX10_2)
#define HAVE_avx10_2_fmaddbf16_v32bf_mask (TARGET_AVX10_2)
#define HAVE_avx10_2_fmaddbf16_v16bf_mask (TARGET_AVX10_2)
#define HAVE_avx10_2_fmaddbf16_v8bf_mask (TARGET_AVX10_2)
#define HAVE_avx10_2_fmaddbf16_v32bf_mask3 (TARGET_AVX10_2)
#define HAVE_avx10_2_fmaddbf16_v16bf_mask3 (TARGET_AVX10_2)
#define HAVE_avx10_2_fmaddbf16_v8bf_mask3 (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v32bf_maskz_1 (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v16bf_maskz_1 (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v8bf_maskz_1 (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v32bf_mask (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v16bf_mask (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v8bf_mask (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v32bf_mask3 (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v16bf_mask3 (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v8bf_mask3 (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v32bf_maskz_1 (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v16bf_maskz_1 (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v8bf_maskz_1 (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v32bf_mask (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v16bf_mask (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v8bf_mask (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v32bf_mask3 (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v16bf_mask3 (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v8bf_mask3 (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v32bf_maskz_1 (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v16bf_maskz_1 (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v8bf_maskz_1 (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v32bf_mask (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v16bf_mask (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v8bf_mask (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v32bf_mask3 (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v16bf_mask3 (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v8bf_mask3 (TARGET_AVX10_2)
#define HAVE_avx10_2_rsqrtbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_rsqrtbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_rsqrtbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_rsqrtbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_rsqrtbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_rsqrtbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_sqrtbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_sqrtbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_sqrtbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_sqrtbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_sqrtbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_sqrtbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_rcpbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_rcpbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_rcpbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_rcpbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_rcpbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_rcpbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_getexpbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_getexpbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_getexpbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_getexpbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_getexpbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_getexpbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_rndscalebf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_rndscalebf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_reducebf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_reducebf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_getmantbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_getmantbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_rndscalebf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_rndscalebf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_reducebf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_reducebf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_getmantbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_getmantbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_rndscalebf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_rndscalebf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_reducebf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_reducebf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_getmantbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_getmantbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_fpclassbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fpclassbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_fpclassbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fpclassbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_fpclassbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_fpclassbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cmpbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cmpbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cmpbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cmpbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cmpbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cmpbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvtbf162ibsv32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cvtbf162ibsv32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvtbf162iubsv32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cvtbf162iubsv32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvttbf162ibsv32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cvttbf162ibsv32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvttbf162iubsv32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cvttbf162iubsv32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvtbf162ibsv16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cvtbf162ibsv16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvtbf162iubsv16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cvtbf162iubsv16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvttbf162ibsv16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cvttbf162ibsv16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvttbf162iubsv16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cvttbf162iubsv16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvtbf162ibsv8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cvtbf162ibsv8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvtbf162iubsv8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cvtbf162iubsv8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvttbf162ibsv8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cvttbf162ibsv8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvttbf162iubsv8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_cvttbf162iubsv8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvtph2ibsv32hf ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvtph2ibsv32hf_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvtph2ibsv32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvtph2ibsv32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_cvtph2iubsv32hf ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvtph2iubsv32hf_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvtph2iubsv32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvtph2iubsv32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_cvtph2ibsv16hf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvtph2ibsv16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvtph2iubsv16hf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvtph2iubsv16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvtph2ibsv8hf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvtph2ibsv8hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvtph2iubsv8hf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvtph2iubsv8hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvttph2ibsv32hf ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvttph2ibsv32hf_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V32HFmode == V16SFmode \
									      || V32HFmode == V8DFmode \
									      || V32HFmode == V8DImode \
									      || V32HFmode == V16SImode \
									      || V32HFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvttph2ibsv32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvttph2ibsv32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V32HFmode == V16SFmode \
									      || V32HFmode == V8DFmode \
									      || V32HFmode == V8DImode \
									      || V32HFmode == V16SImode \
									      || V32HFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_cvttph2iubsv32hf ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvttph2iubsv32hf_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V32HFmode == V16SFmode \
									      || V32HFmode == V8DFmode \
									      || V32HFmode == V8DImode \
									      || V32HFmode == V16SImode \
									      || V32HFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvttph2iubsv32hf_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvttph2iubsv32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V32HFmode == V16SFmode \
									      || V32HFmode == V8DFmode \
									      || V32HFmode == V8DImode \
									      || V32HFmode == V16SImode \
									      || V32HFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_cvttph2ibsv16hf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvttph2ibsv16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvttph2iubsv16hf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvttph2iubsv16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvttph2ibsv8hf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvttph2ibsv8hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvttph2iubsv8hf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvttph2iubsv8hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvtps2ibsv16sf ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvtps2ibsv16sf_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvtps2ibsv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvtps2ibsv16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_cvtps2iubsv16sf ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvtps2iubsv16sf_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvtps2iubsv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvtps2iubsv16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_cvtps2ibsv8sf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvtps2ibsv8sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvtps2iubsv8sf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvtps2iubsv8sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvtps2ibsv4sf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvtps2ibsv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvtps2iubsv4sf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvtps2iubsv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvttps2ibsv16sf ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvttps2ibsv16sf_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvttps2ibsv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvttps2ibsv16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_cvttps2iubsv16sf ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_cvttps2iubsv16sf_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvttps2iubsv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_cvttps2iubsv16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_cvttps2ibsv8sf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvttps2ibsv8sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvttps2iubsv8sf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvttps2iubsv8sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvttps2ibsv4sf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvttps2ibsv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_cvttps2iubsv4sf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_cvttps2iubsv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttps2dqsv16sf ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vcvttps2dqsv16sf_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttps2dqsv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttps2dqsv16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_vcvttps2udqsv16sf ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vcvttps2udqsv16sf_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttps2udqsv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttps2udqsv16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_vcvttps2dqsv8sf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttps2dqsv8sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttps2udqsv8sf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttps2udqsv8sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttps2dqsv4sf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttps2dqsv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttps2udqsv4sf (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttps2udqsv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttpd2dqsv8df ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vcvttpd2dqsv8df_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttpd2dqsv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttpd2dqsv8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_vcvttpd2udqsv8df ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vcvttpd2udqsv8df_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttpd2udqsv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttpd2udqsv8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_vcvttpd2dqsv4df (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttpd2dqsv4df_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttpd2udqsv4df (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttpd2udqsv4df_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttpd2dqsv2df (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttpd2dqsv2df_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttpd2udqsv2df (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttpd2udqsv2df_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttpd2qqsv8df ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vcvttpd2qqsv8df_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttpd2qqsv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttpd2qqsv8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_vcvttpd2uqqsv8df ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vcvttpd2uqqsv8df_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttpd2uqqsv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttpd2uqqsv8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_vcvttpd2qqsv4df (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttpd2qqsv4df_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttpd2uqqsv4df (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttpd2uqqsv4df_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttpd2qqsv2df (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttpd2qqsv2df_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttpd2uqqsv2df (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttpd2uqqsv2df_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttps2qqsv8di ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vcvttps2qqsv8di_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V8DImode == V16SFmode \
									      || V8DImode == V8DFmode \
									      || V8DImode == V8DImode \
									      || V8DImode == V16SImode \
									      || V8DImode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttps2qqsv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttps2qqsv8di_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V8DImode == V16SFmode \
									      || V8DImode == V8DFmode \
									      || V8DImode == V8DImode \
									      || V8DImode == V16SImode \
									      || V8DImode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_vcvttps2uqqsv8di ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vcvttps2uqqsv8di_round ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V8DImode == V16SFmode \
									      || V8DImode == V8DFmode \
									      || V8DImode == V8DImode \
									      || V8DImode == V16SImode \
									      || V8DImode == V32HFmode)) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttps2uqqsv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && 1) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vcvttps2uqqsv8di_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2 && (V8DImode == V16SFmode \
									      || V8DImode == V8DFmode \
									      || V8DImode == V8DImode \
									      || V8DImode == V16SImode \
									      || V8DImode == V32HFmode)) && (TARGET_AVX10_2))))
#define HAVE_avx10_2_vcvttps2qqsv4di (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttps2qqsv4di_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttps2uqqsv4di (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttps2uqqsv4di_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttps2qqsv2di (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttps2qqsv2di_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttps2uqqsv2di (TARGET_AVX10_2 && 1)
#define HAVE_avx10_2_vcvttps2uqqsv2di_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && 1))
#define HAVE_avx10_2_vcvttsd2sissi (TARGET_AVX10_2)
#define HAVE_avx10_2_vcvttsd2sissi_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vcvttsd2usissi (TARGET_AVX10_2)
#define HAVE_avx10_2_vcvttsd2usissi_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vcvttss2sissi (TARGET_AVX10_2)
#define HAVE_avx10_2_vcvttss2sissi_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vcvttss2usissi (TARGET_AVX10_2)
#define HAVE_avx10_2_vcvttss2usissi_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxbf16_v32bf (TARGET_AVX10_2)
#define HAVE_avx10_2_minmaxbf16_v32bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxbf16_v16bf (TARGET_AVX10_2)
#define HAVE_avx10_2_minmaxbf16_v16bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxbf16_v8bf (TARGET_AVX10_2)
#define HAVE_avx10_2_minmaxbf16_v8bf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv32hf (TARGET_AVX10_2)
#define HAVE_avx10_2_minmaxpv32hf_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv32hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv32hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_minmaxpv16hf (TARGET_AVX10_2)
#define HAVE_avx10_2_minmaxpv16hf_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv16hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv16hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_minmaxpv8hf (TARGET_AVX10_2)
#define HAVE_avx10_2_minmaxpv8hf_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv8hf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_minmaxpv16sf (TARGET_AVX10_2)
#define HAVE_avx10_2_minmaxpv16sf_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv16sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv16sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_minmaxpv8sf (TARGET_AVX10_2)
#define HAVE_avx10_2_minmaxpv8sf_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv8sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv8sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_minmaxpv4sf (TARGET_AVX10_2)
#define HAVE_avx10_2_minmaxpv4sf_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv4sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_minmaxpv8df (TARGET_AVX10_2)
#define HAVE_avx10_2_minmaxpv8df_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv8df_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv8df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_minmaxpv4df (TARGET_AVX10_2)
#define HAVE_avx10_2_minmaxpv4df_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv4df_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv4df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_minmaxpv2df (TARGET_AVX10_2)
#define HAVE_avx10_2_minmaxpv2df_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv2df_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxpv2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_minmaxsv8hf ((TARGET_AVX10_2) && (TARGET_AVX512FP16))
#define HAVE_avx10_2_minmaxsv8hf_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2) && (TARGET_AVX512FP16)))
#define HAVE_avx10_2_minmaxsv8hf_round ((TARGET_AVX512F) && ((TARGET_AVX10_2) && (TARGET_AVX512FP16)))
#define HAVE_avx10_2_minmaxsv8hf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2) && (TARGET_AVX512FP16))))
#define HAVE_avx10_2_minmaxsv4sf (TARGET_AVX10_2)
#define HAVE_avx10_2_minmaxsv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxsv4sf_round ((TARGET_AVX512F) && (TARGET_AVX10_2))
#define HAVE_avx10_2_minmaxsv4sf_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_minmaxsv2df ((TARGET_AVX10_2) && (TARGET_SSE2))
#define HAVE_avx10_2_minmaxsv2df_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2) && (TARGET_SSE2)))
#define HAVE_avx10_2_minmaxsv2df_round ((TARGET_AVX512F) && ((TARGET_AVX10_2) && (TARGET_SSE2)))
#define HAVE_avx10_2_minmaxsv2df_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_AVX10_2) && (TARGET_SSE2))))
#define HAVE_avx10_2_vmovrsbv64qi ((TARGET_AVX10_2 && TARGET_MOVRS) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vmovrsbv64qi_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && TARGET_MOVRS) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vmovrsbv32qi (TARGET_AVX10_2 && TARGET_MOVRS)
#define HAVE_avx10_2_vmovrsbv32qi_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && TARGET_MOVRS))
#define HAVE_avx10_2_vmovrsbv16qi (TARGET_AVX10_2 && TARGET_MOVRS)
#define HAVE_avx10_2_vmovrsbv16qi_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && TARGET_MOVRS))
#define HAVE_avx10_2_vmovrswv32hi ((TARGET_AVX10_2 && TARGET_MOVRS) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vmovrswv32hi_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && TARGET_MOVRS) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vmovrswv16hi (TARGET_AVX10_2 && TARGET_MOVRS)
#define HAVE_avx10_2_vmovrswv16hi_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && TARGET_MOVRS))
#define HAVE_avx10_2_vmovrswv8hi (TARGET_AVX10_2 && TARGET_MOVRS)
#define HAVE_avx10_2_vmovrswv8hi_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && TARGET_MOVRS))
#define HAVE_avx10_2_vmovrsdv16si ((TARGET_AVX10_2 && TARGET_MOVRS) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vmovrsdv16si_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && TARGET_MOVRS) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vmovrsdv8si (TARGET_AVX10_2 && TARGET_MOVRS)
#define HAVE_avx10_2_vmovrsdv8si_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && TARGET_MOVRS))
#define HAVE_avx10_2_vmovrsdv4si (TARGET_AVX10_2 && TARGET_MOVRS)
#define HAVE_avx10_2_vmovrsdv4si_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && TARGET_MOVRS))
#define HAVE_avx10_2_vmovrsqv8di ((TARGET_AVX10_2 && TARGET_MOVRS) && (TARGET_AVX10_2))
#define HAVE_avx10_2_vmovrsqv8di_mask ((TARGET_AVX512F) && ((TARGET_AVX10_2 && TARGET_MOVRS) && (TARGET_AVX10_2)))
#define HAVE_avx10_2_vmovrsqv4di (TARGET_AVX10_2 && TARGET_MOVRS)
#define HAVE_avx10_2_vmovrsqv4di_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && TARGET_MOVRS))
#define HAVE_avx10_2_vmovrsqv2di (TARGET_AVX10_2 && TARGET_MOVRS)
#define HAVE_avx10_2_vmovrsqv2di_mask ((TARGET_AVX512F) && (TARGET_AVX10_2 && TARGET_MOVRS))
#define HAVE_mfence_sse2 (TARGET_64BIT || TARGET_SSE2)
#define HAVE_mfence_nosse 1
#define HAVE_atomic_loaddi_fpu (!TARGET_64BIT && (TARGET_80387 || TARGET_SSE))
#define HAVE_atomic_storeqi_1 1
#define HAVE_atomic_storehi_1 1
#define HAVE_atomic_storesi_1 1
#define HAVE_atomic_storedi_fpu (!TARGET_64BIT && (TARGET_80387 || TARGET_SSE))
#define HAVE_loaddi_via_fpu (TARGET_80387)
#define HAVE_storedi_via_fpu (TARGET_80387)
#define HAVE_loaddi_via_sse (TARGET_SSE)
#define HAVE_storedi_via_sse (TARGET_SSE)
#define HAVE_atomic_compare_and_swapdi_doubleword ((TARGET_CMPXCHG8B) && (!TARGET_64BIT))
#define HAVE_atomic_compare_and_swapqi_1 (TARGET_CMPXCHG)
#define HAVE_atomic_compare_and_swaphi_1 (TARGET_CMPXCHG)
#define HAVE_atomic_compare_and_swapsi_1 (TARGET_CMPXCHG)
#define HAVE_atomic_fetch_addqi (TARGET_XADD)
#define HAVE_atomic_fetch_addhi (TARGET_XADD)
#define HAVE_atomic_fetch_addsi (TARGET_XADD)
#define HAVE_atomic_exchangeqi 1
#define HAVE_atomic_exchangehi 1
#define HAVE_atomic_exchangesi 1
#define HAVE_rao_aandsi (TARGET_RAOINT)
#define HAVE_rao_aorsi (TARGET_RAOINT)
#define HAVE_rao_axorsi (TARGET_RAOINT)
#define HAVE_rao_aaddsi (TARGET_RAOINT)
#define HAVE_atomic_addqi 1
#define HAVE_atomic_addhi 1
#define HAVE_atomic_addsi 1
#define HAVE_atomic_subqi 1
#define HAVE_atomic_subhi 1
#define HAVE_atomic_subsi 1
#define HAVE_atomic_andqi 1
#define HAVE_atomic_orqi 1
#define HAVE_atomic_xorqi 1
#define HAVE_atomic_andhi 1
#define HAVE_atomic_orhi 1
#define HAVE_atomic_xorhi 1
#define HAVE_atomic_andsi 1
#define HAVE_atomic_orsi 1
#define HAVE_atomic_xorsi 1
#define HAVE_atomic_bit_test_and_sethi_1 1
#define HAVE_atomic_bit_test_and_setsi_1 1
#define HAVE_atomic_bit_test_and_complementhi_1 1
#define HAVE_atomic_bit_test_and_complementsi_1 1
#define HAVE_atomic_bit_test_and_resethi_1 1
#define HAVE_atomic_bit_test_and_resetsi_1 1
#define HAVE_atomic_add_fetch_cmp_0qi_1 1
#define HAVE_atomic_add_fetch_cmp_0hi_1 1
#define HAVE_atomic_add_fetch_cmp_0si_1 1
#define HAVE_atomic_sub_fetch_cmp_0qi_1 1
#define HAVE_atomic_sub_fetch_cmp_0hi_1 1
#define HAVE_atomic_sub_fetch_cmp_0si_1 1
#define HAVE_atomic_and_fetch_cmp_0qi_1 1
#define HAVE_atomic_or_fetch_cmp_0qi_1 1
#define HAVE_atomic_xor_fetch_cmp_0qi_1 1
#define HAVE_atomic_and_fetch_cmp_0hi_1 1
#define HAVE_atomic_or_fetch_cmp_0hi_1 1
#define HAVE_atomic_xor_fetch_cmp_0hi_1 1
#define HAVE_atomic_and_fetch_cmp_0si_1 1
#define HAVE_atomic_or_fetch_cmp_0si_1 1
#define HAVE_atomic_xor_fetch_cmp_0si_1 1
#define HAVE_cbranchqi4 (TARGET_QIMODE_MATH)
#define HAVE_cbranchhi4 (TARGET_HIMODE_MATH)
#define HAVE_cbranchsi4 1
#define HAVE_cbranchdi4 1
#define HAVE_cbranchti4 (TARGET_64BIT || TARGET_SSE4_1)
#define HAVE_cbranchoi4 (TARGET_AVX)
#define HAVE_cbranchxi4 (TARGET_AVX512F && TARGET_EVEX512 && !TARGET_PREFER_AVX256)
#define HAVE_cstoreqi4 (TARGET_QIMODE_MATH)
#define HAVE_cstorehi4 (TARGET_HIMODE_MATH)
#define HAVE_cstoresi4 1
#define HAVE_cstoredi4 1
#define HAVE_cmpqi_1 1
#define HAVE_cmphi_1 1
#define HAVE_cmpsi_1 1
#define HAVE_cmpqi_ext_3 1
#define HAVE_cbranchxf4 (TARGET_80387)
#define HAVE_cstorexf4 (TARGET_80387)
#define HAVE_cbranchhf4 (TARGET_AVX512FP16)
#define HAVE_cbranchsf4 (TARGET_80387 || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_cbranchdf4 (TARGET_80387 || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH))
#define HAVE_cbranchbf4 (TARGET_80387 || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_cstorehf4 (TARGET_AVX512FP16)
#define HAVE_cstorebf4 (TARGET_80387 || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_cstoresf4 (TARGET_80387 || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_cstoredf4 (TARGET_80387 || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH))
#define HAVE_cbranchcc4 1
#define HAVE_cstoreccgc4 1
#define HAVE_cstoreccgoc4 1
#define HAVE_cstoreccno4 1
#define HAVE_cstoreccgz4 1
#define HAVE_cstorecca4 1
#define HAVE_cstoreccc4 1
#define HAVE_cstorecco4 1
#define HAVE_cstoreccp4 1
#define HAVE_cstoreccs4 1
#define HAVE_cstoreccz4 1
#define HAVE_cstorecc4 1
#define HAVE_movxi (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_movoi (TARGET_AVX)
#define HAVE_movti (TARGET_64BIT || TARGET_SSE)
#define HAVE_movcdi 1
#define HAVE_movqi 1
#define HAVE_movhi 1
#define HAVE_movsi 1
#define HAVE_movdi 1
#define HAVE_movstrictqi 1
#define HAVE_movstricthi 1
#define HAVE_extvhi 1
#define HAVE_extvsi 1
#define HAVE_extzvhi 1
#define HAVE_extzvsi 1
#define HAVE_insvhi 1
#define HAVE_insvsi 1
#define HAVE_movtf (TARGET_64BIT || TARGET_SSE)
#define HAVE_movhf 1
#define HAVE_movsf 1
#define HAVE_movdf 1
#define HAVE_movxf 1
#define HAVE_movbf 1
#define HAVE_zero_extendsidi2 1
#define HAVE_zero_extendqisi2 1
#define HAVE_zero_extendhisi2 1
#define HAVE_zero_extendqihi2 1
#define HAVE_extendsidi2 1
#define HAVE_extendsfdf2 (TARGET_80387 || (TARGET_SSE2 && TARGET_SSE_MATH))
#define HAVE_extendhfsf2 (TARGET_AVX512FP16 || TARGET_F16C || TARGET_AVX512VL)
#define HAVE_extendhfdf2 (TARGET_AVX512FP16)
#define HAVE_extendbfsf2 (TARGET_SSE2 && !HONOR_NANS (BFmode))
#define HAVE_extendsfxf2 (TARGET_80387)
#define HAVE_extenddfxf2 (TARGET_80387)
#define HAVE_truncsfhf2 (TARGET_AVX512FP16 || TARGET_F16C || TARGET_AVX512VL)
#define HAVE_truncdfhf2 (TARGET_AVX512FP16)
#define HAVE_fix_truncxfdi2 (TARGET_80387)
#define HAVE_fix_truncsfdi2 (TARGET_80387 || (TARGET_64BIT && SSE_FLOAT_MODE_P (SFmode)))
#define HAVE_fix_truncdfdi2 (TARGET_80387 || (TARGET_64BIT && SSE_FLOAT_MODE_P (DFmode)))
#define HAVE_fix_truncxfsi2 (TARGET_80387)
#define HAVE_fix_truncsfsi2 (TARGET_80387 || SSE_FLOAT_MODE_P (SFmode))
#define HAVE_fix_truncdfsi2 (TARGET_80387 || SSE_FLOAT_MODE_P (DFmode))
#define HAVE_fix_truncsfhi2 (TARGET_80387 \
   && !(SSE_FLOAT_MODE_P (SFmode) && (!TARGET_FISTTP || TARGET_SSE_MATH)))
#define HAVE_fix_truncdfhi2 (TARGET_80387 \
   && !(SSE_FLOAT_MODE_P (DFmode) && (!TARGET_FISTTP || TARGET_SSE_MATH)))
#define HAVE_fix_truncxfhi2 (TARGET_80387 \
   && !(SSE_FLOAT_MODE_P (XFmode) && (!TARGET_FISTTP || TARGET_SSE_MATH)))
#define HAVE_fixuns_truncsfsi2 ((!TARGET_64BIT || TARGET_AVX512F) && TARGET_SSE2 && TARGET_SSE_MATH)
#define HAVE_fixuns_truncdfsi2 ((!TARGET_64BIT || TARGET_AVX512F) && TARGET_SSE2 && TARGET_SSE_MATH)
#define HAVE_fixuns_trunchfhi2 (TARGET_AVX512FP16)
#define HAVE_fixuns_truncsfhi2 (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH)
#define HAVE_fixuns_truncdfhi2 (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH)
#define HAVE_floatsisf2 ((TARGET_80387 && X87_ENABLE_FLOAT (SFmode, SImode)) \
   || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH \
       && ((SImode != DImode) || TARGET_64BIT)))
#define HAVE_floatdisf2 ((TARGET_80387 && X87_ENABLE_FLOAT (SFmode, DImode)) \
   || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH \
       && ((DImode != DImode) || TARGET_64BIT)))
#define HAVE_floatsidf2 ((TARGET_80387 && X87_ENABLE_FLOAT (DFmode, SImode)) \
   || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH \
       && ((SImode != DImode) || TARGET_64BIT)))
#define HAVE_floatdidf2 ((TARGET_80387 && X87_ENABLE_FLOAT (DFmode, DImode)) \
   || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH \
       && ((DImode != DImode) || TARGET_64BIT)))
#define HAVE_floatunsqisf2 (!TARGET_64BIT \
   && SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH)
#define HAVE_floatunshisf2 (!TARGET_64BIT \
   && SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH)
#define HAVE_floatunsqidf2 (!TARGET_64BIT \
   && SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH)
#define HAVE_floatunshidf2 (!TARGET_64BIT \
   && SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH)
#define HAVE_floatunssisf2 ((!TARGET_64BIT \
    && TARGET_80387 && X87_ENABLE_FLOAT (SFmode, DImode) \
    && TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC) \
   || ((!TARGET_64BIT || TARGET_AVX512F) \
       && SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_floatunssidf2 ((!TARGET_64BIT \
    && TARGET_80387 && X87_ENABLE_FLOAT (DFmode, DImode) \
    && TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC) \
   || ((!TARGET_64BIT || TARGET_AVX512F) \
       && SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH))
#define HAVE_floatunssixf2 ((!TARGET_64BIT \
    && TARGET_80387 && X87_ENABLE_FLOAT (XFmode, DImode) \
    && TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC) \
   || ((!TARGET_64BIT || TARGET_AVX512F) \
       && SSE_FLOAT_MODE_P (XFmode) && TARGET_SSE_MATH))
#define HAVE_addqi3 (TARGET_QIMODE_MATH)
#define HAVE_addhi3 (TARGET_HIMODE_MATH)
#define HAVE_addsi3 1
#define HAVE_adddi3 1
#define HAVE_addqi_ext_1 1
#define HAVE_addvqi4 1
#define HAVE_addvhi4 1
#define HAVE_addvsi4 1
#define HAVE_addvdi4 1
#define HAVE_uaddvqi4 1
#define HAVE_uaddvhi4 1
#define HAVE_uaddvsi4 1
#define HAVE_uaddvdi4 1
#define HAVE_subqi3 (TARGET_QIMODE_MATH)
#define HAVE_subhi3 (TARGET_HIMODE_MATH)
#define HAVE_subsi3 1
#define HAVE_subdi3 1
#define HAVE_subvqi4 1
#define HAVE_subvhi4 1
#define HAVE_subvsi4 1
#define HAVE_subvdi4 1
#define HAVE_usubvqi4 1
#define HAVE_usubvhi4 1
#define HAVE_usubvsi4 1
#define HAVE_subqi_3 1
#define HAVE_subhi_3 1
#define HAVE_subsi_3 1
#define HAVE_addcarrysi_0 (ix86_binary_operator_ok (PLUS, SImode, operands, TARGET_APX_NDD))
#define HAVE_addcarrydi_0 ((ix86_binary_operator_ok (PLUS, DImode, operands, TARGET_APX_NDD)) && (TARGET_64BIT))
#define HAVE_subborrowsi_0 (ix86_binary_operator_ok (MINUS, SImode, operands, TARGET_APX_NDD))
#define HAVE_subborrowdi_0 ((ix86_binary_operator_ok (MINUS, DImode, operands, TARGET_APX_NDD)) && (TARGET_64BIT))
#define HAVE_uaddcsi5 1
#define HAVE_usubcsi5 1
#define HAVE_addqi3_cconly_overflow (!(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_usaddqi3 1
#define HAVE_usaddhi3 1
#define HAVE_usaddsi3 1
#define HAVE_ussubqi3 1
#define HAVE_ussubhi3 1
#define HAVE_ussubsi3 1
#define HAVE_ustruncsiqi2 1
#define HAVE_ustruncsihi2 1
#define HAVE_ustrunchiqi2 1
#define HAVE_addxf3 (TARGET_80387)
#define HAVE_subxf3 (TARGET_80387)
#define HAVE_addhf3 (TARGET_AVX512FP16)
#define HAVE_subhf3 (TARGET_AVX512FP16)
#define HAVE_addsf3 ((TARGET_80387 && X87_ENABLE_ARITH (SFmode)) \
    || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_subsf3 ((TARGET_80387 && X87_ENABLE_ARITH (SFmode)) \
    || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_adddf3 ((TARGET_80387 && X87_ENABLE_ARITH (DFmode)) \
    || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH))
#define HAVE_subdf3 ((TARGET_80387 && X87_ENABLE_ARITH (DFmode)) \
    || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH))
#define HAVE_mulhi3 (TARGET_HIMODE_MATH)
#define HAVE_mulsi3 1
#define HAVE_mulqi3 (TARGET_QIMODE_MATH)
#define HAVE_mulvhi4 1
#define HAVE_mulvsi4 1
#define HAVE_umulvhi4 1
#define HAVE_umulvsi4 1
#define HAVE_mulvqi4 (TARGET_QIMODE_MATH)
#define HAVE_umulvqi4 (TARGET_QIMODE_MATH)
#define HAVE_mulsidi3 1
#define HAVE_umulsidi3 1
#define HAVE_mulqihi3 (TARGET_QIMODE_MATH)
#define HAVE_umulqihi3 (TARGET_QIMODE_MATH)
#define HAVE_mulxf3 (TARGET_80387)
#define HAVE_mulhf3 (TARGET_AVX512FP16)
#define HAVE_mulsf3 ((TARGET_80387 && X87_ENABLE_ARITH (SFmode)) \
    || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_muldf3 ((TARGET_80387 && X87_ENABLE_ARITH (DFmode)) \
    || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH))
#define HAVE_divxf3 (TARGET_80387)
#define HAVE_divhf3 (TARGET_AVX512FP16)
#define HAVE_divsf3 ((TARGET_80387 && X87_ENABLE_ARITH (SFmode)) \
    || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_divdf3 ((TARGET_80387 && X87_ENABLE_ARITH (DFmode)) \
    || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH))
#define HAVE_divmodhi4 (TARGET_HIMODE_MATH)
#define HAVE_udivmodhi4 (TARGET_HIMODE_MATH)
#define HAVE_divmodsi4 1
#define HAVE_udivmodsi4 1
#define HAVE_divmodqi4 (TARGET_QIMODE_MATH)
#define HAVE_udivmodqi4 (TARGET_QIMODE_MATH)
#define HAVE_testsi_ccno_1 1
#define HAVE_testqi_ccz_1 1
#define HAVE_testqi_ext_1_ccno 1
#define HAVE_andqi3 (TARGET_QIMODE_MATH)
#define HAVE_andhi3 (TARGET_HIMODE_MATH)
#define HAVE_andsi3 1
#define HAVE_anddi3 1
#define HAVE_andqi_ext_1 1
#define HAVE_iorqi3 (TARGET_QIMODE_MATH)
#define HAVE_xorqi3 (TARGET_QIMODE_MATH)
#define HAVE_iorhi3 (TARGET_HIMODE_MATH)
#define HAVE_xorhi3 (TARGET_HIMODE_MATH)
#define HAVE_iorsi3 1
#define HAVE_xorsi3 1
#define HAVE_iordi3 1
#define HAVE_xordi3 1
#define HAVE_xorqi_ext_1_cc 1
#define HAVE_negqi2 (TARGET_QIMODE_MATH)
#define HAVE_neghi2 (TARGET_HIMODE_MATH)
#define HAVE_negsi2 1
#define HAVE_negdi2 1
#define HAVE_x86_negsi_ccc 1
#define HAVE_negvqi3 1
#define HAVE_negvhi3 1
#define HAVE_negvsi3 1
#define HAVE_absqi2 ((TARGET_CMOVE \
   && (QImode != QImode || !TARGET_PARTIAL_REG_STALL)) && (TARGET_QIMODE_MATH))
#define HAVE_abshi2 ((TARGET_CMOVE \
   && (HImode != QImode || !TARGET_PARTIAL_REG_STALL)) && (TARGET_HIMODE_MATH))
#define HAVE_abssi2 (TARGET_CMOVE \
   && (SImode != QImode || !TARGET_PARTIAL_REG_STALL))
#define HAVE_absdi2 (TARGET_CMOVE \
   && (DImode != QImode || !TARGET_PARTIAL_REG_STALL))
#define HAVE_abstf2 (TARGET_SSE)
#define HAVE_negtf2 (TARGET_SSE)
#define HAVE_abshf2 (TARGET_AVX512FP16)
#define HAVE_neghf2 (TARGET_AVX512FP16)
#define HAVE_abssf2 (TARGET_80387 || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_negsf2 (TARGET_80387 || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_absdf2 (TARGET_80387 || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH))
#define HAVE_negdf2 (TARGET_80387 || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH))
#define HAVE_absxf2 (TARGET_80387 || (SSE_FLOAT_MODE_P (XFmode) && TARGET_SSE_MATH))
#define HAVE_negxf2 (TARGET_80387 || (SSE_FLOAT_MODE_P (XFmode) && TARGET_SSE_MATH))
#define HAVE_copysignhf3 ((SSE_FLOAT_MODE_P (HFmode) && TARGET_SSE_MATH) \
   || (TARGET_SSE && (HFmode == TFmode)) \
   || (TARGET_AVX512FP16 && (HFmode ==HFmode)))
#define HAVE_copysignsf3 ((SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
   || (TARGET_SSE && (SFmode == TFmode)) \
   || (TARGET_AVX512FP16 && (SFmode ==HFmode)))
#define HAVE_copysigndf3 ((SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
   || (TARGET_SSE && (DFmode == TFmode)) \
   || (TARGET_AVX512FP16 && (DFmode ==HFmode)))
#define HAVE_copysigntf3 ((SSE_FLOAT_MODE_P (TFmode) && TARGET_SSE_MATH) \
   || (TARGET_SSE && (TFmode == TFmode)) \
   || (TARGET_AVX512FP16 && (TFmode ==HFmode)))
#define HAVE_xorsignhf3 (((SSE_FLOAT_MODE_P (HFmode) && TARGET_SSE_MATH) \
  || HFmode == HFmode) && (TARGET_AVX512FP16))
#define HAVE_xorsignsf3 ((SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
  || SFmode == HFmode)
#define HAVE_xorsigndf3 ((SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
  || DFmode == HFmode)
#define HAVE_one_cmplqi2 (TARGET_QIMODE_MATH)
#define HAVE_one_cmplhi2 (TARGET_HIMODE_MATH)
#define HAVE_one_cmplsi2 1
#define HAVE_one_cmpldi2 1
#define HAVE_ashlqi3 (TARGET_QIMODE_MATH)
#define HAVE_ashlhi3 (TARGET_HIMODE_MATH)
#define HAVE_ashlsi3 1
#define HAVE_ashldi3 1
#define HAVE_x86_shiftsi_adj_1 (TARGET_CMOVE)
#define HAVE_x86_shiftsi_adj_2 1
#define HAVE_lshrqi3 (TARGET_QIMODE_MATH)
#define HAVE_ashrqi3 (TARGET_QIMODE_MATH)
#define HAVE_lshrhi3 (TARGET_HIMODE_MATH)
#define HAVE_ashrhi3 (TARGET_HIMODE_MATH)
#define HAVE_lshrsi3 1
#define HAVE_ashrsi3 1
#define HAVE_lshrdi3 1
#define HAVE_ashrdi3 1
#define HAVE_x86_shiftsi_adj_3 1
#define HAVE_rotldi3 1
#define HAVE_rotrdi3 1
#define HAVE_rotlqi3 (TARGET_QIMODE_MATH)
#define HAVE_rotrqi3 (TARGET_QIMODE_MATH)
#define HAVE_rotlhi3 (TARGET_HIMODE_MATH)
#define HAVE_rotrhi3 (TARGET_HIMODE_MATH)
#define HAVE_rotlsi3 1
#define HAVE_rotrsi3 1
#define HAVE_setcc_si_slp 1
#define HAVE_indirect_jump 1
#define HAVE_tablejump 1
#define HAVE_call 1
#define HAVE_sibcall 1
#define HAVE_call_pop 1
#define HAVE_call_value 1
#define HAVE_sibcall_value 1
#define HAVE_call_value_pop 1
#define HAVE_untyped_call 1
#define HAVE_memory_blockage 1
#define HAVE_return (ix86_can_use_return_insn_p ())
#define HAVE_simple_return (!TARGET_SEH && !ix86_static_chain_on_stack && !ix86_function_ms_hook_prologue (cfun->decl))
#define HAVE_simple_return_indirect_internal 1
#define HAVE_prologue 1
#define HAVE_set_got 1
#define HAVE_set_got_labelled 1
#define HAVE_epilogue 1
#define HAVE_sibcall_epilogue 1
#define HAVE_eh_return 1
#define HAVE_leave_si (word_mode == SImode)
#define HAVE_leave_di (word_mode == DImode)
#define HAVE_split_stack_prologue 1
#define HAVE_split_stack_space_check 1
#define HAVE_ffssi2 1
#define HAVE_clzsi2 1
#define HAVE_bmi2_bzhi_si3 (TARGET_BMI2)
#define HAVE_bswapsi2 1
#define HAVE_bswaphi2 1
#define HAVE_paritydi2 (! TARGET_POPCNT)
#define HAVE_paritysi2 (! TARGET_POPCNT)
#define HAVE_parityhi2 (! TARGET_POPCNT)
#define HAVE_parityqi2 (! TARGET_POPCNT)
#define HAVE_tls_global_dynamic_32 1
#define HAVE_tls_local_dynamic_base_32 1
#define HAVE_get_thread_pointersi (ptr_mode == SImode)
#define HAVE_get_thread_pointerdi (ptr_mode == DImode)
#define HAVE_tls_dynamic_gnu2_32 (!TARGET_64BIT && TARGET_GNU2_TLS)
#define HAVE_rsqrtsf2 (TARGET_SSE && TARGET_SSE_MATH)
#define HAVE_sqrtsf2 ((TARGET_USE_FANCY_MATH_387 && X87_ENABLE_ARITH (SFmode)) \
   || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_sqrtdf2 ((TARGET_USE_FANCY_MATH_387 && X87_ENABLE_ARITH (DFmode)) \
   || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH))
#define HAVE_hypotsf3 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_finite_math_only \
   && flag_unsafe_math_optimizations)
#define HAVE_hypotdf3 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_finite_math_only \
   && flag_unsafe_math_optimizations)
#define HAVE_fmodxf3 (TARGET_USE_FANCY_MATH_387)
#define HAVE_fmodsf3 (TARGET_USE_FANCY_MATH_387)
#define HAVE_fmoddf3 (TARGET_USE_FANCY_MATH_387)
#define HAVE_remainderxf3 (TARGET_USE_FANCY_MATH_387)
#define HAVE_remaindersf3 (TARGET_USE_FANCY_MATH_387)
#define HAVE_remainderdf3 (TARGET_USE_FANCY_MATH_387)
#define HAVE_sinsf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_cossf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_sindf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_cosdf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_sincossf3 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_sincosdf3 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_tanxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_tansf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_tandf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_atan2sf3 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_atan2df3 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_atanxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_atansf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_atandf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_asinxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_asinsf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_asindf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_acosxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_acossf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_acosdf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_sinhxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_finite_math_only \
   && flag_unsafe_math_optimizations)
#define HAVE_sinhsf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_finite_math_only \
   && flag_unsafe_math_optimizations)
#define HAVE_sinhdf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_finite_math_only \
   && flag_unsafe_math_optimizations)
#define HAVE_coshxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_coshsf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_coshdf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_tanhxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_tanhsf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_tanhdf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_asinhxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_finite_math_only \
   && flag_unsafe_math_optimizations)
#define HAVE_asinhsf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_finite_math_only \
   && flag_unsafe_math_optimizations)
#define HAVE_asinhdf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_finite_math_only \
   && flag_unsafe_math_optimizations)
#define HAVE_acoshxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_acoshsf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_acoshdf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_atanhxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_atanhsf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_atanhdf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_logxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_logsf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_logdf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_log10xf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_log10sf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_log10df2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_log2xf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_log2sf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_log2df2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_log1pxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_log1psf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_log1pdf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_logbxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_logbsf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_logbdf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_ilogbxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_ilogbsf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_ilogbdf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_expNcorexf3 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_expxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_expsf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_expdf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_exp10xf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_exp10sf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_exp10df2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_exp2xf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_exp2sf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_exp2df2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_expm1xf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_expm1sf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_expm1df2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_ldexpxf3 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_ldexpsf3 (((TARGET_USE_FANCY_MATH_387 \
     && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
	 || TARGET_MIX_SSE_I387)) \
    || (TARGET_AVX512F && TARGET_SSE_MATH)) \
   && flag_unsafe_math_optimizations)
#define HAVE_ldexpdf3 (((TARGET_USE_FANCY_MATH_387 \
     && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
	 || TARGET_MIX_SSE_I387)) \
    || (TARGET_AVX512F && TARGET_SSE_MATH)) \
   && flag_unsafe_math_optimizations)
#define HAVE_scalbxf3 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_scalbsf3 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_scalbdf3 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_significandxf2 (TARGET_USE_FANCY_MATH_387 \
   && flag_unsafe_math_optimizations)
#define HAVE_significandsf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_significanddf2 (TARGET_USE_FANCY_MATH_387 \
   && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
       || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_rinthf2 (TARGET_AVX512FP16)
#define HAVE_rintsf2 (TARGET_USE_FANCY_MATH_387 \
   || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_rintdf2 (TARGET_USE_FANCY_MATH_387 \
   || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH))
#define HAVE_nearbyintxf2 (TARGET_USE_FANCY_MATH_387 \
   && !flag_trapping_math)
#define HAVE_nearbyinthf2 (TARGET_AVX512FP16)
#define HAVE_nearbyintsf2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
	  || TARGET_MIX_SSE_I387) \
    && !flag_trapping_math) \
   || (TARGET_SSE4_1 && TARGET_SSE_MATH))
#define HAVE_nearbyintdf2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
	  || TARGET_MIX_SSE_I387) \
    && !flag_trapping_math) \
   || (TARGET_SSE4_1 && TARGET_SSE_MATH))
#define HAVE_roundhf2 (TARGET_AVX512FP16 && !flag_trapping_math && !flag_rounding_math)
#define HAVE_roundsf2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && flag_unsafe_math_optimizations \
    && (flag_fp_int_builtin_inexact || !flag_trapping_math)) \
   || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH \
       && !flag_trapping_math && !flag_rounding_math))
#define HAVE_rounddf2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && flag_unsafe_math_optimizations \
    && (flag_fp_int_builtin_inexact || !flag_trapping_math)) \
   || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH \
       && !flag_trapping_math && !flag_rounding_math))
#define HAVE_roundxf2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (XFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && flag_unsafe_math_optimizations \
    && (flag_fp_int_builtin_inexact || !flag_trapping_math)) \
   || (SSE_FLOAT_MODE_P (XFmode) && TARGET_SSE_MATH \
       && !flag_trapping_math && !flag_rounding_math))
#define HAVE_lroundhfhi2 (TARGET_AVX512FP16 && !flag_trapping_math && !flag_rounding_math)
#define HAVE_lroundhfsi2 (TARGET_AVX512FP16 && !flag_trapping_math && !flag_rounding_math)
#define HAVE_lrinthfsi2 (TARGET_AVX512FP16)
#define HAVE_lrintsfsi2 (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH)
#define HAVE_lrintdfsi2 (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH)
#define HAVE_lroundsfhi2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && flag_unsafe_math_optimizations) \
   || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH \
       && HImode != HImode  \
       && ((HImode != DImode) || TARGET_64BIT) \
       && !flag_trapping_math && !flag_rounding_math))
#define HAVE_lrounddfhi2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && flag_unsafe_math_optimizations) \
   || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH \
       && HImode != HImode  \
       && ((HImode != DImode) || TARGET_64BIT) \
       && !flag_trapping_math && !flag_rounding_math))
#define HAVE_lroundxfhi2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (XFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && flag_unsafe_math_optimizations) \
   || (SSE_FLOAT_MODE_P (XFmode) && TARGET_SSE_MATH \
       && HImode != HImode  \
       && ((HImode != DImode) || TARGET_64BIT) \
       && !flag_trapping_math && !flag_rounding_math))
#define HAVE_lroundsfsi2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && flag_unsafe_math_optimizations) \
   || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH \
       && SImode != HImode  \
       && ((SImode != DImode) || TARGET_64BIT) \
       && !flag_trapping_math && !flag_rounding_math))
#define HAVE_lrounddfsi2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && flag_unsafe_math_optimizations) \
   || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH \
       && SImode != HImode  \
       && ((SImode != DImode) || TARGET_64BIT) \
       && !flag_trapping_math && !flag_rounding_math))
#define HAVE_lroundxfsi2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (XFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && flag_unsafe_math_optimizations) \
   || (SSE_FLOAT_MODE_P (XFmode) && TARGET_SSE_MATH \
       && SImode != HImode  \
       && ((SImode != DImode) || TARGET_64BIT) \
       && !flag_trapping_math && !flag_rounding_math))
#define HAVE_lroundsfdi2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && flag_unsafe_math_optimizations) \
   || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH \
       && DImode != HImode  \
       && ((DImode != DImode) || TARGET_64BIT) \
       && !flag_trapping_math && !flag_rounding_math))
#define HAVE_lrounddfdi2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && flag_unsafe_math_optimizations) \
   || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH \
       && DImode != HImode  \
       && ((DImode != DImode) || TARGET_64BIT) \
       && !flag_trapping_math && !flag_rounding_math))
#define HAVE_lroundxfdi2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (XFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && flag_unsafe_math_optimizations) \
   || (SSE_FLOAT_MODE_P (XFmode) && TARGET_SSE_MATH \
       && DImode != HImode  \
       && ((DImode != DImode) || TARGET_64BIT) \
       && !flag_trapping_math && !flag_rounding_math))
#define HAVE_roundevenxf2 (TARGET_USE_FANCY_MATH_387 \
   && (flag_fp_int_builtin_inexact || !flag_trapping_math))
#define HAVE_floorxf2 (TARGET_USE_FANCY_MATH_387 \
   && (flag_fp_int_builtin_inexact || !flag_trapping_math))
#define HAVE_ceilxf2 (TARGET_USE_FANCY_MATH_387 \
   && (flag_fp_int_builtin_inexact || !flag_trapping_math))
#define HAVE_btruncxf2 (TARGET_USE_FANCY_MATH_387 \
   && (flag_fp_int_builtin_inexact || !flag_trapping_math))
#define HAVE_roundevenhf2 (TARGET_AVX512FP16)
#define HAVE_floorhf2 (TARGET_AVX512FP16)
#define HAVE_ceilhf2 (TARGET_AVX512FP16)
#define HAVE_btrunchf2 (TARGET_AVX512FP16)
#define HAVE_roundevensf2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && (flag_fp_int_builtin_inexact || !flag_trapping_math)) \
   || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH \
       && (TARGET_SSE4_1 \
	   || (ROUND_ROUNDEVEN != ROUND_ROUNDEVEN \
	       && (flag_fp_int_builtin_inexact || !flag_trapping_math)))))
#define HAVE_floorsf2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && (flag_fp_int_builtin_inexact || !flag_trapping_math)) \
   || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH \
       && (TARGET_SSE4_1 \
	   || (ROUND_FLOOR != ROUND_ROUNDEVEN \
	       && (flag_fp_int_builtin_inexact || !flag_trapping_math)))))
#define HAVE_ceilsf2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && (flag_fp_int_builtin_inexact || !flag_trapping_math)) \
   || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH \
       && (TARGET_SSE4_1 \
	   || (ROUND_CEIL != ROUND_ROUNDEVEN \
	       && (flag_fp_int_builtin_inexact || !flag_trapping_math)))))
#define HAVE_btruncsf2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && (flag_fp_int_builtin_inexact || !flag_trapping_math)) \
   || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH \
       && (TARGET_SSE4_1 \
	   || (ROUND_TRUNC != ROUND_ROUNDEVEN \
	       && (flag_fp_int_builtin_inexact || !flag_trapping_math)))))
#define HAVE_roundevendf2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && (flag_fp_int_builtin_inexact || !flag_trapping_math)) \
   || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH \
       && (TARGET_SSE4_1 \
	   || (ROUND_ROUNDEVEN != ROUND_ROUNDEVEN \
	       && (flag_fp_int_builtin_inexact || !flag_trapping_math)))))
#define HAVE_floordf2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && (flag_fp_int_builtin_inexact || !flag_trapping_math)) \
   || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH \
       && (TARGET_SSE4_1 \
	   || (ROUND_FLOOR != ROUND_ROUNDEVEN \
	       && (flag_fp_int_builtin_inexact || !flag_trapping_math)))))
#define HAVE_ceildf2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && (flag_fp_int_builtin_inexact || !flag_trapping_math)) \
   || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH \
       && (TARGET_SSE4_1 \
	   || (ROUND_CEIL != ROUND_ROUNDEVEN \
	       && (flag_fp_int_builtin_inexact || !flag_trapping_math)))))
#define HAVE_btruncdf2 ((TARGET_USE_FANCY_MATH_387 \
    && (!(SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH) \
	|| TARGET_MIX_SSE_I387) \
    && (flag_fp_int_builtin_inexact || !flag_trapping_math)) \
   || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH \
       && (TARGET_SSE4_1 \
	   || (ROUND_TRUNC != ROUND_ROUNDEVEN \
	       && (flag_fp_int_builtin_inexact || !flag_trapping_math)))))
#define HAVE_lfloorxfhi2 (TARGET_USE_FANCY_MATH_387 \
   && (!TARGET_SSE_MATH || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_lceilxfhi2 (TARGET_USE_FANCY_MATH_387 \
   && (!TARGET_SSE_MATH || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_lfloorxfsi2 (TARGET_USE_FANCY_MATH_387 \
   && (!TARGET_SSE_MATH || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_lceilxfsi2 (TARGET_USE_FANCY_MATH_387 \
   && (!TARGET_SSE_MATH || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_lfloorxfdi2 (TARGET_USE_FANCY_MATH_387 \
   && (!TARGET_SSE_MATH || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_lceilxfdi2 (TARGET_USE_FANCY_MATH_387 \
   && (!TARGET_SSE_MATH || TARGET_MIX_SSE_I387) \
   && flag_unsafe_math_optimizations)
#define HAVE_lfloorhfsi2 (TARGET_AVX512FP16)
#define HAVE_lceilhfsi2 (TARGET_AVX512FP16)
#define HAVE_lfloorsfsi2 (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH \
   && (TARGET_SSE4_1 || !flag_trapping_math))
#define HAVE_lceilsfsi2 (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH \
   && (TARGET_SSE4_1 || !flag_trapping_math))
#define HAVE_lfloordfsi2 (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH \
   && (TARGET_SSE4_1 || !flag_trapping_math))
#define HAVE_lceildfsi2 (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH \
   && (TARGET_SSE4_1 || !flag_trapping_math))
#define HAVE_signbittf2 (TARGET_SSE)
#define HAVE_signbitxf2 (TARGET_USE_FANCY_MATH_387)
#define HAVE_signbitdf2 (TARGET_USE_FANCY_MATH_387 \
   || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH))
#define HAVE_signbitsf2 (TARGET_USE_FANCY_MATH_387 \
   && !(SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_cpymemsi 1
#define HAVE_strmov 1
#define HAVE_strmov_singleop 1
#define HAVE_rep_mov 1
#define HAVE_setmemsi 1
#define HAVE_strset 1
#define HAVE_strset_singleop 1
#define HAVE_rep_stos 1
#define HAVE_cmpmemsi 1
#define HAVE_cmpstrnsi 1
#define HAVE_cmpintqi 1
#define HAVE_cmpstrnqi_nz_1 1
#define HAVE_cmpstrnqi_1 1
#define HAVE_strlensi (Pmode == SImode)
#define HAVE_strlendi (Pmode == DImode)
#define HAVE_strlenqi_1 1
#define HAVE_movqicc (TARGET_QIMODE_MATH)
#define HAVE_movhicc (TARGET_HIMODE_MATH)
#define HAVE_movsicc 1
#define HAVE_x86_movsicc_0_m1 1
#define HAVE_x86_movqicc_0_m1_neg 1
#define HAVE_x86_movhicc_0_m1_neg 1
#define HAVE_x86_movsicc_0_m1_neg 1
#define HAVE_movhfcc (TARGET_AVX512FP16)
#define HAVE_movsfcc ((TARGET_80387 && TARGET_CMOVE) \
   || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH))
#define HAVE_movdfcc ((TARGET_80387 && TARGET_CMOVE) \
   || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH))
#define HAVE_movxfcc ((TARGET_80387 && TARGET_CMOVE) \
   || (SSE_FLOAT_MODE_P (XFmode) && TARGET_SSE_MATH))
#define HAVE_addqicc 1
#define HAVE_addhicc 1
#define HAVE_addsicc 1
#define HAVE_smaxqi3 ((TARGET_CMOVE \
   && (QImode != QImode || !TARGET_PARTIAL_REG_STALL)) && (TARGET_QIMODE_MATH))
#define HAVE_sminqi3 ((TARGET_CMOVE \
   && (QImode != QImode || !TARGET_PARTIAL_REG_STALL)) && (TARGET_QIMODE_MATH))
#define HAVE_umaxqi3 ((TARGET_CMOVE \
   && (QImode != QImode || !TARGET_PARTIAL_REG_STALL)) && (TARGET_QIMODE_MATH))
#define HAVE_uminqi3 ((TARGET_CMOVE \
   && (QImode != QImode || !TARGET_PARTIAL_REG_STALL)) && (TARGET_QIMODE_MATH))
#define HAVE_smaxhi3 ((TARGET_CMOVE \
   && (HImode != QImode || !TARGET_PARTIAL_REG_STALL)) && (TARGET_HIMODE_MATH))
#define HAVE_sminhi3 ((TARGET_CMOVE \
   && (HImode != QImode || !TARGET_PARTIAL_REG_STALL)) && (TARGET_HIMODE_MATH))
#define HAVE_umaxhi3 ((TARGET_CMOVE \
   && (HImode != QImode || !TARGET_PARTIAL_REG_STALL)) && (TARGET_HIMODE_MATH))
#define HAVE_uminhi3 ((TARGET_CMOVE \
   && (HImode != QImode || !TARGET_PARTIAL_REG_STALL)) && (TARGET_HIMODE_MATH))
#define HAVE_smaxsi3 (TARGET_CMOVE \
   && (SImode != QImode || !TARGET_PARTIAL_REG_STALL))
#define HAVE_sminsi3 (TARGET_CMOVE \
   && (SImode != QImode || !TARGET_PARTIAL_REG_STALL))
#define HAVE_umaxsi3 (TARGET_CMOVE \
   && (SImode != QImode || !TARGET_PARTIAL_REG_STALL))
#define HAVE_uminsi3 (TARGET_CMOVE \
   && (SImode != QImode || !TARGET_PARTIAL_REG_STALL))
#define HAVE_smaxdi3 (TARGET_CMOVE \
   && (DImode != QImode || !TARGET_PARTIAL_REG_STALL))
#define HAVE_smindi3 (TARGET_CMOVE \
   && (DImode != QImode || !TARGET_PARTIAL_REG_STALL))
#define HAVE_umaxdi3 (TARGET_CMOVE \
   && (DImode != QImode || !TARGET_PARTIAL_REG_STALL))
#define HAVE_umindi3 (TARGET_CMOVE \
   && (DImode != QImode || !TARGET_PARTIAL_REG_STALL))
#define HAVE_allocate_stack (ix86_target_stack_probe ())
#define HAVE_probe_stack 1
#define HAVE_builtin_setjmp_receiver (!TARGET_64BIT && flag_pic)
#define HAVE_save_stack_nonlocal 1
#define HAVE_restore_stack_nonlocal 1
#define HAVE_stack_protect_set 1
#define HAVE_stack_protect_test 1
#define HAVE_prefetch (TARGET_3DNOW || TARGET_PREFETCH_SSE || TARGET_PRFCHW || TARGET_MOVRS)
#define HAVE_pause 1
#define HAVE_xbegin (TARGET_RTM)
#define HAVE_xtest (TARGET_RTM)
#define HAVE_rdpkru (TARGET_PKU)
#define HAVE_wrpkru (TARGET_PKU)
#define HAVE_spaceshipsf4 ((TARGET_80387 || (SSE_FLOAT_MODE_P (SFmode) && TARGET_SSE_MATH)) \
   && (TARGET_CMOVE || (TARGET_SAHF && TARGET_USE_SAHF)))
#define HAVE_spaceshipdf4 ((TARGET_80387 || (SSE_FLOAT_MODE_P (DFmode) && TARGET_SSE_MATH)) \
   && (TARGET_CMOVE || (TARGET_SAHF && TARGET_USE_SAHF)))
#define HAVE_spaceshipxf4 (TARGET_80387 && (TARGET_CMOVE || (TARGET_SAHF && TARGET_USE_SAHF)))
#define HAVE_spaceshipqi4 1
#define HAVE_spaceshiphi4 1
#define HAVE_spaceshipsi4 1
#define HAVE_issignalingxf2 1
#define HAVE_movv8qi (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_movv4hi (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_movv2si (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_movv1di (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_movv2sf (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_movv4hf (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_movv4bf (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_movmisalignv8qi (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_movmisalignv4hi (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_movmisalignv2si (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_movmisalignv1di (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_movmisalignv2sf (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_movmisalignv4hf (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_movmisalignv4bf (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_movv4qi 1
#define HAVE_movv2hi 1
#define HAVE_movv1si 1
#define HAVE_movv2hf 1
#define HAVE_movv2bf 1
#define HAVE_movmisalignv4qi 1
#define HAVE_movmisalignv2hi 1
#define HAVE_movmisalignv1si 1
#define HAVE_movmisalignv2hf 1
#define HAVE_movmisalignv2bf 1
#define HAVE_movv2qi 1
#define HAVE_movmisalignv2qi 1
#define HAVE_movq_v2sf_to_sse (TARGET_SSE2)
#define HAVE_movq_v2si_to_sse (TARGET_SSE2)
#define HAVE_movq_v4hf_to_sse (TARGET_SSE2)
#define HAVE_movq_v4hi_to_sse (TARGET_SSE2)
#define HAVE_mmx_addv2sf3 (TARGET_3DNOW)
#define HAVE_mmx_subv2sf3 (TARGET_3DNOW)
#define HAVE_mmx_subrv2sf3 (TARGET_3DNOW)
#define HAVE_mmx_mulv2sf3 (TARGET_3DNOW)
#define HAVE_mmx_smaxv2sf3 (TARGET_3DNOW)
#define HAVE_mmx_sminv2sf3 (TARGET_3DNOW)
#define HAVE_mmx_haddv2sf3 (TARGET_3DNOW)
#define HAVE_mmx_haddsubv2sf3 (TARGET_3DNOW_A)
#define HAVE_mmx_eqv2sf3 (TARGET_3DNOW)
#define HAVE_vec_setv2sf (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_vec_extractv2sfsf (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_vec_initv2sfsf ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSE)
#define HAVE_divv4hf3 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_movd_v2hf_to_sse (TARGET_SSE)
#define HAVE_movd_v2bf_to_sse (TARGET_SSE)
#define HAVE_movd_v2hi_to_sse (TARGET_SSE)
#define HAVE_movd_v2hf_to_sse_reg (TARGET_SSE)
#define HAVE_movd_v2bf_to_sse_reg (TARGET_SSE)
#define HAVE_movd_v2hi_to_sse_reg (TARGET_SSE)
#define HAVE_addv2hf3 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_subv2hf3 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_mulv2hf3 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_addv2bf3 (TARGET_AVX10_2)
#define HAVE_subv2bf3 (TARGET_AVX10_2)
#define HAVE_mulv2bf3 (TARGET_AVX10_2)
#define HAVE_divv2bf3 (TARGET_AVX10_2)
#define HAVE_divv2hf3 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_smaxv2hf3 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_sminv2hf3 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_smaxv2bf3 (TARGET_AVX10_2)
#define HAVE_sminv2bf3 (TARGET_AVX10_2)
#define HAVE_sqrtv2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_sqrtv2bf2 (TARGET_AVX10_2)
#define HAVE_absv2bf2 (TARGET_SSE)
#define HAVE_negv2bf2 (TARGET_SSE)
#define HAVE_absv2hf2 (TARGET_SSE)
#define HAVE_negv2hf2 (TARGET_SSE)
#define HAVE_vec_cmpv2hfqi (TARGET_AVX512FP16 && TARGET_AVX512VL \
   && ix86_partial_vec_fp_math)
#define HAVE_vcond_mask_v2hfv2hi (TARGET_SSE4_1)
#define HAVE_vcond_mask_v2bfv2hi (TARGET_SSE4_1)
#define HAVE_vcond_mask_v2hfqi (TARGET_AVX512BW && TARGET_AVX512VL)
#define HAVE_vcond_mask_v2bfqi (TARGET_AVX512BW && TARGET_AVX512VL)
#define HAVE_vcond_mask_v2hiqi (TARGET_AVX512BW && TARGET_AVX512VL)
#define HAVE_vec_cmpv2bfqi (TARGET_AVX10_2)
#define HAVE_btruncv2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL \
   && ix86_partial_vec_fp_math \
   && !flag_trapping_math)
#define HAVE_nearbyintv2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL \
   && ix86_partial_vec_fp_math)
#define HAVE_rintv2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL \
   && ix86_partial_vec_fp_math)
#define HAVE_lrintv2hfv2hi2 (TARGET_AVX512FP16 && TARGET_AVX512VL \
   && ix86_partial_vec_fp_math)
#define HAVE_floorv2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL \
   && ix86_partial_vec_fp_math \
   && !flag_trapping_math)
#define HAVE_lfloorv2hfv2hi2 (TARGET_AVX512FP16 && TARGET_AVX512VL \
   && ix86_partial_vec_fp_math \
   && !flag_trapping_math)
#define HAVE_ceilv2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL \
   && ix86_partial_vec_fp_math \
   && !flag_trapping_math)
#define HAVE_lceilv2hfv2hi2 (TARGET_AVX512FP16 && TARGET_AVX512VL \
   && ix86_partial_vec_fp_math \
   && !flag_trapping_math)
#define HAVE_roundv2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL \
   && ix86_partial_vec_fp_math \
   && !flag_trapping_math)
#define HAVE_lroundv2hfv2hi2 (TARGET_AVX512FP16 && TARGET_AVX512VL \
   && ix86_partial_vec_fp_math \
   && !flag_trapping_math)
#define HAVE_copysignv2bf3 (TARGET_SSE)
#define HAVE_copysignv2hf3 (TARGET_SSE)
#define HAVE_xorsignv2bf3 (TARGET_SSE)
#define HAVE_xorsignv2hf3 (TARGET_SSE)
#define HAVE_signbitv2bf2 (TARGET_SSE2)
#define HAVE_signbitv2hf2 (TARGET_SSE2)
#define HAVE_fmav2hf4 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_fmsv2hf4 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_fnmav2hf4 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_fnmsv2hf4 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_fmav2bf4 (TARGET_AVX10_2)
#define HAVE_fmsv2bf4 (TARGET_AVX10_2)
#define HAVE_fnmav2bf4 (TARGET_AVX10_2)
#define HAVE_fnmsv2bf4 (TARGET_AVX10_2)
#define HAVE_cmlav4hf4 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_cmla_conjv4hf4 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_cmulv4hf3 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_cmul_conjv4hf3 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fix_truncv2hfv2hi2 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_fixuns_truncv2hfv2hi2 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_floatv2hiv2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_floatunsv2hiv2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL && ix86_partial_vec_fp_math)
#define HAVE_negv4qi2 (TARGET_SSE2)
#define HAVE_negv2hi2 (TARGET_SSE2)
#define HAVE_mmx_addv8qi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_subv8qi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_addv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_subv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_addv2si3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_subv2si3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_addv1di3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && (TARGET_SSE2))
#define HAVE_mmx_subv1di3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && (TARGET_SSE2))
#define HAVE_mmx_ssaddv8qi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_usaddv8qi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_sssubv8qi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_ussubv8qi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_ssaddv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_usaddv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_sssubv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_ussubv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_mulv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mulv4qi3 (TARGET_SSE2)
#define HAVE_mmx_smulv4hi3_highpart (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_umulv4hi3_highpart ((TARGET_MMX || TARGET_MMX_WITH_SSE) \
   && (TARGET_SSE || TARGET_3DNOW_A))
#define HAVE_mmx_pmaddwd (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_pmulhrwv4hi3 (TARGET_3DNOW)
#define HAVE_sse2_umulv1siv1di3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSE2)
#define HAVE_mmx_smaxv4hi3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) \
   && (TARGET_SSE || TARGET_3DNOW_A))
#define HAVE_mmx_sminv4hi3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) \
   && (TARGET_SSE || TARGET_3DNOW_A))
#define HAVE_mmx_umaxv8qi3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) \
   && (TARGET_SSE || TARGET_3DNOW_A))
#define HAVE_mmx_uminv8qi3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) \
   && (TARGET_SSE || TARGET_3DNOW_A))
#define HAVE_ashlv4qi3 (TARGET_SSE2)
#define HAVE_lshrv4qi3 (TARGET_SSE2)
#define HAVE_ashrv4qi3 (TARGET_SSE2)
#define HAVE_vashlv4qi3 (TARGET_AVX512BW && TARGET_AVX512VL)
#define HAVE_vlshrv4qi3 (TARGET_AVX512BW && TARGET_AVX512VL)
#define HAVE_vashrv4qi3 (TARGET_AVX512BW && TARGET_AVX512VL)
#define HAVE_vec_shl_v2hf (TARGET_SSE2)
#define HAVE_vec_shl_v2bf (TARGET_SSE2)
#define HAVE_vec_shl_v2hi (TARGET_SSE2)
#define HAVE_vec_shl_v4qi (TARGET_SSE2)
#define HAVE_vec_shr_v2hf (TARGET_SSE2)
#define HAVE_vec_shr_v2bf (TARGET_SSE2)
#define HAVE_vec_shr_v2hi (TARGET_SSE2)
#define HAVE_vec_shr_v4qi (TARGET_SSE2)
#define HAVE_mmx_eqv8qi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_eqv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_eqv2si3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_vec_cmpv4qiv4qi (TARGET_SSE2)
#define HAVE_vec_cmpv2qiv2qi (TARGET_SSE2)
#define HAVE_vec_cmpv2hiv2hi (TARGET_SSE2)
#define HAVE_vec_cmpuv4qiv4qi (TARGET_SSE2)
#define HAVE_vec_cmpuv2qiv2qi (TARGET_SSE2)
#define HAVE_vec_cmpuv2hiv2hi (TARGET_SSE2)
#define HAVE_vcond_mask_v4qiv4qi (TARGET_SSE2)
#define HAVE_vcond_mask_v2qiv2qi (TARGET_SSE2)
#define HAVE_vcond_mask_v2hiv2hi (TARGET_SSE2)
#define HAVE_mmx_andv8qi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_iorv8qi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_xorv8qi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_andv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_iorv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_xorv4hi3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_andv2si3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_iorv2si3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_mmx_xorv2si3 (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_andv4qi3 1
#define HAVE_iorv4qi3 1
#define HAVE_xorv4qi3 1
#define HAVE_andv2qi3 1
#define HAVE_iorv2qi3 1
#define HAVE_xorv2qi3 1
#define HAVE_andv2hi3 1
#define HAVE_iorv2hi3 1
#define HAVE_xorv2hi3 1
#define HAVE_extendv2qiv2hi2 (TARGET_SSE2)
#define HAVE_zero_extendv2qiv2hi2 (TARGET_SSE2)
#define HAVE_truncv2hiv2qi2 (TARGET_AVX2)
#define HAVE_vec_pack_trunc_v2hi (TARGET_SSE2)
#define HAVE_vec_unpacks_lo_v4qi (TARGET_SSE2)
#define HAVE_vec_unpacks_hi_v4qi (TARGET_SSE2)
#define HAVE_vec_unpacku_lo_v4qi (TARGET_SSE2)
#define HAVE_vec_unpacku_hi_v4qi (TARGET_SSE2)
#define HAVE_mmx_pshufw ((TARGET_MMX || TARGET_MMX_WITH_SSE) \
   && (TARGET_SSE || TARGET_3DNOW_A))
#define HAVE_vec_setv2si (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_vec_extractv2sisi (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_vec_initv2sisi ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSE)
#define HAVE_vec_setv4hf (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_vec_setv4bf (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_vec_setv4hi (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_vec_extractv4hfhf (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_vec_extractv4bfbf (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_vec_extractv4hihi (TARGET_MMX || TARGET_MMX_WITH_SSE)
#define HAVE_vec_initv4hihi ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSE)
#define HAVE_vec_initv8qiqi ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSE)
#define HAVE_vec_setv2hf (TARGET_SSE2)
#define HAVE_vec_setv2bf (TARGET_SSE2)
#define HAVE_vec_setv2hi (TARGET_SSE2)
#define HAVE_vec_extractv2hfhf (TARGET_SSE2)
#define HAVE_vec_extractv2bfbf (TARGET_SSE2)
#define HAVE_vec_extractv2hihi (TARGET_SSE2)
#define HAVE_vec_setv4qi (TARGET_SSE4_1)
#define HAVE_vec_extractv4qiqi (TARGET_SSE4_1)
#define HAVE_vec_initv2hfhf (TARGET_SSE2)
#define HAVE_vec_initv2bfbf (TARGET_SSE2)
#define HAVE_vec_initv2hihi (TARGET_SSE2)
#define HAVE_vec_initv4qiqi (TARGET_SSE2)
#define HAVE_mmx_uavgv8qi3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) \
   && (TARGET_SSE || TARGET_3DNOW))
#define HAVE_mmx_uavgv4hi3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) \
   && (TARGET_SSE || TARGET_3DNOW))
#define HAVE_mmx_psadbw ((TARGET_MMX || TARGET_MMX_WITH_SSE) && (TARGET_SSE || TARGET_3DNOW_A))
#define HAVE_reduc_and_scal_v4qi (TARGET_SSE2)
#define HAVE_reduc_ior_scal_v4qi (TARGET_SSE2)
#define HAVE_reduc_xor_scal_v4qi (TARGET_SSE2)
#define HAVE_reduc_smax_scal_v4qi (TARGET_SSE4_1)
#define HAVE_reduc_smin_scal_v4qi (TARGET_SSE4_1)
#define HAVE_reduc_umax_scal_v4qi (TARGET_SSE4_1)
#define HAVE_reduc_umin_scal_v4qi (TARGET_SSE4_1)
#define HAVE_reduc_plus_scal_v4qi (TARGET_SSE2)
#define HAVE_mmx_maskmovq (TARGET_SSE || TARGET_3DNOW_A)
#define HAVE_mmx_emms ((TARGET_MMX || TARGET_MMX_WITH_SSE) && (TARGET_MMX))
#define HAVE_mmx_femms ((TARGET_MMX || TARGET_MMX_WITH_SSE) && (TARGET_3DNOW))
#define HAVE_movv64qi ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv32qi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movv16qi (TARGET_SSE)
#define HAVE_movv32hi ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv16hi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movv8hi (TARGET_SSE)
#define HAVE_movv16si ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv8si ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movv4si (TARGET_SSE)
#define HAVE_movv8di ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv4di ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movv2di (TARGET_SSE)
#define HAVE_movv4ti ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv2ti ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movv1ti (TARGET_SSE)
#define HAVE_movv32hf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv16hf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movv8hf (TARGET_SSE)
#define HAVE_movv32bf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv16bf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movv8bf (TARGET_SSE)
#define HAVE_movv16sf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv8sf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movv4sf (TARGET_SSE)
#define HAVE_movv8df ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movv4df ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movv2df (TARGET_SSE)
#define HAVE_avx512f_loadv16si_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_loadv8si_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_loadv4si_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_loadv8di_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_loadv4di_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_loadv2di_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_loadv16sf_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_loadv8sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_loadv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_loadv8df_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_loadv4df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_loadv2df_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512bw_loadv64qi_mask ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_loadv16qi_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_loadv32qi_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_loadv32hi_mask ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_loadv16hi_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_loadv8hi_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512f_loadhf_mask ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_loadsf_mask (TARGET_AVX512F)
#define HAVE_avx512f_loaddf_mask (TARGET_AVX512F)
#define HAVE_sse2_movq128 (TARGET_SSE2)
#define HAVE_movmisalignv64qi ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movmisalignv32qi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movmisalignv16qi (TARGET_SSE)
#define HAVE_movmisalignv32hi ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movmisalignv16hi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movmisalignv8hi (TARGET_SSE)
#define HAVE_movmisalignv16si ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movmisalignv8si ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movmisalignv4si (TARGET_SSE)
#define HAVE_movmisalignv8di ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movmisalignv4di ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movmisalignv2di (TARGET_SSE)
#define HAVE_movmisalignv4ti ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movmisalignv2ti ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movmisalignv1ti (TARGET_SSE)
#define HAVE_movmisalignv32hf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movmisalignv16hf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movmisalignv8hf (TARGET_SSE)
#define HAVE_movmisalignv32bf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movmisalignv16bf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movmisalignv8bf (TARGET_SSE)
#define HAVE_movmisalignv16sf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movmisalignv8sf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movmisalignv4sf (TARGET_SSE)
#define HAVE_movmisalignv8df ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_movmisalignv4df ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_movmisalignv2df (TARGET_SSE)
#define HAVE_storentsi ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_storentsf ((TARGET_SSE) && (TARGET_SSE4A))
#define HAVE_storentdf ((TARGET_SSE) && (TARGET_SSE4A))
#define HAVE_storentv8di ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_storentv4di ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_storentv2di ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_storentv16sf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_storentv8sf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_storentv4sf (TARGET_SSE)
#define HAVE_storentv8df ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_storentv4df ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_storentv2df ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_kmovb ((TARGET_AVX512F \
   && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_AVX512DQ))
#define HAVE_kmovw (TARGET_AVX512F \
   && !(MEM_P (operands[0]) && MEM_P (operands[1])))
#define HAVE_kmovd ((TARGET_AVX512F \
   && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_AVX512BW))
#define HAVE_kmovq ((TARGET_AVX512F \
   && !(MEM_P (operands[0]) && MEM_P (operands[1]))) && (TARGET_AVX512BW))
#define HAVE_kortestqi ((TARGET_AVX512F) && (TARGET_AVX512DQ))
#define HAVE_kortesthi (TARGET_AVX512F)
#define HAVE_kortestsi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_kortestdi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_absv32bf2 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_negv32bf2 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_absv16bf2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_negv16bf2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_absv8bf2 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_negv8bf2 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_absv32hf2 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_negv32hf2 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_absv16hf2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_negv16hf2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_absv8hf2 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_negv8hf2 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_absv16sf2 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_negv16sf2 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_absv8sf2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_negv8sf2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_absv4sf2 (TARGET_SSE)
#define HAVE_negv4sf2 (TARGET_SSE)
#define HAVE_absv8df2 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_negv8df2 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_absv4df2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_negv4df2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_absv2df2 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_negv2df2 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_cond_addv32hf ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_cond_subv32hf ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_cond_addv16hf ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_subv16hf ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_addv8hf ((16 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_subv8hf ((16 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_addv16sf ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cond_subv16sf ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cond_addv8sf ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX))
#define HAVE_cond_subv8sf ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX))
#define HAVE_cond_addv4sf (16 == 64 || TARGET_AVX512VL)
#define HAVE_cond_subv4sf (16 == 64 || TARGET_AVX512VL)
#define HAVE_cond_addv8df ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cond_subv8df ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cond_addv4df ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX))
#define HAVE_cond_subv4df ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX))
#define HAVE_cond_addv2df ((16 == 64 || TARGET_AVX512VL) && (TARGET_SSE2))
#define HAVE_cond_subv2df ((16 == 64 || TARGET_AVX512VL) && (TARGET_SSE2))
#define HAVE_addv32hf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_addv32hf3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_addv32hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_addv32hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_subv32hf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_subv32hf3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_subv32hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_subv32hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_addv16hf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_addv16hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_subv16hf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_subv16hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_addv8hf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_addv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_subv8hf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_subv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_addv16sf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_addv16sf3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_addv16sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_addv16sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_subv16sf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_subv16sf3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_subv16sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_subv16sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_addv8sf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX))
#define HAVE_addv8sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX)))
#define HAVE_subv8sf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX))
#define HAVE_subv8sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX)))
#define HAVE_addv4sf3 (TARGET_SSE && 1 && 1)
#define HAVE_addv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1))
#define HAVE_subv4sf3 (TARGET_SSE && 1 && 1)
#define HAVE_subv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1))
#define HAVE_addv8df3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_addv8df3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_addv8df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_addv8df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_subv8df3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_subv8df3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_subv8df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_subv8df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_addv4df3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX))
#define HAVE_addv4df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX)))
#define HAVE_subv4df3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX))
#define HAVE_subv4df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX)))
#define HAVE_addv2df3 ((TARGET_SSE && 1 && 1) && (TARGET_SSE2))
#define HAVE_addv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_SSE2)))
#define HAVE_subv2df3 ((TARGET_SSE && 1 && 1) && (TARGET_SSE2))
#define HAVE_subv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_SSE2)))
#define HAVE_addv32bf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX10_2))
#define HAVE_addv32bf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX10_2)))
#define HAVE_subv32bf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX10_2))
#define HAVE_subv32bf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX10_2)))
#define HAVE_addv16bf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX10_2))
#define HAVE_addv16bf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX10_2)))
#define HAVE_subv16bf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX10_2))
#define HAVE_subv16bf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX10_2)))
#define HAVE_addv8bf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX10_2))
#define HAVE_addv8bf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX10_2)))
#define HAVE_subv8bf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX10_2))
#define HAVE_subv8bf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX10_2)))
#define HAVE_cond_mulv32hf ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_cond_mulv16hf ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_mulv8hf ((16 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_mulv16sf ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cond_mulv8sf ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX))
#define HAVE_cond_mulv4sf (16 == 64 || TARGET_AVX512VL)
#define HAVE_cond_mulv8df ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cond_mulv4df ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX))
#define HAVE_cond_mulv2df ((16 == 64 || TARGET_AVX512VL) && (TARGET_SSE2))
#define HAVE_mulv32hf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_mulv32hf3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_mulv32hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_mulv32hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V32HFmode == V16SFmode \
							      || V32HFmode == V8DFmode \
							      || V32HFmode == V8DImode \
							      || V32HFmode == V16SImode \
							      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_mulv16hf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_mulv16hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_mulv8hf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_mulv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_mulv16sf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_mulv16sf3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_mulv16sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_mulv16sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V16SFmode == V16SFmode \
							      || V16SFmode == V8DFmode \
							      || V16SFmode == V8DImode \
							      || V16SFmode == V16SImode \
							      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_mulv8sf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX))
#define HAVE_mulv8sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX)))
#define HAVE_mulv4sf3 (TARGET_SSE && 1 && 1)
#define HAVE_mulv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1))
#define HAVE_mulv8df3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_mulv8df3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_mulv8df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_mulv8df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && (V8DFmode == V16SFmode \
							      || V8DFmode == V8DFmode \
							      || V8DFmode == V8DImode \
							      || V8DFmode == V16SImode \
							      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_mulv4df3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX))
#define HAVE_mulv4df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX)))
#define HAVE_mulv2df3 ((TARGET_SSE && 1 && 1) && (TARGET_SSE2))
#define HAVE_mulv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_SSE2)))
#define HAVE_mulv32bf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX10_2))
#define HAVE_mulv32bf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX10_2)))
#define HAVE_mulv16bf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX10_2))
#define HAVE_mulv16bf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX10_2)))
#define HAVE_mulv8bf3 ((TARGET_SSE && 1 && 1) && (TARGET_AVX10_2))
#define HAVE_mulv8bf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) && 1) && (TARGET_AVX10_2)))
#define HAVE_divv8df3 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_divv4df3 ((TARGET_SSE2) && (TARGET_AVX))
#define HAVE_divv2df3 (TARGET_SSE2)
#define HAVE_divv32hf3 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_divv16hf3 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_divv8hf3 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_divv16sf3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_divv8sf3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_divv4sf3 (TARGET_SSE)
#define HAVE_divv32bf3 (TARGET_AVX10_2)
#define HAVE_divv16bf3 (TARGET_AVX10_2)
#define HAVE_divv8bf3 (TARGET_AVX10_2)
#define HAVE_cond_divv32hf ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_cond_divv16hf ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_divv8hf ((16 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_divv16sf ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cond_divv8sf ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX))
#define HAVE_cond_divv4sf (16 == 64 || TARGET_AVX512VL)
#define HAVE_cond_divv8df ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cond_divv4df ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX))
#define HAVE_cond_divv2df ((16 == 64 || TARGET_AVX512VL) && (TARGET_SSE2))
#define HAVE_sqrtv32bf2 ((TARGET_SSE2) && (TARGET_AVX10_2))
#define HAVE_sqrtv16bf2 ((TARGET_SSE2) && (TARGET_AVX10_2))
#define HAVE_sqrtv8bf2 ((TARGET_SSE2) && (TARGET_AVX10_2))
#define HAVE_sqrtv32hf2 ((TARGET_SSE2) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_sqrtv16hf2 ((TARGET_SSE2) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_sqrtv8hf2 ((TARGET_SSE2) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_sqrtv8df2 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_sqrtv4df2 ((TARGET_SSE2) && (TARGET_AVX))
#define HAVE_sqrtv2df2 (TARGET_SSE2)
#define HAVE_sqrtv16sf2 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_sqrtv8sf2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_sqrtv4sf2 (TARGET_SSE)
#define HAVE_rsqrtv8sf2 ((TARGET_SSE && TARGET_SSE_MATH) && (TARGET_AVX))
#define HAVE_rsqrtv4sf2 (TARGET_SSE && TARGET_SSE_MATH)
#define HAVE_rsqrtv32hf2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_rsqrtv16hf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_rsqrtv8hf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_cond_smaxv32hf ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_cond_sminv32hf ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_cond_smaxv16hf ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_sminv16hf ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_smaxv8hf ((16 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_sminv8hf ((16 == 64 || TARGET_AVX512VL) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_smaxv16sf ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cond_sminv16sf ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cond_smaxv8sf ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX))
#define HAVE_cond_sminv8sf ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX))
#define HAVE_cond_smaxv4sf (16 == 64 || TARGET_AVX512VL)
#define HAVE_cond_sminv4sf (16 == 64 || TARGET_AVX512VL)
#define HAVE_cond_smaxv8df ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cond_sminv8df ((64 == 64 || TARGET_AVX512VL) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cond_smaxv4df ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX))
#define HAVE_cond_sminv4df ((32 == 64 || TARGET_AVX512VL) && (TARGET_AVX))
#define HAVE_cond_smaxv2df ((16 == 64 || TARGET_AVX512VL) && (TARGET_SSE2))
#define HAVE_cond_sminv2df ((16 == 64 || TARGET_AVX512VL) && (TARGET_SSE2))
#define HAVE_smaxv32hf3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_smaxv32hf3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 \
   && (V32HFmode == V16SFmode \
									      || V32HFmode == V8DFmode \
									      || V32HFmode == V8DImode \
									      || V32HFmode == V16SImode \
									      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_smaxv32hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_smaxv32hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) \
   && (V32HFmode == V16SFmode \
									      || V32HFmode == V8DFmode \
									      || V32HFmode == V8DImode \
									      || V32HFmode == V16SImode \
									      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_sminv32hf3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_sminv32hf3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 \
   && (V32HFmode == V16SFmode \
									      || V32HFmode == V8DFmode \
									      || V32HFmode == V8DImode \
									      || V32HFmode == V16SImode \
									      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_sminv32hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_sminv32hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) \
   && (V32HFmode == V16SFmode \
									      || V32HFmode == V8DFmode \
									      || V32HFmode == V8DImode \
									      || V32HFmode == V16SImode \
									      || V32HFmode == V32HFmode)) && (TARGET_AVX512FP16 && TARGET_EVEX512))))
#define HAVE_smaxv16hf3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_smaxv16hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_sminv16hf3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_sminv16hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_smaxv8hf3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_smaxv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_sminv8hf3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_sminv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_smaxv16sf3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_smaxv16sf3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 \
   && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_smaxv16sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_smaxv16sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) \
   && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_sminv16sf3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_sminv16sf3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 \
   && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_sminv16sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_sminv16sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) \
   && (V16SFmode == V16SFmode \
									      || V16SFmode == V8DFmode \
									      || V16SFmode == V8DImode \
									      || V16SFmode == V16SImode \
									      || V16SFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_smaxv8sf3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_AVX))
#define HAVE_smaxv8sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX)))
#define HAVE_sminv8sf3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_AVX))
#define HAVE_sminv8sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX)))
#define HAVE_smaxv4sf3 (TARGET_SSE && 1 \
   && 1)
#define HAVE_smaxv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE && (16 == 64 || TARGET_AVX512VL) \
   && 1))
#define HAVE_sminv4sf3 (TARGET_SSE && 1 \
   && 1)
#define HAVE_sminv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE && (16 == 64 || TARGET_AVX512VL) \
   && 1))
#define HAVE_smaxv8df3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_smaxv8df3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 \
   && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_smaxv8df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_smaxv8df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) \
   && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_sminv8df3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_sminv8df3_round ((TARGET_AVX512F) && ((TARGET_SSE && 1 \
   && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_sminv8df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_sminv8df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE && (64 == 64 || TARGET_AVX512VL) \
   && (V8DFmode == V16SFmode \
									      || V8DFmode == V8DFmode \
									      || V8DFmode == V8DImode \
									      || V8DFmode == V16SImode \
									      || V8DFmode == V32HFmode)) && (TARGET_AVX512F && TARGET_EVEX512))))
#define HAVE_smaxv4df3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_AVX))
#define HAVE_smaxv4df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX)))
#define HAVE_sminv4df3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_AVX))
#define HAVE_sminv4df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (32 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_AVX)))
#define HAVE_smaxv2df3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_SSE2))
#define HAVE_smaxv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_SSE2)))
#define HAVE_sminv2df3 ((TARGET_SSE && 1 \
   && 1) && (TARGET_SSE2))
#define HAVE_sminv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && (16 == 64 || TARGET_AVX512VL) \
   && 1) && (TARGET_SSE2)))
#define HAVE_avx512fp16_vmsmaxv8hf3 ((TARGET_SSE) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vmsmaxv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vmsmaxv8hf3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vmsmaxv8hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16))))
#define HAVE_avx512fp16_vmsminv8hf3 ((TARGET_SSE) && (TARGET_AVX512FP16))
#define HAVE_avx512fp16_vmsminv8hf3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vmsminv8hf3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16)))
#define HAVE_avx512fp16_vmsminv8hf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_AVX512FP16))))
#define HAVE_sse_vmsmaxv4sf3 (TARGET_SSE)
#define HAVE_sse_vmsmaxv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_vmsmaxv4sf3_round ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_vmsmaxv4sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE)))
#define HAVE_sse_vmsminv4sf3 (TARGET_SSE)
#define HAVE_sse_vmsminv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_vmsminv4sf3_round ((TARGET_AVX512F) && (TARGET_SSE))
#define HAVE_sse_vmsminv4sf3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE)))
#define HAVE_sse2_vmsmaxv2df3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_sse2_vmsmaxv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_vmsmaxv2df3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_vmsmaxv2df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2))))
#define HAVE_sse2_vmsminv2df3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_sse2_vmsminv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_vmsminv2df3_round ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2)))
#define HAVE_sse2_vmsminv2df3_mask_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && ((TARGET_SSE) && (TARGET_SSE2))))
#define HAVE_sse3_haddv2df3 (TARGET_SSE3)
#define HAVE_reduc_plus_scal_v2df (TARGET_SSE)
#define HAVE_reduc_plus_scal_v4sf (TARGET_SSE)
#define HAVE_reduc_plus_scal_v8hf (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_reduc_plus_scal_v8hi (TARGET_SSE2)
#define HAVE_reduc_plus_scal_v4si (TARGET_SSE2)
#define HAVE_reduc_plus_scal_v2di (TARGET_SSE2)
#define HAVE_reduc_plus_scal_v16qi (TARGET_SSE2)
#define HAVE_reduc_plus_scal_v4df (TARGET_AVX)
#define HAVE_reduc_plus_scal_v8sf (TARGET_AVX)
#define HAVE_reduc_plus_scal_v16hf (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_reduc_plus_scal_v8df (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_plus_scal_v16sf (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_plus_scal_v32hf (TARGET_AVX512FP16 && TARGET_AVX512VL && TARGET_EVEX512)
#define HAVE_reduc_plus_scal_v32qi (TARGET_AVX)
#define HAVE_reduc_plus_scal_v16hi (TARGET_AVX)
#define HAVE_reduc_plus_scal_v8si (TARGET_AVX)
#define HAVE_reduc_plus_scal_v4di (TARGET_AVX)
#define HAVE_reduc_plus_scal_v64qi (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_plus_scal_v32hi (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_plus_scal_v16si (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_plus_scal_v8di (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_smax_scal_v8hf (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_reduc_smin_scal_v8hf (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_reduc_smax_scal_v4sf (TARGET_SSE)
#define HAVE_reduc_smin_scal_v4sf (TARGET_SSE)
#define HAVE_reduc_smax_scal_v2df (TARGET_SSE)
#define HAVE_reduc_smin_scal_v2df (TARGET_SSE)
#define HAVE_reduc_smax_scal_v4si (TARGET_SSE2)
#define HAVE_reduc_smin_scal_v4si (TARGET_SSE2)
#define HAVE_reduc_smax_scal_v8hi (TARGET_SSE2)
#define HAVE_reduc_smin_scal_v8hi (TARGET_SSE2)
#define HAVE_reduc_smax_scal_v16qi (TARGET_SSE2)
#define HAVE_reduc_smin_scal_v16qi (TARGET_SSE2)
#define HAVE_reduc_smax_scal_v2di (TARGET_SSE4_2)
#define HAVE_reduc_smin_scal_v2di (TARGET_SSE4_2)
#define HAVE_reduc_smax_scal_v32qi (TARGET_AVX2)
#define HAVE_reduc_smin_scal_v32qi (TARGET_AVX2)
#define HAVE_reduc_smax_scal_v16hi (TARGET_AVX2)
#define HAVE_reduc_smin_scal_v16hi (TARGET_AVX2)
#define HAVE_reduc_smax_scal_v16hf (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_reduc_smin_scal_v16hf (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_reduc_smax_scal_v8si (TARGET_AVX2)
#define HAVE_reduc_smin_scal_v8si (TARGET_AVX2)
#define HAVE_reduc_smax_scal_v4di (TARGET_AVX2)
#define HAVE_reduc_smin_scal_v4di (TARGET_AVX2)
#define HAVE_reduc_smax_scal_v8sf (TARGET_AVX)
#define HAVE_reduc_smin_scal_v8sf (TARGET_AVX)
#define HAVE_reduc_smax_scal_v4df (TARGET_AVX)
#define HAVE_reduc_smin_scal_v4df (TARGET_AVX)
#define HAVE_reduc_smax_scal_v64qi (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_reduc_smin_scal_v64qi (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_reduc_smax_scal_v32hf (TARGET_AVX512FP16 && TARGET_AVX512VL && TARGET_EVEX512)
#define HAVE_reduc_smin_scal_v32hf (TARGET_AVX512FP16 && TARGET_AVX512VL && TARGET_EVEX512)
#define HAVE_reduc_smax_scal_v32hi (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_reduc_smin_scal_v32hi (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_reduc_smax_scal_v16si (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_smin_scal_v16si (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_smax_scal_v8di (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_smin_scal_v8di (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_smax_scal_v16sf (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_smin_scal_v16sf (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_smax_scal_v8df (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_smin_scal_v8df (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_umax_scal_v16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_reduc_umin_scal_v16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_reduc_umax_scal_v8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_reduc_umin_scal_v8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_reduc_umax_scal_v32hi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_reduc_umin_scal_v32hi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_reduc_umax_scal_v64qi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_reduc_umin_scal_v64qi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_reduc_umax_scal_v32qi (TARGET_AVX2)
#define HAVE_reduc_umin_scal_v32qi (TARGET_AVX2)
#define HAVE_reduc_umax_scal_v16hi (TARGET_AVX2)
#define HAVE_reduc_umin_scal_v16hi (TARGET_AVX2)
#define HAVE_reduc_umax_scal_v8si (TARGET_AVX2)
#define HAVE_reduc_umin_scal_v8si (TARGET_AVX2)
#define HAVE_reduc_umax_scal_v4di (TARGET_AVX2)
#define HAVE_reduc_umin_scal_v4di (TARGET_AVX2)
#define HAVE_reduc_umin_scal_v8hi (TARGET_SSE4_1)
#define HAVE_reduc_and_scal_v16qi (TARGET_SSE2)
#define HAVE_reduc_ior_scal_v16qi (TARGET_SSE2)
#define HAVE_reduc_xor_scal_v16qi (TARGET_SSE2)
#define HAVE_reduc_and_scal_v8hi (TARGET_SSE2)
#define HAVE_reduc_ior_scal_v8hi (TARGET_SSE2)
#define HAVE_reduc_xor_scal_v8hi (TARGET_SSE2)
#define HAVE_reduc_and_scal_v4si (TARGET_SSE2)
#define HAVE_reduc_ior_scal_v4si (TARGET_SSE2)
#define HAVE_reduc_xor_scal_v4si (TARGET_SSE2)
#define HAVE_reduc_and_scal_v2di (TARGET_SSE2)
#define HAVE_reduc_ior_scal_v2di (TARGET_SSE2)
#define HAVE_reduc_xor_scal_v2di (TARGET_SSE2)
#define HAVE_reduc_and_scal_v32qi (TARGET_AVX)
#define HAVE_reduc_ior_scal_v32qi (TARGET_AVX)
#define HAVE_reduc_xor_scal_v32qi (TARGET_AVX)
#define HAVE_reduc_and_scal_v16hi (TARGET_AVX)
#define HAVE_reduc_ior_scal_v16hi (TARGET_AVX)
#define HAVE_reduc_xor_scal_v16hi (TARGET_AVX)
#define HAVE_reduc_and_scal_v8si (TARGET_AVX)
#define HAVE_reduc_ior_scal_v8si (TARGET_AVX)
#define HAVE_reduc_xor_scal_v8si (TARGET_AVX)
#define HAVE_reduc_and_scal_v4di (TARGET_AVX)
#define HAVE_reduc_ior_scal_v4di (TARGET_AVX)
#define HAVE_reduc_xor_scal_v4di (TARGET_AVX)
#define HAVE_reduc_and_scal_v64qi (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_ior_scal_v64qi (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_xor_scal_v64qi (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_and_scal_v32hi (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_ior_scal_v32hi (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_xor_scal_v32hi (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_and_scal_v16si (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_ior_scal_v16si (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_xor_scal_v16si (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_and_scal_v8di (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_ior_scal_v8di (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_reduc_xor_scal_v8di (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_vec_cmpv16sihi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_cmpv8siqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vec_cmpv4siqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vec_cmpv8diqi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_cmpv4diqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vec_cmpv2diqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vec_cmpv32hfsi ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_vec_cmpv16hfhi ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_vec_cmpv8hfqi ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_vec_cmpv16sfhi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_cmpv8sfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vec_cmpv4sfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vec_cmpv8dfqi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_cmpv4dfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vec_cmpv2dfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vec_cmpv64qidi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_vec_cmpv16qihi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vec_cmpv32qisi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vec_cmpv32hisi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_vec_cmpv16hihi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vec_cmpv8hiqi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vec_cmpv32bfsi (TARGET_AVX10_2)
#define HAVE_vec_cmpv16bfhi (TARGET_AVX10_2)
#define HAVE_vec_cmpv8bfqi (TARGET_AVX10_2)
#define HAVE_vec_cmpv32qiv32qi (TARGET_AVX2)
#define HAVE_vec_cmpv16hiv16hi (TARGET_AVX2)
#define HAVE_vec_cmpv8siv8si (TARGET_AVX2)
#define HAVE_vec_cmpv4div4di (TARGET_AVX2)
#define HAVE_vec_cmpv16qiv16qi (TARGET_SSE2)
#define HAVE_vec_cmpv8hiv8hi (TARGET_SSE2)
#define HAVE_vec_cmpv4siv4si (TARGET_SSE2)
#define HAVE_vec_cmpv2div2di (TARGET_SSE4_2)
#define HAVE_vec_cmpv8sfv8si (TARGET_AVX)
#define HAVE_vec_cmpv4dfv4di (TARGET_AVX)
#define HAVE_vec_cmpv4sfv4si (TARGET_SSE)
#define HAVE_vec_cmpv2dfv2di ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_vec_cmpuv16sihi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_cmpuv8siqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vec_cmpuv4siqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vec_cmpuv8diqi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vec_cmpuv4diqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vec_cmpuv2diqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vec_cmpuv64qidi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_vec_cmpuv16qihi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vec_cmpuv32qisi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vec_cmpuv32hisi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_vec_cmpuv16hihi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vec_cmpuv8hiqi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vec_cmpuv32qiv32qi (TARGET_AVX2)
#define HAVE_vec_cmpuv16hiv16hi (TARGET_AVX2)
#define HAVE_vec_cmpuv8siv8si (TARGET_AVX2)
#define HAVE_vec_cmpuv4div4di (TARGET_AVX2)
#define HAVE_vec_cmpuv16qiv16qi (TARGET_SSE2)
#define HAVE_vec_cmpuv8hiv8hi (TARGET_SSE2)
#define HAVE_vec_cmpuv4siv4si (TARGET_SSE2)
#define HAVE_vec_cmpuv2div2di (TARGET_SSE4_2)
#define HAVE_vec_cmpeqv2div2di (TARGET_SSE2)
#define HAVE_vec_cmpeqv1tiv1ti (TARGET_SSE2)
#define HAVE_vcond_mask_v16sihi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vcond_mask_v8siqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v4siqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v8diqi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vcond_mask_v4diqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v2diqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v16sfhi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vcond_mask_v8sfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v4sfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v8dfqi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vcond_mask_v4dfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v2dfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v64qidi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_vcond_mask_v16qihi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v32qisi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v32hisi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_vcond_mask_v16hihi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v8hiqi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v32hfsi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_vcond_mask_v16hfhi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v8hfqi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v32bfsi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_vcond_mask_v16bfhi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v8bfqi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vcond_mask_v32qiv32qi ((TARGET_AVX) && (TARGET_AVX2))
#define HAVE_vcond_mask_v16hiv16hi ((TARGET_AVX) && (TARGET_AVX2))
#define HAVE_vcond_mask_v8siv8si (TARGET_AVX)
#define HAVE_vcond_mask_v4div4di (TARGET_AVX)
#define HAVE_vcond_mask_v16qiv16qi (TARGET_SSE2)
#define HAVE_vcond_mask_v8hiv8hi (TARGET_SSE2)
#define HAVE_vcond_mask_v4siv4si (TARGET_SSE2)
#define HAVE_vcond_mask_v2div2di (TARGET_SSE2)
#define HAVE_vcond_mask_v1tiv1ti (TARGET_SSE2)
#define HAVE_vcond_mask_v8sfv8si (TARGET_AVX)
#define HAVE_vcond_mask_v4dfv4di (TARGET_AVX)
#define HAVE_vcond_mask_v4sfv4si (TARGET_SSE)
#define HAVE_vcond_mask_v2dfv2di ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_vcond_mask_qiqi (TARGET_AVX512F)
#define HAVE_vcond_mask_hihi (TARGET_AVX512F)
#define HAVE_vcond_mask_sisi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_vcond_mask_didi ((TARGET_AVX512F) && (TARGET_AVX512BW))
#define HAVE_andv16bf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_AVX))
#define HAVE_iorv16bf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_AVX))
#define HAVE_xorv16bf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_AVX))
#define HAVE_andv8bf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_SSE2))
#define HAVE_iorv8bf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_SSE2))
#define HAVE_xorv8bf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_SSE2))
#define HAVE_andv16hf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_AVX))
#define HAVE_iorv16hf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_AVX))
#define HAVE_xorv16hf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_AVX))
#define HAVE_andv8hf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_SSE2))
#define HAVE_iorv8hf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_SSE2))
#define HAVE_xorv8hf3 ((TARGET_SSE && 1 \
   && (!false || 16 != 16)) && (TARGET_SSE2))
#define HAVE_andv8sf3 ((TARGET_SSE && 1 \
   && (!false || 32 != 16)) && (TARGET_AVX))
#define HAVE_andv8sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && TARGET_AVX512VL \
   && (!true || 32 != 16)) && (TARGET_AVX)))
#define HAVE_iorv8sf3 ((TARGET_SSE && 1 \
   && (!false || 32 != 16)) && (TARGET_AVX))
#define HAVE_iorv8sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && TARGET_AVX512VL \
   && (!true || 32 != 16)) && (TARGET_AVX)))
#define HAVE_xorv8sf3 ((TARGET_SSE && 1 \
   && (!false || 32 != 16)) && (TARGET_AVX))
#define HAVE_xorv8sf3_mask ((TARGET_AVX512F) && ((TARGET_SSE && TARGET_AVX512VL \
   && (!true || 32 != 16)) && (TARGET_AVX)))
#define HAVE_andv4sf3 (TARGET_SSE && 1 \
   && (!false || 32 != 16))
#define HAVE_andv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE && TARGET_AVX512VL \
   && (!true || 32 != 16)))
#define HAVE_iorv4sf3 (TARGET_SSE && 1 \
   && (!false || 32 != 16))
#define HAVE_iorv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE && TARGET_AVX512VL \
   && (!true || 32 != 16)))
#define HAVE_xorv4sf3 (TARGET_SSE && 1 \
   && (!false || 32 != 16))
#define HAVE_xorv4sf3_mask ((TARGET_AVX512F) && (TARGET_SSE && TARGET_AVX512VL \
   && (!true || 32 != 16)))
#define HAVE_andv4df3 ((TARGET_SSE && 1 \
   && (!false || 64 != 16)) && (TARGET_AVX))
#define HAVE_andv4df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && TARGET_AVX512VL \
   && (!true || 64 != 16)) && (TARGET_AVX)))
#define HAVE_iorv4df3 ((TARGET_SSE && 1 \
   && (!false || 64 != 16)) && (TARGET_AVX))
#define HAVE_iorv4df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && TARGET_AVX512VL \
   && (!true || 64 != 16)) && (TARGET_AVX)))
#define HAVE_xorv4df3 ((TARGET_SSE && 1 \
   && (!false || 64 != 16)) && (TARGET_AVX))
#define HAVE_xorv4df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && TARGET_AVX512VL \
   && (!true || 64 != 16)) && (TARGET_AVX)))
#define HAVE_andv2df3 ((TARGET_SSE && 1 \
   && (!false || 64 != 16)) && (TARGET_SSE2))
#define HAVE_andv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && TARGET_AVX512VL \
   && (!true || 64 != 16)) && (TARGET_SSE2)))
#define HAVE_iorv2df3 ((TARGET_SSE && 1 \
   && (!false || 64 != 16)) && (TARGET_SSE2))
#define HAVE_iorv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && TARGET_AVX512VL \
   && (!true || 64 != 16)) && (TARGET_SSE2)))
#define HAVE_xorv2df3 ((TARGET_SSE && 1 \
   && (!false || 64 != 16)) && (TARGET_SSE2))
#define HAVE_xorv2df3_mask ((TARGET_AVX512F) && ((TARGET_SSE && TARGET_AVX512VL \
   && (!true || 64 != 16)) && (TARGET_SSE2)))
#define HAVE_andv32bf3 ((TARGET_AVX512F && (!false || 16 != 16)) && (TARGET_EVEX512))
#define HAVE_iorv32bf3 ((TARGET_AVX512F && (!false || 16 != 16)) && (TARGET_EVEX512))
#define HAVE_xorv32bf3 ((TARGET_AVX512F && (!false || 16 != 16)) && (TARGET_EVEX512))
#define HAVE_andv32hf3 ((TARGET_AVX512F && (!false || 16 != 16)) && (TARGET_EVEX512))
#define HAVE_iorv32hf3 ((TARGET_AVX512F && (!false || 16 != 16)) && (TARGET_EVEX512))
#define HAVE_xorv32hf3 ((TARGET_AVX512F && (!false || 16 != 16)) && (TARGET_EVEX512))
#define HAVE_andv16sf3 ((TARGET_AVX512F && (!false || 32 != 16)) && (TARGET_EVEX512))
#define HAVE_andv16sf3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && (!true || 32 != 16)) && (TARGET_EVEX512)))
#define HAVE_iorv16sf3 ((TARGET_AVX512F && (!false || 32 != 16)) && (TARGET_EVEX512))
#define HAVE_iorv16sf3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && (!true || 32 != 16)) && (TARGET_EVEX512)))
#define HAVE_xorv16sf3 ((TARGET_AVX512F && (!false || 32 != 16)) && (TARGET_EVEX512))
#define HAVE_xorv16sf3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && (!true || 32 != 16)) && (TARGET_EVEX512)))
#define HAVE_andv8df3 ((TARGET_AVX512F && (!false || 64 != 16)) && (TARGET_EVEX512))
#define HAVE_andv8df3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && (!true || 64 != 16)) && (TARGET_EVEX512)))
#define HAVE_iorv8df3 ((TARGET_AVX512F && (!false || 64 != 16)) && (TARGET_EVEX512))
#define HAVE_iorv8df3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && (!true || 64 != 16)) && (TARGET_EVEX512)))
#define HAVE_xorv8df3 ((TARGET_AVX512F && (!false || 64 != 16)) && (TARGET_EVEX512))
#define HAVE_xorv8df3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F && (!true || 64 != 16)) && (TARGET_EVEX512)))
#define HAVE_copysignv32bf3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_copysignv16bf3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_copysignv8bf3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_copysignv32hf3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_copysignv16hf3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_copysignv8hf3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_copysignv16sf3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_copysignv8sf3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_copysignv4sf3 (TARGET_SSE)
#define HAVE_copysignv8df3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_copysignv4df3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_copysignv2df3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_xorsignv32bf3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_xorsignv16bf3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_xorsignv8bf3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_xorsignv32hf3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_xorsignv16hf3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_xorsignv8hf3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_xorsignv16sf3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_xorsignv8sf3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_xorsignv4sf3 (TARGET_SSE)
#define HAVE_xorsignv8df3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_xorsignv4df3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_xorsignv2df3 ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_signbitv16sf2 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_signbitv8sf2 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_signbitv4sf2 (TARGET_SSE2)
#define HAVE_andtf3 (TARGET_SSE)
#define HAVE_iortf3 (TARGET_SSE)
#define HAVE_xortf3 (TARGET_SSE)
#define HAVE_fmasf4 (TARGET_SSE_MATH && (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F))
#define HAVE_fmadf4 (TARGET_SSE_MATH && (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F))
#define HAVE_fmav4sf4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fmav2df4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fmav8sf4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fmav4df4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fmav16sf4 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fmav8df4 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fmahf4 (TARGET_AVX512FP16)
#define HAVE_fmav8hf4 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fmav16hf4 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fmav32hf4 (TARGET_AVX512FP16 && TARGET_EVEX512)
#define HAVE_fmav8bf4 (TARGET_AVX10_2)
#define HAVE_fmav16bf4 (TARGET_AVX10_2)
#define HAVE_fmav32bf4 (TARGET_AVX10_2)
#define HAVE_fmssf4 (TARGET_SSE_MATH && (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F))
#define HAVE_fmsdf4 (TARGET_SSE_MATH && (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F))
#define HAVE_fmsv4sf4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fmsv2df4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fmsv8sf4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fmsv4df4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fmsv16sf4 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fmsv8df4 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fmshf4 (TARGET_AVX512FP16)
#define HAVE_fmsv8hf4 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fmsv16hf4 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fmsv32hf4 (TARGET_AVX512FP16 && TARGET_EVEX512)
#define HAVE_fmsv8bf4 (TARGET_AVX10_2)
#define HAVE_fmsv16bf4 (TARGET_AVX10_2)
#define HAVE_fmsv32bf4 (TARGET_AVX10_2)
#define HAVE_fnmasf4 (TARGET_SSE_MATH && (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F))
#define HAVE_fnmadf4 (TARGET_SSE_MATH && (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F))
#define HAVE_fnmav4sf4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fnmav2df4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fnmav8sf4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fnmav4df4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fnmav16sf4 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fnmav8df4 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fnmahf4 (TARGET_AVX512FP16)
#define HAVE_fnmav8hf4 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fnmav16hf4 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fnmav32hf4 (TARGET_AVX512FP16 && TARGET_EVEX512)
#define HAVE_fnmav8bf4 (TARGET_AVX10_2)
#define HAVE_fnmav16bf4 (TARGET_AVX10_2)
#define HAVE_fnmav32bf4 (TARGET_AVX10_2)
#define HAVE_fnmssf4 (TARGET_SSE_MATH && (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F))
#define HAVE_fnmsdf4 (TARGET_SSE_MATH && (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F))
#define HAVE_fnmsv4sf4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fnmsv2df4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fnmsv8sf4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fnmsv4df4 (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fnmsv16sf4 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fnmsv8df4 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fnmshf4 (TARGET_AVX512FP16)
#define HAVE_fnmsv8hf4 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fnmsv16hf4 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fnmsv32hf4 (TARGET_AVX512FP16 && TARGET_EVEX512)
#define HAVE_fnmsv8bf4 (TARGET_AVX10_2)
#define HAVE_fnmsv16bf4 (TARGET_AVX10_2)
#define HAVE_fnmsv32bf4 (TARGET_AVX10_2)
#define HAVE_fma4i_fmadd_sf (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F)
#define HAVE_fma4i_fmadd_df (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F)
#define HAVE_fma4i_fmadd_v4sf (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fmadd_v2df (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fmadd_v8sf (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fmadd_v4df (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fmadd_v16sf (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fma4i_fmadd_v8df (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fma4i_fmsub_sf (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F)
#define HAVE_fma4i_fmsub_df (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F)
#define HAVE_fma4i_fmsub_v4sf (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fmsub_v2df (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fmsub_v8sf (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fmsub_v4df (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fmsub_v16sf (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fma4i_fmsub_v8df (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fma4i_fnmadd_sf (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F)
#define HAVE_fma4i_fnmadd_df (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F)
#define HAVE_fma4i_fnmadd_v4sf (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fnmadd_v2df (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fnmadd_v8sf (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fnmadd_v4df (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fnmadd_v16sf (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fma4i_fnmadd_v8df (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fma4i_fnmsub_sf (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F)
#define HAVE_fma4i_fnmsub_df (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F)
#define HAVE_fma4i_fnmsub_v4sf (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fnmsub_v2df (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fnmsub_v8sf (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fnmsub_v4df (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512VL)
#define HAVE_fma4i_fnmsub_v16sf (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_fma4i_fnmsub_v8df (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512bw_fmadd_v32hf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fmadd_v32hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fmadd_v16hf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_fmadd_v16hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_fmadd_v8hf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fmadd_v8hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_fmadd_v16sf_maskz ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fmadd_v16sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmadd_v8sf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmadd_v8sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmadd_v4sf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmadd_v4sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fmadd_v8df_maskz ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fmadd_v8df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmadd_v4df_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmadd_v4df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmadd_v2df_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmadd_v2df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_cond_fmav32hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_cond_fmav16hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_fmav8hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_fmav16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_fmav8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_fmav4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_fmav8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_fmav4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_fmav2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512bw_fmsub_v32hf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fmsub_v32hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fmsub_v16hf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_fmsub_v16hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_fmsub_v8hf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fmsub_v8hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_fmsub_v16sf_maskz ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fmsub_v16sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmsub_v8sf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsub_v8sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmsub_v4sf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsub_v4sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fmsub_v8df_maskz ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fmsub_v8df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmsub_v4df_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsub_v4df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmsub_v2df_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsub_v2df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_cond_fmsv32hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_cond_fmsv16hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_fmsv8hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_fmsv16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_fmsv8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_fmsv4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_fmsv8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_fmsv4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_fmsv2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512bw_fnmadd_v32hf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fnmadd_v32hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fnmadd_v16hf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_fnmadd_v16hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_fnmadd_v8hf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fnmadd_v8hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_fnmadd_v16sf_maskz ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fnmadd_v16sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fnmadd_v8sf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmadd_v8sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fnmadd_v4sf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmadd_v4sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fnmadd_v8df_maskz ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fnmadd_v8df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fnmadd_v4df_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmadd_v4df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fnmadd_v2df_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmadd_v2df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_cond_fnmav32hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_cond_fnmav16hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_fnmav8hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_fnmav16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_fnmav8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_fnmav4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_fnmav8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_fnmav4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_fnmav2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512bw_fnmsub_v32hf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fnmsub_v32hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fnmsub_v16hf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_fnmsub_v16hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_fnmsub_v8hf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fnmsub_v8hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_fnmsub_v16sf_maskz ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fnmsub_v16sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fnmsub_v8sf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmsub_v8sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fnmsub_v4sf_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmsub_v4sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fnmsub_v8df_maskz ((TARGET_AVX512F && 1) && (TARGET_EVEX512))
#define HAVE_avx512f_fnmsub_v8df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fnmsub_v4df_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmsub_v4df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fnmsub_v2df_maskz ((TARGET_AVX512F && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fnmsub_v2df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F && 1) && (TARGET_AVX512VL)))
#define HAVE_cond_fnmsv32hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_cond_fnmsv16hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_fnmsv8hf ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_cond_fnmsv16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_fnmsv8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_fnmsv4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_fnmsv8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_fnmsv4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_fnmsv2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vec_fmaddsubv32hf4 ((TARGET_FMA || TARGET_FMA4 || (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_vec_fmaddsubv16hf4 ((TARGET_FMA || TARGET_FMA4 || (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_vec_fmaddsubv8hf4 ((TARGET_FMA || TARGET_FMA4 || (16 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_vec_fmaddsubv16sf4 ((TARGET_FMA || TARGET_FMA4 || (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_fmaddsubv8sf4 ((TARGET_FMA || TARGET_FMA4 || (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX))
#define HAVE_vec_fmaddsubv4sf4 (TARGET_FMA || TARGET_FMA4 || (16 == 64 || TARGET_AVX512VL))
#define HAVE_vec_fmaddsubv8df4 ((TARGET_FMA || TARGET_FMA4 || (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_fmaddsubv4df4 ((TARGET_FMA || TARGET_FMA4 || (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX))
#define HAVE_vec_fmaddsubv2df4 ((TARGET_FMA || TARGET_FMA4 || (16 == 64 || TARGET_AVX512VL)) && (TARGET_SSE2))
#define HAVE_vec_fmsubaddv32hf4 ((TARGET_FMA || TARGET_FMA4 || (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_vec_fmsubaddv16hf4 ((TARGET_FMA || TARGET_FMA4 || (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_vec_fmsubaddv8hf4 ((TARGET_FMA || TARGET_FMA4 || (16 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_vec_fmsubaddv16sf4 ((TARGET_FMA || TARGET_FMA4 || (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_fmsubaddv8sf4 ((TARGET_FMA || TARGET_FMA4 || (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX))
#define HAVE_vec_fmsubaddv4sf4 (TARGET_FMA || TARGET_FMA4 || (16 == 64 || TARGET_AVX512VL))
#define HAVE_vec_fmsubaddv8df4 ((TARGET_FMA || TARGET_FMA4 || (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_fmsubaddv4df4 ((TARGET_FMA || TARGET_FMA4 || (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX))
#define HAVE_vec_fmsubaddv2df4 ((TARGET_FMA || TARGET_FMA4 || (16 == 64 || TARGET_AVX512VL)) && (TARGET_SSE2))
#define HAVE_fmaddsub_v16sf ((TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_fmaddsub_v8sf ((TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F) && (TARGET_AVX))
#define HAVE_fmaddsub_v4sf (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F)
#define HAVE_fmaddsub_v8df ((TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_fmaddsub_v4df ((TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F) && (TARGET_AVX))
#define HAVE_fmaddsub_v2df ((TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512bw_fmaddsub_v32hf_maskz ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fmaddsub_v32hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fmaddsub_v16hf_maskz ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v16hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_fmaddsub_v8hf_maskz ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fmaddsub_v8hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_fmaddsub_v16sf_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmaddsub_v16sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmaddsub_v8sf_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v8sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmaddsub_v4sf_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v4sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fmaddsub_v8df_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmaddsub_v8df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmaddsub_v4df_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v4df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmaddsub_v2df_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddsub_v2df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_fmsubadd_v32hf_maskz ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_avx512bw_fmsubadd_v32hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_EVEX512)))
#define HAVE_avx512vl_fmsubadd_v16hf_maskz ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v16hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512fp16_fmsubadd_v8hf_maskz ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_avx512fp16_fmsubadd_v8hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16 && TARGET_AVX512VL)))
#define HAVE_avx512f_fmsubadd_v16sf_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmsubadd_v16sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmsubadd_v8sf_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v8sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmsubadd_v4sf_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v4sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fmsubadd_v8df_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fmsubadd_v8df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmsubadd_v4df_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v4df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fmsubadd_v2df_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmsubadd_v2df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_fmai_vmfmadd_v8hf ((TARGET_FMA) && (TARGET_AVX512FP16))
#define HAVE_fmai_vmfmadd_v8hf_round ((TARGET_AVX512F) && ((TARGET_FMA) && (TARGET_AVX512FP16)))
#define HAVE_fmai_vmfmadd_v4sf (TARGET_FMA)
#define HAVE_fmai_vmfmadd_v4sf_round ((TARGET_AVX512F) && (TARGET_FMA))
#define HAVE_fmai_vmfmadd_v2df ((TARGET_FMA) && (TARGET_SSE2))
#define HAVE_fmai_vmfmadd_v2df_round ((TARGET_AVX512F) && ((TARGET_FMA) && (TARGET_SSE2)))
#define HAVE_fmai_vmfmsub_v8hf ((TARGET_FMA) && (TARGET_AVX512FP16))
#define HAVE_fmai_vmfmsub_v8hf_round ((TARGET_AVX512F) && ((TARGET_FMA) && (TARGET_AVX512FP16)))
#define HAVE_fmai_vmfmsub_v4sf (TARGET_FMA)
#define HAVE_fmai_vmfmsub_v4sf_round ((TARGET_AVX512F) && (TARGET_FMA))
#define HAVE_fmai_vmfmsub_v2df ((TARGET_FMA) && (TARGET_SSE2))
#define HAVE_fmai_vmfmsub_v2df_round ((TARGET_AVX512F) && ((TARGET_FMA) && (TARGET_SSE2)))
#define HAVE_fmai_vmfnmadd_v8hf ((TARGET_FMA) && (TARGET_AVX512FP16))
#define HAVE_fmai_vmfnmadd_v8hf_round ((TARGET_AVX512F) && ((TARGET_FMA) && (TARGET_AVX512FP16)))
#define HAVE_fmai_vmfnmadd_v4sf (TARGET_FMA)
#define HAVE_fmai_vmfnmadd_v4sf_round ((TARGET_AVX512F) && (TARGET_FMA))
#define HAVE_fmai_vmfnmadd_v2df ((TARGET_FMA) && (TARGET_SSE2))
#define HAVE_fmai_vmfnmadd_v2df_round ((TARGET_AVX512F) && ((TARGET_FMA) && (TARGET_SSE2)))
#define HAVE_fmai_vmfnmsub_v8hf ((TARGET_FMA) && (TARGET_AVX512FP16))
#define HAVE_fmai_vmfnmsub_v8hf_round ((TARGET_AVX512F) && ((TARGET_FMA) && (TARGET_AVX512FP16)))
#define HAVE_fmai_vmfnmsub_v4sf (TARGET_FMA)
#define HAVE_fmai_vmfnmsub_v4sf_round ((TARGET_AVX512F) && (TARGET_FMA))
#define HAVE_fmai_vmfnmsub_v2df ((TARGET_FMA) && (TARGET_SSE2))
#define HAVE_fmai_vmfnmsub_v2df_round ((TARGET_AVX512F) && ((TARGET_FMA) && (TARGET_SSE2)))
#define HAVE_avx512f_vmfmadd_v8hf_maskz ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_vmfmadd_v8hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vmfmadd_v4sf_maskz (TARGET_AVX512F)
#define HAVE_avx512f_vmfmadd_v4sf_maskz_round (TARGET_AVX512F)
#define HAVE_avx512f_vmfmadd_v2df_maskz ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_vmfmadd_v2df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_vmfnmadd_v8hf_maskz ((TARGET_AVX512F) && (TARGET_AVX512FP16))
#define HAVE_avx512f_vmfnmadd_v8hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512FP16)))
#define HAVE_avx512f_vmfnmadd_v4sf_maskz (TARGET_AVX512F)
#define HAVE_avx512f_vmfnmadd_v4sf_maskz_round (TARGET_AVX512F)
#define HAVE_avx512f_vmfnmadd_v2df_maskz ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_vmfnmadd_v2df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_fma4i_vmfmadd_v4sf (TARGET_FMA4)
#define HAVE_fma4i_vmfmadd_v2df ((TARGET_FMA4) && (TARGET_SSE2))
#define HAVE_avx512bw_fmaddc_v32hf_mask1 ((TARGET_AVX512FP16 && 1) && (TARGET_EVEX512))
#define HAVE_avx512bw_fmaddc_v32hf_mask1_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmaddc_v16hf_mask1 ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddc_v16hf_mask1_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fmaddc_v8hf_mask1 ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fmaddc_v8hf_mask1_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_fmaddc_v32hf_maskz ((TARGET_AVX512FP16 && 1) && (TARGET_EVEX512))
#define HAVE_avx512bw_fmaddc_v32hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fmaddc_v16hf_maskz ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fmaddc_v16hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fmaddc_v8hf_maskz ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fmaddc_v8hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_fcmaddc_v32hf_mask1 ((TARGET_AVX512FP16 && 1) && (TARGET_EVEX512))
#define HAVE_avx512bw_fcmaddc_v32hf_mask1_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fcmaddc_v16hf_mask1 ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fcmaddc_v16hf_mask1_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fcmaddc_v8hf_mask1 ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fcmaddc_v8hf_mask1_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_fcmaddc_v32hf_maskz ((TARGET_AVX512FP16 && 1) && (TARGET_EVEX512))
#define HAVE_avx512bw_fcmaddc_v32hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fcmaddc_v16hf_maskz ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fcmaddc_v16hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL)))
#define HAVE_avx512fp16_fcmaddc_v8hf_maskz ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fcmaddc_v8hf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512FP16 && 1) && (TARGET_AVX512VL)))
#define HAVE_cmlav32hf4 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_cmla_conjv32hf4 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_cmlav16hf4 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_cmla_conjv16hf4 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_cmlav8hf4 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_cmla_conjv8hf4 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_cmulv32hf3 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_cmul_conjv32hf3 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_cmulv16hf3 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_cmul_conjv16hf3 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_cmulv8hf3 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_cmul_conjv8hf3 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx512fp16_fmaddcsh_v8hf_maskz (TARGET_AVX512FP16 && 1)
#define HAVE_avx512fp16_fmaddcsh_v8hf_maskz_round ((TARGET_AVX512F) && (TARGET_AVX512FP16 && 1))
#define HAVE_avx512fp16_fmaddcsh_v8hf_mask1 (TARGET_AVX512FP16 && 1)
#define HAVE_avx512fp16_fmaddcsh_v8hf_mask1_round ((TARGET_AVX512F) && (TARGET_AVX512FP16 && 1))
#define HAVE_avx512fp16_fcmaddcsh_v8hf_maskz (TARGET_AVX512FP16 && 1)
#define HAVE_avx512fp16_fcmaddcsh_v8hf_maskz_round ((TARGET_AVX512F) && (TARGET_AVX512FP16 && 1))
#define HAVE_avx512fp16_fcmaddcsh_v8hf_mask1 (TARGET_AVX512FP16 && 1)
#define HAVE_avx512fp16_fcmaddcsh_v8hf_mask1_round ((TARGET_AVX512F) && (TARGET_AVX512FP16 && 1))
#define HAVE_avx512fp16_fcmaddcsh_v8hf_mask3 (TARGET_AVX512FP16 && 1)
#define HAVE_avx512fp16_fcmaddcsh_v8hf_mask3_round ((TARGET_AVX512F) && (TARGET_AVX512FP16 && 1))
#define HAVE_avx512fp16_fmaddcsh_v8hf_mask3 (TARGET_AVX512FP16 && 1)
#define HAVE_avx512fp16_fmaddcsh_v8hf_mask3_round ((TARGET_AVX512F) && (TARGET_AVX512FP16 && 1))
#define HAVE_vec_unpacks_lo_v32hf ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_vec_unpacks_lo_v16hf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_unpacks_lo_v8hf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_unpacks_hi_v32hf ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_vec_unpacks_hi_v16hf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_unpacks_hi_v8hf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_lrintv32hfv32hi2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_lrintv16hfv16hi2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_lrintv8hfv8hi2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_floatv8hiv8hf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_floatunsv8hiv8hf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_floatv16hiv16hf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_floatunsv16hiv16hf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_floatv32hiv32hf2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_floatunsv32hiv32hf2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_floatv8siv8hf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_floatunsv8siv8hf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_floatv16siv16hf2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_floatunsv16siv16hf2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_floatv8div8hf2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_floatunsv8div8hf2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_floatv4siv4hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_floatunsv4siv4hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_floatv4div4hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_floatunsv4div4hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_floatv4siv4hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_floatunsv4siv4hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_floatv4div4hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_floatunsv4div4hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_vcvtdq2ph_v4si_mask (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_vcvtudq2ph_v4si_mask (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_vcvtqq2ph_v4di_mask (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_vcvtuqq2ph_v4di_mask (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_floatv2div2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_floatunsv2div2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_floatv2div2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_floatunsv2div2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_vcvtqq2ph_v2di_mask (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_vcvtuqq2ph_v2di_mask (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fix_truncv8hfv8hi2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_fixuns_truncv8hfv8hi2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_fix_truncv16hfv16hi2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_fixuns_truncv16hfv16hi2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_fix_truncv32hfv32hi2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_fixuns_truncv32hfv32hi2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_fix_truncv8hfv8si2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_fixuns_truncv8hfv8si2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_fix_truncv16hfv16si2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_fixuns_truncv16hfv16si2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_fix_truncv8hfv8di2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_fixuns_truncv8hfv8di2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_fix_truncv4hfv4si2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fixuns_truncv4hfv4si2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fix_truncv4hfv4di2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fixuns_truncv4hfv4di2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fix_truncv2hfv2di2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_fixuns_truncv2hfv2di2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_extendv8hfv8df2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_extendv16hfv16sf2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_extendv8hfv8sf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_extendv4hfv4df2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_extendv4hfv4sf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_extendv2hfv2df2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_truncv8dfv8hf2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_truncv16sfv16hf2 ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_truncv8sfv8hf2 ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_truncv4dfv4hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_truncv4sfv4hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_truncv4dfv4hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_truncv4sfv4hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_vcvtpd2ph_v4df_mask (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_vcvtps2ph_v4sf_mask (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_truncv2dfv2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_truncv2dfv2hf2 (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_avx512fp16_vcvtpd2ph_v2df_mask (TARGET_AVX512FP16 && TARGET_AVX512VL)
#define HAVE_floatunsv16siv16sf2 ((TARGET_SSE2 && (V16SFmode == V4SFmode || TARGET_AVX2)) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_floatunsv8siv8sf2 ((TARGET_SSE2 && (V8SFmode == V4SFmode || TARGET_AVX2)) && (TARGET_AVX))
#define HAVE_floatunsv4siv4sf2 (TARGET_SSE2 && (V4SFmode == V4SFmode || TARGET_AVX2))
#define HAVE_fixuns_truncv8sfv8si2 ((TARGET_SSE2) && (TARGET_AVX))
#define HAVE_fixuns_truncv4sfv4si2 (TARGET_SSE2)
#define HAVE_avx512dq_floatv2div2sf2 (TARGET_AVX512DQ && TARGET_AVX512VL)
#define HAVE_avx512dq_floatunsv2div2sf2 (TARGET_AVX512DQ && TARGET_AVX512VL)
#define HAVE_floatv2div2sf2 (TARGET_AVX512DQ && TARGET_AVX512VL)
#define HAVE_floatunsv2div2sf2 (TARGET_AVX512DQ && TARGET_AVX512VL)
#define HAVE_vec_packs_float_v8di ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_vec_packu_float_v8di ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_vec_packs_float_v4di ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_vec_packu_float_v4di ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_vec_packs_float_v2di ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_vec_packu_float_v2di ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_vec_packs_float_v16si ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_vec_packu_float_v16si ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_vec_packs_float_v8si ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_packu_float_v8si ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_packs_float_v4si ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_packu_float_v4si ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_floatv2div2sf2_mask (TARGET_AVX512DQ && TARGET_AVX512VL)
#define HAVE_floatunsv2div2sf2_mask (TARGET_AVX512DQ && TARGET_AVX512VL)
#define HAVE_avx_cvtpd2dq256_2 (TARGET_AVX)
#define HAVE_fix_truncv2sfv2di2 (TARGET_AVX512DQ && TARGET_AVX512VL)
#define HAVE_fixuns_truncv2sfv2di2 (TARGET_AVX512DQ && TARGET_AVX512VL)
#define HAVE_vec_unpack_sfix_trunc_lo_v16sf ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_vec_unpack_ufix_trunc_lo_v16sf ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_vec_unpack_sfix_trunc_lo_v8sf ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_ufix_trunc_lo_v8sf ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_sfix_trunc_lo_v4sf ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_ufix_trunc_lo_v4sf ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_sfix_trunc_hi_v16sf ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_vec_unpack_ufix_trunc_hi_v16sf ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_vec_unpack_sfix_trunc_hi_v8sf ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_ufix_trunc_hi_v8sf ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_sfix_trunc_hi_v4sf ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_ufix_trunc_hi_v4sf ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_sfix_trunc_lo_v32hf ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_vec_unpack_ufix_trunc_lo_v32hf ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_vec_unpack_sfix_trunc_lo_v16hf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_ufix_trunc_lo_v16hf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_sfix_trunc_lo_v8hf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_ufix_trunc_lo_v8hf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_sfix_trunc_hi_v32hf ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_vec_unpack_ufix_trunc_hi_v32hf ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_vec_unpack_sfix_trunc_hi_v16hf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_ufix_trunc_hi_v16hf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_sfix_trunc_hi_v8hf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_unpack_ufix_trunc_hi_v8hf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_avx_cvttpd2dq256_2 (TARGET_AVX)
#define HAVE_sse2_cvtpd2ps (TARGET_SSE2)
#define HAVE_sse2_cvtpd2ps_mask (TARGET_SSE2)
#define HAVE_truncv8dfv8sf2 ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_truncv4dfv4sf2 (TARGET_AVX)
#define HAVE_extendv8sfv8df2 ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_extendv4sfv4df2 (TARGET_AVX)
#define HAVE_avx512bw_cvtmask2bv64qi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_cvtmask2bv16qi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cvtmask2bv32qi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512bw_cvtmask2wv32hi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512vl_cvtmask2wv16hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cvtmask2wv8hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512f_cvtmask2dv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_cvtmask2dv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cvtmask2dv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_cvtmask2qv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_cvtmask2qv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_cvtmask2qv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vec_unpacks_hi_v4sf (TARGET_SSE2)
#define HAVE_vec_unpacks_hi_v8sf (TARGET_AVX)
#define HAVE_vec_unpacks_hi_v16sf (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_vec_unpacks_lo_v4sf (TARGET_SSE2)
#define HAVE_vec_unpacks_lo_v8sf (TARGET_AVX)
#define HAVE_vec_unpacks_float_hi_v32hi ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_unpacks_float_hi_v16hi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacks_float_hi_v8hi (TARGET_SSE2)
#define HAVE_vec_unpacks_float_lo_v32hi ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_unpacks_float_lo_v16hi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacks_float_lo_v8hi (TARGET_SSE2)
#define HAVE_vec_unpacku_float_hi_v32hi ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_unpacku_float_hi_v16hi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacku_float_hi_v8hi (TARGET_SSE2)
#define HAVE_vec_unpacku_float_lo_v32hi ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_unpacku_float_lo_v16hi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacku_float_lo_v8hi (TARGET_SSE2)
#define HAVE_vec_unpacks_float_hi_v4si (TARGET_SSE2)
#define HAVE_vec_unpacks_float_lo_v4si (TARGET_SSE2)
#define HAVE_vec_unpacks_float_hi_v8si (TARGET_AVX)
#define HAVE_vec_unpacks_float_lo_v8si (TARGET_AVX)
#define HAVE_vec_unpacks_float_hi_v16si (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_vec_unpacks_float_lo_v16si (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_vec_unpacku_float_hi_v4si (TARGET_SSE2)
#define HAVE_vec_unpacku_float_lo_v4si (TARGET_SSE2)
#define HAVE_vec_unpacku_float_hi_v8si (TARGET_AVX)
#define HAVE_vec_unpacku_float_hi_v16si (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_vec_unpacku_float_lo_v8si (TARGET_AVX)
#define HAVE_vec_unpacku_float_lo_v16si (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_vec_pack_trunc_v8df ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_pack_trunc_v4df (TARGET_AVX)
#define HAVE_vec_pack_trunc_v16sf ((TARGET_AVX512FP16) && (TARGET_EVEX512))
#define HAVE_vec_pack_trunc_v8sf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_pack_trunc_v4sf ((TARGET_AVX512FP16) && (TARGET_AVX512VL))
#define HAVE_vec_pack_trunc_v2df (TARGET_SSE2)
#define HAVE_vec_pack_sfix_trunc_v8df (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_vec_pack_sfix_trunc_v4df (TARGET_AVX)
#define HAVE_vec_pack_sfix_trunc_v2df (TARGET_SSE2)
#define HAVE_vec_pack_ufix_trunc_v8df ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_pack_ufix_trunc_v4df ((TARGET_SSE2) && (TARGET_AVX))
#define HAVE_vec_pack_ufix_trunc_v2df (TARGET_SSE2)
#define HAVE_avx512f_vec_pack_sfix_v8df (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_vec_pack_sfix_v4df (TARGET_AVX)
#define HAVE_vec_pack_sfix_v2df (TARGET_SSE2)
#define HAVE_sse_movhlps_exp (TARGET_SSE)
#define HAVE_sse_movlhps_exp (TARGET_SSE)
#define HAVE_vec_interleave_highv8sf (TARGET_AVX)
#define HAVE_vec_interleave_lowv8sf (TARGET_AVX)
#define HAVE_avx_shufps256 (TARGET_AVX)
#define HAVE_avx_shufps256_mask ((TARGET_AVX512VL) && (TARGET_AVX))
#define HAVE_sse_shufps (TARGET_SSE)
#define HAVE_sse_shufps_mask ((TARGET_AVX512VL) && (TARGET_SSE))
#define HAVE_sse_loadhps_exp (TARGET_SSE)
#define HAVE_sse_loadlps_exp (TARGET_SSE)
#define HAVE_vec_setv16qi (TARGET_SSE)
#define HAVE_vec_setv8hi (TARGET_SSE)
#define HAVE_vec_setv8hf (TARGET_SSE)
#define HAVE_vec_setv8bf (TARGET_SSE)
#define HAVE_vec_setv4si (TARGET_SSE)
#define HAVE_vec_setv2di (TARGET_SSE)
#define HAVE_vec_setv4sf (TARGET_SSE)
#define HAVE_vec_setv2df ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_vec_setv32qi (TARGET_AVX)
#define HAVE_vec_setv16hi (TARGET_AVX)
#define HAVE_vec_setv16hf (TARGET_AVX)
#define HAVE_vec_setv16bf (TARGET_AVX)
#define HAVE_vec_setv8si (TARGET_AVX)
#define HAVE_vec_setv4di (TARGET_AVX)
#define HAVE_vec_setv8sf (TARGET_AVX)
#define HAVE_vec_setv4df (TARGET_AVX)
#define HAVE_vec_setv64qi ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_setv32hi ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_setv32hf ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_setv32bf ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_setv16si ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_setv8di ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_setv16sf ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_setv8df ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512dq_vextractf64x2_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_EVEX512))
#define HAVE_avx512dq_vextracti64x2_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_EVEX512))
#define HAVE_avx512f_vextractf32x4_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vextracti32x4_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512dq_vextractf32x8_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_EVEX512))
#define HAVE_avx512dq_vextracti32x8_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_EVEX512))
#define HAVE_avx512f_vextractf64x4_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vextracti64x4_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_vextractf128v8si (TARGET_AVX512VL)
#define HAVE_avx512vl_vextractf128v8sf (TARGET_AVX512VL)
#define HAVE_avx512vl_vextractf128v4di ((TARGET_AVX512VL) && (TARGET_AVX512DQ))
#define HAVE_avx512vl_vextractf128v4df ((TARGET_AVX512VL) && (TARGET_AVX512DQ))
#define HAVE_avx_vextractf128v32qi (TARGET_AVX)
#define HAVE_avx_vextractf128v16hi (TARGET_AVX)
#define HAVE_avx_vextractf128v8si (TARGET_AVX)
#define HAVE_avx_vextractf128v4di (TARGET_AVX)
#define HAVE_avx_vextractf128v8sf (TARGET_AVX)
#define HAVE_avx_vextractf128v4df (TARGET_AVX)
#define HAVE_avx_vextractf128v16hf (TARGET_AVX)
#define HAVE_avx_vextractf128v16bf (TARGET_AVX)
#define HAVE_vec_extractv64qiqi ((TARGET_SSE) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_vec_extractv32qiqi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_extractv16qiqi (TARGET_SSE)
#define HAVE_vec_extractv32hihi ((TARGET_SSE) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_vec_extractv16hihi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_extractv8hihi (TARGET_SSE)
#define HAVE_vec_extractv16sisi ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_extractv8sisi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_extractv4sisi (TARGET_SSE)
#define HAVE_vec_extractv8didi ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_extractv4didi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_extractv2didi (TARGET_SSE)
#define HAVE_vec_extractv32hfhf ((TARGET_SSE) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_vec_extractv16hfhf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_extractv8hfhf (TARGET_SSE)
#define HAVE_vec_extractv32bfbf ((TARGET_SSE) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_vec_extractv16bfbf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_extractv8bfbf (TARGET_SSE)
#define HAVE_vec_extractv16sfsf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_extractv8sfsf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_extractv4sfsf (TARGET_SSE)
#define HAVE_vec_extractv8dfdf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_extractv4dfdf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_extractv2dfdf (TARGET_SSE)
#define HAVE_vec_extractv4titi ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_extractv2titi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_extractv32qiv16qi (TARGET_AVX)
#define HAVE_vec_extractv16hiv8hi (TARGET_AVX)
#define HAVE_vec_extractv16hfv8hf (TARGET_AVX)
#define HAVE_vec_extractv16bfv8bf (TARGET_AVX)
#define HAVE_vec_extractv8siv4si (TARGET_AVX)
#define HAVE_vec_extractv4div2di (TARGET_AVX)
#define HAVE_vec_extractv8sfv4sf (TARGET_AVX)
#define HAVE_vec_extractv4dfv2df (TARGET_AVX)
#define HAVE_vec_extractv64qiv32qi ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_extractv32hiv16hi ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_extractv32hfv16hf ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_extractv32bfv16bf ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_extractv16siv8si ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_extractv8div4di ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_extractv16sfv8sf ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_extractv8dfv4df ((TARGET_AVX) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_interleave_highv4df (TARGET_AVX)
#define HAVE_vec_interleave_highv2df (TARGET_SSE2)
#define HAVE_vec_interleave_lowv4df (TARGET_AVX)
#define HAVE_vec_interleave_lowv2df (TARGET_SSE2)
#define HAVE_avx512f_vternlogv16si_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_vternlogv8si_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vternlogv4si_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_vternlogv8di_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_vternlogv4di_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vternlogv2di_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_vternlogv16si_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_vternlogv8si_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vternlogv4si_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_vternlogv8di_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_vternlogv4di_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vternlogv2di_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_shufps512_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_fixupimmv16sf_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fixupimmv16sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fixupimmv8sf_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv8sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fixupimmv4sf_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv4sf_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_fixupimmv8df_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_fixupimmv8df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_fixupimmv4df_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv4df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_fixupimmv2df_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_fixupimmv2df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_sfixupimmv4sf_maskz (TARGET_AVX512F)
#define HAVE_avx512f_sfixupimmv4sf_maskz_round (TARGET_AVX512F)
#define HAVE_avx512f_sfixupimmv2df_maskz ((TARGET_AVX512F) && (TARGET_SSE2))
#define HAVE_avx512f_sfixupimmv2df_maskz_round ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_SSE2)))
#define HAVE_avx512f_shufpd512_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx_shufpd256 (TARGET_AVX)
#define HAVE_avx_shufpd256_mask ((TARGET_AVX512VL) && (TARGET_AVX))
#define HAVE_sse2_shufpd (TARGET_SSE2)
#define HAVE_sse2_shufpd_mask ((TARGET_AVX512VL) && (TARGET_SSE2))
#define HAVE_sse2_loadhpd_exp (TARGET_SSE2)
#define HAVE_sse2_loadlpd_exp (TARGET_SSE2)
#define HAVE_truncv16siv16qi2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_truncv16siv16hi2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_truncv8div8si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_truncv8div8hi2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_ss_truncatev16siv16qi2_mask_store (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_truncatev16siv16qi2_mask_store (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_us_truncatev16siv16qi2_mask_store (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_ss_truncatev16siv16hi2_mask_store (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_truncatev16siv16hi2_mask_store (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_us_truncatev16siv16hi2_mask_store (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_ss_truncatev8div8si2_mask_store (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_truncatev8div8si2_mask_store (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_us_truncatev8div8si2_mask_store (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_ss_truncatev8div8hi2_mask_store (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_truncatev8div8hi2_mask_store (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_us_truncatev8div8hi2_mask_store (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_truncv32hiv32qi2 (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_ss_truncatev32hiv32qi2_mask_store (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_truncatev32hiv32qi2_mask_store (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_avx512bw_us_truncatev32hiv32qi2_mask_store (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_truncv4div4si2 (TARGET_AVX2)
#define HAVE_truncv8siv8hi2 (TARGET_AVX2)
#define HAVE_truncv16hiv16qi2 (TARGET_AVX2)
#define HAVE_avx512vl_ss_truncatev4div4si2_mask_store (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4div4si2_mask_store (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4div4si2_mask_store (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev8siv8hi2_mask_store (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev8siv8hi2_mask_store (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev8siv8hi2_mask_store (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev16hiv16qi2_mask_store ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_truncatev16hiv16qi2_mask_store ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_us_truncatev16hiv16qi2_mask_store ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_truncv4div4qi2 ((TARGET_SSSE3) && (TARGET_AVX2))
#define HAVE_truncv2div2qi2 (TARGET_SSSE3)
#define HAVE_truncv8siv8qi2 ((TARGET_SSSE3) && (TARGET_AVX2))
#define HAVE_truncv4siv4qi2 (TARGET_SSSE3)
#define HAVE_truncv8hiv8qi2 (TARGET_SSSE3)
#define HAVE_avx512vl_ss_truncatev2div2qi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev2div2qi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev2div2qi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev4siv4qi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4siv4qi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4siv4qi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev4div4qi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4div4qi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4div4qi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev8hiv8qi2_mask_store_2 ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_truncatev8hiv8qi2_mask_store_2 ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_us_truncatev8hiv8qi2_mask_store_2 ((TARGET_AVX512VL) && (TARGET_AVX512BW))
#define HAVE_avx512vl_ss_truncatev8siv8qi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev8siv8qi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev8siv8qi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_truncv4div4hi2 ((TARGET_SSSE3) && (TARGET_AVX2))
#define HAVE_truncv2div2hi2 (TARGET_SSSE3)
#define HAVE_truncv4siv4hi2 (TARGET_SSSE3)
#define HAVE_avx512vl_ss_truncatev4siv4hi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4siv4hi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4siv4hi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev4div4hi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev4div4hi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev4div4hi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_ss_truncatev2div2hi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev2div2hi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev2div2hi2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_truncv2div2si2 (TARGET_SSE)
#define HAVE_avx512vl_ss_truncatev2div2si2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_truncatev2div2si2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_avx512vl_us_truncatev2div2si2_mask_store_2 (TARGET_AVX512VL)
#define HAVE_truncv8div8qi2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_ss_truncatev8div16qi2_mask_store_2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_truncatev8div16qi2_mask_store_2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_us_truncatev8div16qi2_mask_store_2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_negv64qi2 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_negv32qi2 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_negv16qi2 (TARGET_SSE2)
#define HAVE_negv32hi2 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_negv16hi2 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_negv8hi2 (TARGET_SSE2)
#define HAVE_negv16si2 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_negv8si2 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_negv4si2 (TARGET_SSE2)
#define HAVE_negv8di2 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_negv4di2 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_negv2di2 (TARGET_SSE2)
#define HAVE_addv64qi3 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_subv64qi3 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_addv32qi3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_subv32qi3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_addv16qi3 (TARGET_SSE2)
#define HAVE_subv16qi3 (TARGET_SSE2)
#define HAVE_addv32hi3 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_subv32hi3 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_addv16hi3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_subv16hi3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_addv8hi3 (TARGET_SSE2)
#define HAVE_subv8hi3 (TARGET_SSE2)
#define HAVE_addv16si3 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_subv16si3 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_addv8si3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_subv8si3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_addv4si3 (TARGET_SSE2)
#define HAVE_subv4si3 (TARGET_SSE2)
#define HAVE_addv8di3 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_subv8di3 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_addv4di3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_subv4di3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_addv2di3 (TARGET_SSE2)
#define HAVE_subv2di3 (TARGET_SSE2)
#define HAVE_cond_addv64qi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_subv64qi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_addv32qi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_subv32qi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_addv16qi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_subv16qi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_addv32hi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_subv32hi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_addv16hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_subv16hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_addv8hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_subv8hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_addv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_subv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_addv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_subv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_addv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_subv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_addv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_subv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_addv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_subv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_addv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_subv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_addv16si3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_subv16si3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_addv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_subv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_addv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_subv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_addv8di3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_subv8di3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_addv4di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_subv4di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_addv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_subv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_addv64qi3_mask ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_subv64qi3_mask ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_addv16qi3_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_subv16qi3_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_addv32qi3_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_subv32qi3_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_addv32hi3_mask ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_subv32hi3_mask ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_addv16hi3_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_subv16hi3_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_addv8hi3_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_subv8hi3_mask ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_ssaddv64qi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_ssaddv64qi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_usaddv64qi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_usaddv64qi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_sssubv64qi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_sssubv64qi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_ussubv64qi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_ussubv64qi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_ssaddv32qi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX2))
#define HAVE_ssaddv32qi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_usaddv32qi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX2))
#define HAVE_usaddv32qi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_sssubv32qi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX2))
#define HAVE_sssubv32qi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_ussubv32qi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX2))
#define HAVE_ussubv32qi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_ssaddv16qi3 (TARGET_SSE2 && 1 && 1)
#define HAVE_ssaddv16qi3_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_usaddv16qi3 (TARGET_SSE2 && 1 && 1)
#define HAVE_usaddv16qi3_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_sssubv16qi3 (TARGET_SSE2 && 1 && 1)
#define HAVE_sssubv16qi3_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_ussubv16qi3 (TARGET_SSE2 && 1 && 1)
#define HAVE_ussubv16qi3_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_ssaddv32hi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_ssaddv32hi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_usaddv32hi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_usaddv32hi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_sssubv32hi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_sssubv32hi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_ussubv32hi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_ussubv32hi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_ssaddv16hi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX2))
#define HAVE_ssaddv16hi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_usaddv16hi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX2))
#define HAVE_usaddv16hi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_sssubv16hi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX2))
#define HAVE_sssubv16hi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_ussubv16hi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX2))
#define HAVE_ussubv16hi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_ssaddv8hi3 (TARGET_SSE2 && 1 && 1)
#define HAVE_ssaddv8hi3_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_usaddv8hi3 (TARGET_SSE2 && 1 && 1)
#define HAVE_usaddv8hi3_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_sssubv8hi3 (TARGET_SSE2 && 1 && 1)
#define HAVE_sssubv8hi3_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_ussubv8hi3 (TARGET_SSE2 && 1 && 1)
#define HAVE_ussubv8hi3_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_mulv64qi3 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_mulv32qi3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_mulv16qi3 (TARGET_SSE2)
#define HAVE_cond_mulv8hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_cond_mulv16hi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_cond_mulv32hi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_mulv32hi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_mulv32hi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_mulv16hi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX2))
#define HAVE_mulv16hi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_mulv8hi3 (TARGET_SSE2 && 1 && 1)
#define HAVE_mulv8hi3_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_smulv32hi3_highpart ((TARGET_SSE2 \
   && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_smulv32hi3_highpart_mask ((TARGET_AVX512F) && ((TARGET_SSE2 \
   && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_umulv32hi3_highpart ((TARGET_SSE2 \
   && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_umulv32hi3_highpart_mask ((TARGET_AVX512F) && ((TARGET_SSE2 \
   && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_smulv16hi3_highpart ((TARGET_SSE2 \
   && 1 && 1) && (TARGET_AVX2))
#define HAVE_smulv16hi3_highpart_mask ((TARGET_AVX512F) && ((TARGET_SSE2 \
   && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_umulv16hi3_highpart ((TARGET_SSE2 \
   && 1 && 1) && (TARGET_AVX2))
#define HAVE_umulv16hi3_highpart_mask ((TARGET_AVX512F) && ((TARGET_SSE2 \
   && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_smulv8hi3_highpart (TARGET_SSE2 \
   && 1 && 1)
#define HAVE_smulv8hi3_highpart_mask ((TARGET_AVX512F) && (TARGET_SSE2 \
   && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_umulv8hi3_highpart (TARGET_SSE2 \
   && 1 && 1)
#define HAVE_umulv8hi3_highpart_mask ((TARGET_AVX512F) && (TARGET_SSE2 \
   && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_vec_widen_umult_even_v16si (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_vec_widen_umult_even_v16si_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_widen_umult_even_v8si (TARGET_AVX2 && 1)
#define HAVE_vec_widen_umult_even_v8si_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_vec_widen_umult_even_v4si (TARGET_SSE2 && 1)
#define HAVE_vec_widen_umult_even_v4si_mask ((TARGET_AVX512F) && (TARGET_SSE2 && TARGET_AVX512VL))
#define HAVE_vec_widen_smult_even_v16si (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_vec_widen_smult_even_v16si_mask ((TARGET_AVX512F) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_widen_smult_even_v8si (TARGET_AVX2 && 1)
#define HAVE_vec_widen_smult_even_v8si_mask ((TARGET_AVX512F) && (TARGET_AVX2 && TARGET_AVX512VL))
#define HAVE_sse4_1_mulv2siv2di3 (TARGET_SSE4_1 && 1)
#define HAVE_sse4_1_mulv2siv2di3_mask ((TARGET_AVX512F) && (TARGET_SSE4_1 && TARGET_AVX512VL))
#define HAVE_avx2_pmaddwd (TARGET_AVX2)
#define HAVE_sse2_pmaddwd (TARGET_SSE2)
#define HAVE_cond_mulv8di ((TARGET_AVX512DQ) && (TARGET_EVEX512))
#define HAVE_cond_mulv4di ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_cond_mulv2di ((TARGET_AVX512DQ) && (TARGET_AVX512VL))
#define HAVE_avx512dq_mulv8di3 ((TARGET_AVX512DQ && 1) && (TARGET_EVEX512))
#define HAVE_avx512dq_mulv8di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (64 == 64 || TARGET_AVX512VL)) && (TARGET_EVEX512)))
#define HAVE_avx512dq_mulv4di3 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_avx512dq_mulv4di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512VL)))
#define HAVE_avx512dq_mulv2di3 ((TARGET_AVX512DQ && 1) && (TARGET_AVX512VL))
#define HAVE_avx512dq_mulv2di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512DQ && (16 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512VL)))
#define HAVE_cond_mulv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_mulv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_mulv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_mulv16si3 ((TARGET_SSE2 && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_mulv16si3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_mulv8si3 ((TARGET_SSE2 && 1) && (TARGET_AVX2))
#define HAVE_mulv8si3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX2)))
#define HAVE_mulv4si3 (TARGET_SSE2 && 1)
#define HAVE_mulv4si3_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL)))
#define HAVE_mulv8di3 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_mulv4di3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_mulv2di3 (TARGET_SSE2)
#define HAVE_vec_widen_smult_hi_v32qi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_widen_umult_hi_v32qi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_widen_smult_hi_v16qi (TARGET_SSE2)
#define HAVE_vec_widen_umult_hi_v16qi (TARGET_SSE2)
#define HAVE_vec_widen_smult_hi_v16hi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_widen_umult_hi_v16hi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_widen_smult_hi_v8hi (TARGET_SSE2)
#define HAVE_vec_widen_umult_hi_v8hi (TARGET_SSE2)
#define HAVE_vec_widen_smult_hi_v8si ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_widen_umult_hi_v8si ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_widen_smult_hi_v4si (TARGET_SSE2)
#define HAVE_vec_widen_umult_hi_v4si (TARGET_SSE2)
#define HAVE_vec_widen_smult_lo_v32qi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_widen_umult_lo_v32qi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_widen_smult_lo_v16qi (TARGET_SSE2)
#define HAVE_vec_widen_umult_lo_v16qi (TARGET_SSE2)
#define HAVE_vec_widen_smult_lo_v16hi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_widen_umult_lo_v16hi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_widen_smult_lo_v8hi (TARGET_SSE2)
#define HAVE_vec_widen_umult_lo_v8hi (TARGET_SSE2)
#define HAVE_vec_widen_smult_lo_v8si ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_widen_umult_lo_v8si ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_widen_smult_lo_v4si (TARGET_SSE2)
#define HAVE_vec_widen_umult_lo_v4si (TARGET_SSE2)
#define HAVE_vec_widen_smult_even_v4si (TARGET_SSE2)
#define HAVE_vec_widen_smult_odd_v16si ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_widen_umult_odd_v16si ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_widen_smult_odd_v8si ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_widen_umult_odd_v8si ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_widen_smult_odd_v4si (TARGET_SSE2)
#define HAVE_vec_widen_umult_odd_v4si (TARGET_SSE2)
#define HAVE_sdot_prodv16siv32hi ((TARGET_SSE2) && ((TARGET_AVX512BW || TARGET_AVX512VNNI) && TARGET_EVEX512))
#define HAVE_sdot_prodv8siv16hi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_sdot_prodv4siv8hi (TARGET_SSE2)
#define HAVE_sdot_prodv2div4si (TARGET_XOP)
#define HAVE_uavgv64qi3_ceil ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_uavgv32qi3_ceil ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_uavgv16qi3_ceil (TARGET_SSE2)
#define HAVE_uavgv32hi3_ceil ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_uavgv16hi3_ceil ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_uavgv8hi3_ceil (TARGET_SSE2)
#define HAVE_usadv16qi (TARGET_SSE2)
#define HAVE_usadv32qi (TARGET_AVX2)
#define HAVE_usadv64qi (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_ashrv32hi3 ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_ashrv16si3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_ashrv8di3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_ashrv4di3 (TARGET_AVX2)
#define HAVE_vec_shl_v16qi (TARGET_SSE2)
#define HAVE_vec_shl_v8hi (TARGET_SSE2)
#define HAVE_vec_shl_v8hf (TARGET_SSE2)
#define HAVE_vec_shl_v8bf (TARGET_SSE2)
#define HAVE_vec_shl_v4si (TARGET_SSE2)
#define HAVE_vec_shl_v2di (TARGET_SSE2)
#define HAVE_vec_shl_v4sf (TARGET_SSE2)
#define HAVE_vec_shl_v2df (TARGET_SSE2)
#define HAVE_vec_shr_v16qi (TARGET_SSE2)
#define HAVE_vec_shr_v8hi (TARGET_SSE2)
#define HAVE_vec_shr_v8hf (TARGET_SSE2)
#define HAVE_vec_shr_v8bf (TARGET_SSE2)
#define HAVE_vec_shr_v4si (TARGET_SSE2)
#define HAVE_vec_shr_v2di (TARGET_SSE2)
#define HAVE_vec_shr_v4sf (TARGET_SSE2)
#define HAVE_vec_shr_v2df (TARGET_SSE2)
#define HAVE_smaxv32qi3 (TARGET_AVX2)
#define HAVE_sminv32qi3 (TARGET_AVX2)
#define HAVE_umaxv32qi3 (TARGET_AVX2)
#define HAVE_uminv32qi3 (TARGET_AVX2)
#define HAVE_smaxv16hi3 (TARGET_AVX2)
#define HAVE_sminv16hi3 (TARGET_AVX2)
#define HAVE_umaxv16hi3 (TARGET_AVX2)
#define HAVE_uminv16hi3 (TARGET_AVX2)
#define HAVE_smaxv8si3 (TARGET_AVX2)
#define HAVE_sminv8si3 (TARGET_AVX2)
#define HAVE_umaxv8si3 (TARGET_AVX2)
#define HAVE_uminv8si3 (TARGET_AVX2)
#define HAVE_smaxv64qi3 ((TARGET_AVX2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_sminv64qi3 ((TARGET_AVX2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_umaxv64qi3 ((TARGET_AVX2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_uminv64qi3 ((TARGET_AVX2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_smaxv32hi3 ((TARGET_AVX2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_sminv32hi3 ((TARGET_AVX2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_umaxv32hi3 ((TARGET_AVX2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_uminv32hi3 ((TARGET_AVX2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_smaxv16si3 ((TARGET_AVX2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_sminv16si3 ((TARGET_AVX2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_umaxv16si3 ((TARGET_AVX2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_uminv16si3 ((TARGET_AVX2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cond_smaxv64qi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_sminv64qi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_umaxv64qi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_uminv64qi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_smaxv32qi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_sminv32qi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_umaxv32qi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_uminv32qi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_smaxv16qi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_sminv16qi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_umaxv16qi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_uminv16qi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_smaxv32hi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_sminv32hi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_umaxv32hi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_uminv32hi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_smaxv16hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_sminv16hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_umaxv16hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_uminv16hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_smaxv8hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_sminv8hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_umaxv8hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_uminv8hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_smaxv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_sminv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_umaxv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_uminv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_smaxv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_sminv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_umaxv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_uminv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_smaxv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_sminv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_umaxv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_uminv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_smaxv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_sminv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_umaxv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_uminv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_smaxv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_sminv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_umaxv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_uminv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_smaxv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_sminv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_umaxv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_uminv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_smaxv64qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_sminv64qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_umaxv64qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_uminv64qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_smaxv32qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_sminv32qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_umaxv32qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_uminv32qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_smaxv16qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_sminv16qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_umaxv16qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_uminv16qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_smaxv32hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_sminv32hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_umaxv32hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_uminv32hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_smaxv16hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_sminv16hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_umaxv16hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_uminv16hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_smaxv8hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_sminv8hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_umaxv8hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_uminv8hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_smaxv16si3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_sminv16si3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_umaxv16si3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_uminv16si3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_smaxv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_sminv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_umaxv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_uminv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_smaxv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_sminv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_umaxv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_uminv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_smaxv8di3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_sminv8di3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_umaxv8di3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_uminv8di3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_smaxv4di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_sminv4di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_umaxv4di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_uminv4di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_smaxv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_sminv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_umaxv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_uminv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_smaxv8di3 ((TARGET_SSE4_2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_sminv8di3 ((TARGET_SSE4_2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_umaxv8di3 ((TARGET_SSE4_2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_uminv8di3 ((TARGET_SSE4_2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_smaxv4di3 ((TARGET_SSE4_2) && (TARGET_AVX2))
#define HAVE_sminv4di3 ((TARGET_SSE4_2) && (TARGET_AVX2))
#define HAVE_umaxv4di3 ((TARGET_SSE4_2) && (TARGET_AVX2))
#define HAVE_uminv4di3 ((TARGET_SSE4_2) && (TARGET_AVX2))
#define HAVE_smaxv2di3 (TARGET_SSE4_2)
#define HAVE_sminv2di3 (TARGET_SSE4_2)
#define HAVE_umaxv2di3 (TARGET_SSE4_2)
#define HAVE_uminv2di3 (TARGET_SSE4_2)
#define HAVE_smaxv16qi3 (TARGET_SSE2)
#define HAVE_sminv16qi3 (TARGET_SSE2)
#define HAVE_smaxv8hi3 (TARGET_SSE2)
#define HAVE_sminv8hi3 (TARGET_SSE2)
#define HAVE_smaxv4si3 (TARGET_SSE2)
#define HAVE_sminv4si3 (TARGET_SSE2)
#define HAVE_umaxv16qi3 (TARGET_SSE2)
#define HAVE_uminv16qi3 (TARGET_SSE2)
#define HAVE_umaxv8hi3 (TARGET_SSE2)
#define HAVE_uminv8hi3 (TARGET_SSE2)
#define HAVE_umaxv4si3 (TARGET_SSE2)
#define HAVE_uminv4si3 (TARGET_SSE2)
#define HAVE_avx512bw_eqv64qi3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_eqv64qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_eqv16qi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_eqv16qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_eqv32qi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_eqv32qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_eqv32hi3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_eqv32hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_eqv16hi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_eqv16hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_eqv8hi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_eqv8hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512f_eqv16si3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_eqv16si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_eqv8si3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_eqv8si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_eqv4si3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_eqv4si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_eqv8di3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_eqv8di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_eqv4di3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_eqv4di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_eqv2di3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_eqv2di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_gtv16si3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_gtv16si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_gtv8si3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gtv8si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_gtv4si3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gtv4si3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512f_gtv8di3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_gtv8di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_EVEX512)))
#define HAVE_avx512vl_gtv4di3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gtv4di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_gtv2di3 ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gtv2di3_mask ((TARGET_AVX512F) && ((TARGET_AVX512F) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_gtv64qi3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_gtv64qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_gtv16qi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gtv16qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_gtv32qi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gtv32qi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512bw_gtv32hi3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_avx512bw_gtv32hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_EVEX512)))
#define HAVE_avx512vl_gtv16hi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gtv16hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_avx512vl_gtv8hi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gtv8hi3_mask ((TARGET_AVX512F) && ((TARGET_AVX512BW) && (TARGET_AVX512VL)))
#define HAVE_vec_permv16qi (TARGET_SSSE3 || TARGET_AVX || TARGET_XOP)
#define HAVE_vec_permv8hi (TARGET_SSSE3 || TARGET_AVX || TARGET_XOP)
#define HAVE_vec_permv4si (TARGET_SSSE3 || TARGET_AVX || TARGET_XOP)
#define HAVE_vec_permv2di (TARGET_SSSE3 || TARGET_AVX || TARGET_XOP)
#define HAVE_vec_permv4sf (TARGET_SSSE3 || TARGET_AVX || TARGET_XOP)
#define HAVE_vec_permv2df (TARGET_SSSE3 || TARGET_AVX || TARGET_XOP)
#define HAVE_vec_permv8hf ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX512FP16))
#define HAVE_vec_permv32qi ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX2))
#define HAVE_vec_permv16hi ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX2))
#define HAVE_vec_permv8si ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX2))
#define HAVE_vec_permv4di ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX2))
#define HAVE_vec_permv8sf ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX2))
#define HAVE_vec_permv4df ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX2))
#define HAVE_vec_permv16hf ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX512FP16))
#define HAVE_vec_permv16sf ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_permv8df ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_permv16si ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_permv8di ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_permv32hi ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_vec_permv64qi ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX512VBMI && TARGET_EVEX512))
#define HAVE_vec_permv32hf ((TARGET_SSSE3 || TARGET_AVX || TARGET_XOP) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_one_cmplv16si2 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_one_cmplv8di2 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_one_cmplv64qi2 ((TARGET_SSE) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_one_cmplv32qi2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_one_cmplv16qi2 (TARGET_SSE)
#define HAVE_one_cmplv32hi2 ((TARGET_SSE) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_one_cmplv16hi2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_one_cmplv8hi2 (TARGET_SSE)
#define HAVE_one_cmplv8si2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_one_cmplv4si2 (TARGET_SSE)
#define HAVE_one_cmplv4di2 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_one_cmplv2di2 (TARGET_SSE)
#define HAVE_avx512bw_andnotv64qi3 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx2_andnotv32qi3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_sse2_andnotv16qi3 (TARGET_SSE2)
#define HAVE_avx512bw_andnotv32hi3 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx2_andnotv16hi3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_sse2_andnotv8hi3 (TARGET_SSE2)
#define HAVE_avx512f_andnotv16si3 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx2_andnotv8si3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_sse2_andnotv4si3 (TARGET_SSE2)
#define HAVE_avx512f_andnotv8di3 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx2_andnotv4di3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_sse2_andnotv2di3 (TARGET_SSE2)
#define HAVE_andnv16si3 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_andnv8di3 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_andnv64qi3 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_andnv32qi3 ((TARGET_SSE2) && (TARGET_AVX))
#define HAVE_andnv16qi3 (TARGET_SSE2)
#define HAVE_andnv32hi3 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_andnv16hi3 ((TARGET_SSE2) && (TARGET_AVX))
#define HAVE_andnv8hi3 (TARGET_SSE2)
#define HAVE_andnv8si3 ((TARGET_SSE2) && (TARGET_AVX))
#define HAVE_andnv4si3 (TARGET_SSE2)
#define HAVE_andnv4di3 ((TARGET_SSE2) && (TARGET_AVX))
#define HAVE_andnv2di3 (TARGET_SSE2)
#define HAVE_avx512f_andnotv16si3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx2_andnotv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_sse2_andnotv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_andnotv8di3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx2_andnotv4di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_sse2_andnotv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_andv16si3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_iorv16si3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_xorv16si3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_andv8di3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_iorv8di3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_xorv8di3 ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_andv64qi3 ((TARGET_SSE) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_iorv64qi3 ((TARGET_SSE) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_xorv64qi3 ((TARGET_SSE) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_andv32qi3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_iorv32qi3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_xorv32qi3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_andv16qi3 (TARGET_SSE)
#define HAVE_iorv16qi3 (TARGET_SSE)
#define HAVE_xorv16qi3 (TARGET_SSE)
#define HAVE_andv32hi3 ((TARGET_SSE) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_iorv32hi3 ((TARGET_SSE) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_xorv32hi3 ((TARGET_SSE) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_andv16hi3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_iorv16hi3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_xorv16hi3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_andv8hi3 (TARGET_SSE)
#define HAVE_iorv8hi3 (TARGET_SSE)
#define HAVE_xorv8hi3 (TARGET_SSE)
#define HAVE_andv8si3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_iorv8si3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_xorv8si3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_andv4si3 (TARGET_SSE)
#define HAVE_iorv4si3 (TARGET_SSE)
#define HAVE_xorv4si3 (TARGET_SSE)
#define HAVE_andv4di3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_iorv4di3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_xorv4di3 ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_andv2di3 (TARGET_SSE)
#define HAVE_iorv2di3 (TARGET_SSE)
#define HAVE_xorv2di3 (TARGET_SSE)
#define HAVE_cond_andv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_iorv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_xorv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_andv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_iorv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_xorv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_andv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_iorv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_xorv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_andv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_iorv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_xorv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_andv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_iorv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_xorv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_andv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_iorv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_xorv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_andv16si3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_iorv16si3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_xorv16si3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_andv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_iorv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_xorv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_andv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_iorv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_xorv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_andv8di3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_iorv8di3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_xorv8di3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_andv4di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_iorv4di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_xorv4di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_andv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_iorv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_xorv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_one_cmplv1ti2 (TARGET_SSE2)
#define HAVE_vec_pack_trunc_v32hi ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_vec_pack_trunc_v16hi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_pack_trunc_v8hi (TARGET_SSE2)
#define HAVE_vec_pack_trunc_v16si ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_vec_pack_trunc_v8si ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_pack_trunc_v4si (TARGET_SSE2)
#define HAVE_vec_pack_trunc_v8di ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_pack_trunc_v4di ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_pack_trunc_v2di (TARGET_SSE2)
#define HAVE_vec_pack_trunc_qi (TARGET_AVX512F)
#define HAVE_vec_pack_trunc_hi (TARGET_AVX512BW)
#define HAVE_vec_pack_trunc_si (TARGET_AVX512BW)
#define HAVE_vec_pack_sbool_trunc_qi (TARGET_AVX512F)
#define HAVE_vec_interleave_highv32qi (TARGET_AVX2)
#define HAVE_vec_interleave_highv16hi (TARGET_AVX2)
#define HAVE_vec_interleave_highv8si (TARGET_AVX2)
#define HAVE_vec_interleave_highv4di (TARGET_AVX2)
#define HAVE_vec_interleave_lowv32qi (TARGET_AVX2)
#define HAVE_vec_interleave_lowv16hi (TARGET_AVX2)
#define HAVE_vec_interleave_lowv8si (TARGET_AVX2)
#define HAVE_vec_interleave_lowv4di (TARGET_AVX2)
#define HAVE_avx512dq_vinsertf64x2_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_EVEX512))
#define HAVE_avx512dq_vinserti64x2_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_EVEX512))
#define HAVE_avx512f_vinsertf32x4_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vinserti32x4_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512dq_vinsertf32x8_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_EVEX512))
#define HAVE_avx512dq_vinserti32x8_mask ((TARGET_AVX512F) && (TARGET_AVX512DQ && TARGET_EVEX512))
#define HAVE_avx512f_vinsertf64x4_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vinserti64x4_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512dq_shuf_i64x2_mask (TARGET_AVX512DQ)
#define HAVE_avx512dq_shuf_f64x2_mask (TARGET_AVX512DQ)
#define HAVE_avx512f_shuf_f64x2_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_shuf_i64x2_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_shuf_i32x4_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_shuf_f32x4_mask (TARGET_AVX512VL)
#define HAVE_avx512f_shuf_f32x4_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_shuf_i32x4_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_pshufdv3_mask (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512vl_pshufdv3_mask (TARGET_AVX512VL)
#define HAVE_avx2_pshufdv3 (TARGET_AVX2)
#define HAVE_avx512vl_pshufd_mask (TARGET_AVX512VL)
#define HAVE_sse2_pshufd (TARGET_SSE2)
#define HAVE_avx512vl_pshuflwv3_mask (TARGET_AVX512VL && TARGET_AVX512BW)
#define HAVE_avx2_pshuflwv3 (TARGET_AVX2)
#define HAVE_avx512vl_pshuflw_mask (TARGET_AVX512VL && TARGET_AVX512BW)
#define HAVE_sse2_pshuflw (TARGET_SSE2)
#define HAVE_avx2_pshufhwv3 (TARGET_AVX2)
#define HAVE_avx512vl_pshufhwv3_mask (TARGET_AVX512VL && TARGET_AVX512BW)
#define HAVE_avx512vl_pshufhw_mask (TARGET_AVX512VL && TARGET_AVX512BW)
#define HAVE_sse2_pshufhw (TARGET_SSE2)
#define HAVE_sse2_loadd (TARGET_SSE)
#define HAVE_vec_unpacks_lo_v64qi ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_vec_unpacks_lo_v32qi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacks_lo_v16qi (TARGET_SSE2)
#define HAVE_vec_unpacks_lo_v32hi ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_unpacks_lo_v16hi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacks_lo_v8hi (TARGET_SSE2)
#define HAVE_vec_unpacks_lo_v16si ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_unpacks_lo_v8si ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacks_lo_v4si (TARGET_SSE2)
#define HAVE_vec_unpacks_hi_v64qi ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_vec_unpacks_hi_v32qi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacks_hi_v16qi (TARGET_SSE2)
#define HAVE_vec_unpacks_hi_v32hi ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_unpacks_hi_v16hi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacks_hi_v8hi (TARGET_SSE2)
#define HAVE_vec_unpacks_hi_v16si ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_unpacks_hi_v8si ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacks_hi_v4si (TARGET_SSE2)
#define HAVE_vec_unpacku_lo_v64qi ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_vec_unpacku_lo_v32qi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacku_lo_v16qi (TARGET_SSE2)
#define HAVE_vec_unpacku_lo_v32hi ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_unpacku_lo_v16hi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacku_lo_v8hi (TARGET_SSE2)
#define HAVE_vec_unpacku_lo_v16si ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_unpacku_lo_v8si ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacku_lo_v4si (TARGET_SSE2)
#define HAVE_vec_unpacks_sbool_lo_qi (TARGET_AVX512F)
#define HAVE_vec_unpacks_lo_hi (TARGET_AVX512F)
#define HAVE_vec_unpacks_lo_si (TARGET_AVX512F)
#define HAVE_vec_unpacks_lo_di (TARGET_AVX512BW)
#define HAVE_vec_unpacku_hi_v64qi ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_vec_unpacku_hi_v32qi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacku_hi_v16qi (TARGET_SSE2)
#define HAVE_vec_unpacku_hi_v32hi ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_unpacku_hi_v16hi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacku_hi_v8hi (TARGET_SSE2)
#define HAVE_vec_unpacku_hi_v16si ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_unpacku_hi_v8si ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_vec_unpacku_hi_v4si (TARGET_SSE2)
#define HAVE_vec_unpacks_sbool_hi_qi (TARGET_AVX512F)
#define HAVE_vec_unpacks_hi_hi (TARGET_AVX512F)
#define HAVE_vec_unpacks_hi_si (TARGET_AVX512BW)
#define HAVE_vec_unpacks_hi_di (TARGET_AVX512BW)
#define HAVE_avx512bw_uavgv64qi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_uavgv64qi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_avx2_uavgv32qi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX2))
#define HAVE_avx2_uavgv32qi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_sse2_uavgv16qi3 (TARGET_SSE2 && 1 && 1)
#define HAVE_sse2_uavgv16qi3_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_avx512bw_uavgv32hi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512bw_uavgv32hi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (64 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX512BW && TARGET_EVEX512)))
#define HAVE_avx2_uavgv16hi3 ((TARGET_SSE2 && 1 && 1) && (TARGET_AVX2))
#define HAVE_avx2_uavgv16hi3_mask ((TARGET_AVX512F) && ((TARGET_SSE2 && (32 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW) && (TARGET_AVX2)))
#define HAVE_sse2_uavgv8hi3 (TARGET_SSE2 && 1 && 1)
#define HAVE_sse2_uavgv8hi3_mask ((TARGET_AVX512F) && (TARGET_SSE2 && (16 == 64 || TARGET_AVX512VL) && TARGET_AVX512BW))
#define HAVE_avx512f_psadbw ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx2_psadbw ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_sse2_psadbw (TARGET_SSE2)
#define HAVE_sse2_maskmovdqu (TARGET_SSE2)
#define HAVE_ssse3_pmulhrswv8hi3_mask (TARGET_AVX512BW && TARGET_AVX512VL)
#define HAVE_avx2_pmulhrswv16hi3_mask ((TARGET_AVX512BW && TARGET_AVX512VL) && (TARGET_AVX2))
#define HAVE_ssse3_pmulhrswv8hi3 (TARGET_SSSE3)
#define HAVE_avx2_pmulhrswv16hi3 ((TARGET_SSSE3) && (TARGET_AVX2))
#define HAVE_smulhrsv32hi3 ((TARGET_SSSE3) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_smulhrsv16hi3 ((TARGET_SSSE3) && (TARGET_AVX2))
#define HAVE_smulhrsv8hi3 (TARGET_SSSE3)
#define HAVE_ssse3_pmulhrswv4hi3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_smulhrsv2hi3 (TARGET_SSSE3)
#define HAVE_ssse3_pshufbv8qi3 ((TARGET_MMX || TARGET_MMX_WITH_SSE) && TARGET_SSSE3)
#define HAVE_absv64qi2 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_absv32qi2 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_absv16qi2 (TARGET_SSE2)
#define HAVE_absv32hi2 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_absv16hi2 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_absv8hi2 (TARGET_SSE2)
#define HAVE_absv16si2 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_absv8si2 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_absv4si2 (TARGET_SSE2)
#define HAVE_absv8di2 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_absv4di2 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_absv2di2 (TARGET_SSE2)
#define HAVE_avx2_pblendw (TARGET_AVX2)
#define HAVE_avx2_pblendph (TARGET_AVX2)
#define HAVE_avx2_pblendbf (TARGET_AVX2)
#define HAVE_avx2_pblendw_1 (TARGET_AVX2 \
  && !((INTVAL (operands[3]) & 0xff) && (INTVAL (operands[3]) & 0xff00)))
#define HAVE_avx2_pblendph_1 (TARGET_AVX2 \
  && !((INTVAL (operands[3]) & 0xff) && (INTVAL (operands[3]) & 0xff00)))
#define HAVE_avx2_pblendbf_1 (TARGET_AVX2 \
  && !((INTVAL (operands[3]) & 0xff) && (INTVAL (operands[3]) & 0xff00)))
#define HAVE_extendv16qiv16hi2 (TARGET_AVX2)
#define HAVE_zero_extendv16qiv16hi2 (TARGET_AVX2)
#define HAVE_extendv32qiv32hi2 (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_zero_extendv32qiv32hi2 (TARGET_AVX512BW && TARGET_EVEX512)
#define HAVE_extendv8qiv8hi2 (TARGET_SSE4_1 || TARGET_MMX_WITH_SSE)
#define HAVE_zero_extendv8qiv8hi2 (TARGET_SSE4_1 || TARGET_MMX_WITH_SSE)
#define HAVE_extendv16qiv16si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_zero_extendv16qiv16si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_extendv8qiv8si2 (TARGET_AVX2)
#define HAVE_zero_extendv8qiv8si2 (TARGET_AVX2)
#define HAVE_extendv4qiv4si2 (TARGET_SSE4_1)
#define HAVE_zero_extendv4qiv4si2 (TARGET_SSE4_1)
#define HAVE_extendv16hiv16si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_zero_extendv16hiv16si2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_extendv8hiv8si2 (TARGET_AVX2)
#define HAVE_zero_extendv8hiv8si2 (TARGET_AVX2)
#define HAVE_extendv4hiv4si2 (TARGET_SSE4_1 || TARGET_MMX_WITH_SSE)
#define HAVE_zero_extendv4hiv4si2 (TARGET_SSE4_1 || TARGET_MMX_WITH_SSE)
#define HAVE_extendv8qiv8di2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_zero_extendv8qiv8di2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_extendv4qiv4di2 (TARGET_AVX2)
#define HAVE_zero_extendv4qiv4di2 (TARGET_AVX2)
#define HAVE_extendv2qiv2di2 (TARGET_SSE4_1)
#define HAVE_zero_extendv2qiv2di2 (TARGET_SSE4_1)
#define HAVE_extendv8hiv8di2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_zero_extendv8hiv8di2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_extendv4hiv4di2 (TARGET_AVX2)
#define HAVE_zero_extendv4hiv4di2 (TARGET_AVX2)
#define HAVE_extendv2hiv2di2 (TARGET_SSE4_1)
#define HAVE_zero_extendv2hiv2di2 (TARGET_SSE4_1)
#define HAVE_extendv8siv8di2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_zero_extendv8siv8di2 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_extendv4siv4di2 (TARGET_AVX2)
#define HAVE_zero_extendv4siv4di2 (TARGET_AVX2)
#define HAVE_extendv2siv2di2 (TARGET_SSE4_1 || TARGET_MMX_WITH_SSE)
#define HAVE_zero_extendv2siv2di2 (TARGET_SSE4_1 || TARGET_MMX_WITH_SSE)
#define HAVE_sse4_1_ptestzv16qi (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestzv8hi (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestzv4si (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestzv2di (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestzv1ti (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestzv4sf (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestzv2df (TARGET_SSE4_1)
#define HAVE_avx_ptestzv32qi ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestzv16hi ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestzv8si ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestzv4di ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestzv2ti ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestzv8sf ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestzv4df ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_sse4_1_ptestcv16qi (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestcv8hi (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestcv4si (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestcv2di (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestcv1ti (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestcv4sf (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestcv2df (TARGET_SSE4_1)
#define HAVE_avx_ptestcv32qi ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestcv16hi ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestcv8si ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestcv4di ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestcv2ti ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestcv8sf ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestcv4df ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_sse4_1_ptestv16qi (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestv8hi (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestv4si (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestv2di (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestv1ti (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestv4sf (TARGET_SSE4_1)
#define HAVE_sse4_1_ptestv2df (TARGET_SSE4_1)
#define HAVE_avx_ptestv32qi ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestv16hi ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestv8si ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestv4di ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestv2ti ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestv8sf ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_avx_ptestv4df ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_nearbyintv32hf2 ((TARGET_SSE4_1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_nearbyintv16hf2 ((TARGET_SSE4_1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_nearbyintv8hf2 ((TARGET_SSE4_1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_nearbyintv16sf2 ((TARGET_SSE4_1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_nearbyintv8sf2 ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_nearbyintv4sf2 (TARGET_SSE4_1)
#define HAVE_nearbyintv8df2 ((TARGET_SSE4_1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_nearbyintv4df2 ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_nearbyintv2df2 ((TARGET_SSE4_1) && (TARGET_SSE2))
#define HAVE_rintv32hf2 ((TARGET_SSE4_1) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_rintv16hf2 ((TARGET_SSE4_1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_rintv8hf2 ((TARGET_SSE4_1) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_rintv16sf2 ((TARGET_SSE4_1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_rintv8sf2 ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_rintv4sf2 (TARGET_SSE4_1)
#define HAVE_rintv8df2 ((TARGET_SSE4_1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_rintv4df2 ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_rintv2df2 ((TARGET_SSE4_1) && (TARGET_SSE2))
#define HAVE_lrintv16sfv16si2 ((TARGET_SSE2) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_lrintv8sfv8si2 ((TARGET_SSE2) && (TARGET_AVX))
#define HAVE_lrintv4sfv4si2 (TARGET_SSE2)
#define HAVE_lrintv8dfv8di2 ((TARGET_SSE2) && (TARGET_AVX512DQ && TARGET_EVEX512))
#define HAVE_lrintv4dfv4di2 ((TARGET_SSE2) && (TARGET_AVX512DQ && TARGET_AVX512VL))
#define HAVE_lrintv2dfv2di2 ((TARGET_SSE2) && (TARGET_AVX512DQ && TARGET_AVX512VL))
#define HAVE_avx_roundps_sfix256 ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_sse4_1_roundps_sfix (TARGET_SSE4_1)
#define HAVE_avx512f_roundps512 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_roundpd512 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_roundps512_sfix (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx512f_roundpd_vec_pack_sfix512 ((TARGET_SSE4_1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx_roundpd_vec_pack_sfix256 ((TARGET_SSE4_1) && (TARGET_AVX))
#define HAVE_sse4_1_roundpd_vec_pack_sfix (TARGET_SSE4_1)
#define HAVE_floorv32hf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_floorv16hf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_floorv8hf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_floorv16sf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_floorv8sf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX))
#define HAVE_floorv4sf2 (TARGET_SSE4_1 && !flag_trapping_math)
#define HAVE_floorv8df2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_floorv4df2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX))
#define HAVE_floorv2df2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_SSE2))
#define HAVE_lfloorv32hfv32hi2 ((TARGET_AVX512FP16 && !flag_trapping_math) && (TARGET_EVEX512))
#define HAVE_lfloorv16hfv16hi2 ((TARGET_AVX512FP16 && !flag_trapping_math) && (TARGET_AVX512VL))
#define HAVE_lfloorv8hfv8hi2 ((TARGET_AVX512FP16 && !flag_trapping_math) && (TARGET_AVX512VL))
#define HAVE_lfloorv16sfv16si2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_lfloorv8sfv8si2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX))
#define HAVE_lfloorv4sfv4si2 (TARGET_SSE4_1 && !flag_trapping_math)
#define HAVE_lfloorv8dfv8di2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512DQ && TARGET_EVEX512))
#define HAVE_lfloorv4dfv4di2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512DQ && TARGET_AVX512VL))
#define HAVE_lfloorv2dfv2di2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512DQ && TARGET_AVX512VL))
#define HAVE_ceilv32hf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_ceilv16hf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_ceilv8hf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_ceilv16sf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_ceilv8sf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX))
#define HAVE_ceilv4sf2 (TARGET_SSE4_1 && !flag_trapping_math)
#define HAVE_ceilv8df2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_ceilv4df2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX))
#define HAVE_ceilv2df2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_SSE2))
#define HAVE_lceilv32hfv32hi2 ((TARGET_AVX512FP16 && !flag_trapping_math) && (TARGET_EVEX512))
#define HAVE_lceilv16hfv16hi2 ((TARGET_AVX512FP16 && !flag_trapping_math) && (TARGET_AVX512VL))
#define HAVE_lceilv8hfv8hi2 ((TARGET_AVX512FP16 && !flag_trapping_math) && (TARGET_AVX512VL))
#define HAVE_lceilv16sfv16si2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_lceilv8sfv8si2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX))
#define HAVE_lceilv4sfv4si2 (TARGET_SSE4_1 && !flag_trapping_math)
#define HAVE_lceilv8dfv8di2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512DQ && TARGET_EVEX512))
#define HAVE_lceilv4dfv4di2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512DQ && TARGET_AVX512VL))
#define HAVE_lceilv2dfv2di2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512DQ && TARGET_AVX512VL))
#define HAVE_btruncv32hf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_btruncv16hf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_btruncv8hf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_btruncv16sf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_btruncv8sf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX))
#define HAVE_btruncv4sf2 (TARGET_SSE4_1 && !flag_trapping_math)
#define HAVE_btruncv8df2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_btruncv4df2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX))
#define HAVE_btruncv2df2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_SSE2))
#define HAVE_roundv32hf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512FP16 && TARGET_EVEX512))
#define HAVE_roundv16hf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_roundv8hf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512FP16 && TARGET_AVX512VL))
#define HAVE_roundv16sf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_roundv8sf2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX))
#define HAVE_roundv4sf2 (TARGET_SSE4_1 && !flag_trapping_math)
#define HAVE_roundv8df2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_roundv4df2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX))
#define HAVE_roundv2df2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_SSE2))
#define HAVE_lroundv32hfv32hi2 ((TARGET_AVX512FP16 && !flag_trapping_math) && (TARGET_EVEX512))
#define HAVE_lroundv16hfv16hi2 ((TARGET_AVX512FP16 && !flag_trapping_math) && (TARGET_AVX512VL))
#define HAVE_lroundv8hfv8hi2 ((TARGET_AVX512FP16 && !flag_trapping_math) && (TARGET_AVX512VL))
#define HAVE_lroundv16sfv16si2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_lroundv8sfv8si2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX))
#define HAVE_lroundv4sfv4si2 (TARGET_SSE4_1 && !flag_trapping_math)
#define HAVE_lroundv8dfv8di2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512DQ && TARGET_EVEX512))
#define HAVE_lroundv4dfv4di2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512DQ && TARGET_AVX512VL))
#define HAVE_lroundv2dfv2di2 ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512DQ && TARGET_AVX512VL))
#define HAVE_roundv16sf2_sfix ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_roundv8sf2_sfix ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX))
#define HAVE_roundv4sf2_sfix (TARGET_SSE4_1 && !flag_trapping_math)
#define HAVE_roundv8df2_vec_pack_sfix ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_roundv4df2_vec_pack_sfix ((TARGET_SSE4_1 && !flag_trapping_math) && (TARGET_AVX))
#define HAVE_roundv2df2_vec_pack_sfix (TARGET_SSE4_1 && !flag_trapping_math)
#define HAVE_rotlv16qi3 (TARGET_XOP)
#define HAVE_rotlv8hi3 (TARGET_XOP)
#define HAVE_rotlv4si3 (TARGET_XOP)
#define HAVE_rotlv2di3 (TARGET_XOP)
#define HAVE_rotrv16qi3 (TARGET_XOP)
#define HAVE_rotrv8hi3 (TARGET_XOP)
#define HAVE_rotrv4si3 (TARGET_XOP)
#define HAVE_rotrv2di3 (TARGET_XOP)
#define HAVE_vrotrv16qi3 (TARGET_XOP)
#define HAVE_vrotrv8hi3 (TARGET_XOP)
#define HAVE_vrotrv4si3 (TARGET_XOP)
#define HAVE_vrotrv2di3 (TARGET_XOP)
#define HAVE_vrotlv16qi3 (TARGET_XOP)
#define HAVE_vrotlv8hi3 (TARGET_XOP)
#define HAVE_vrotlv4si3 (TARGET_XOP)
#define HAVE_vrotlv2di3 (TARGET_XOP)
#define HAVE_vlshrv16qi3 (TARGET_XOP || (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_vlshrv8hi3 (TARGET_XOP || (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_vlshrv4si3 (TARGET_AVX2 || TARGET_XOP)
#define HAVE_vlshrv2di3 (TARGET_AVX2 || TARGET_XOP)
#define HAVE_vashlv64qi3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_vlshrv64qi3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_vashrv64qi3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_vashlv32qi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vlshrv32qi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vashrv32qi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vashlv32hi3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_vlshrv32hi3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_vashrv32hi3 ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_vashlv16hi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vlshrv16hi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vashrv16hi3 ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_vlshrv16si3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vlshrv8di3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vlshrv8si3 (TARGET_AVX2)
#define HAVE_vlshrv4di3 (TARGET_AVX2)
#define HAVE_vashrv8di3 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_vashrv4di3 (TARGET_AVX2)
#define HAVE_vashrv16qi3 (TARGET_XOP || (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_vashrv8hi3 (TARGET_XOP || (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_vashrv2di3 (TARGET_XOP || TARGET_AVX2)
#define HAVE_vashrv4si3 (TARGET_AVX2 || TARGET_XOP)
#define HAVE_vashrv16si3 (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_vashrv8si3 (TARGET_AVX2)
#define HAVE_vashlv16qi3 (TARGET_XOP || (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_vashlv8hi3 (TARGET_XOP || (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_vashlv4si3 (TARGET_AVX2 || TARGET_XOP)
#define HAVE_vashlv2di3 (TARGET_AVX2 || TARGET_XOP)
#define HAVE_vashlv16si3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vashlv8di3 ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_vashlv8si3 (TARGET_AVX2)
#define HAVE_vashlv4di3 (TARGET_AVX2)
#define HAVE_ashlv64qi3 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_lshrv64qi3 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_ashrv64qi3 ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_ashlv32qi3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_lshrv32qi3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_ashrv32qi3 ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_ashlv16qi3 (TARGET_SSE2)
#define HAVE_lshrv16qi3 (TARGET_SSE2)
#define HAVE_ashrv16qi3 (TARGET_SSE2)
#define HAVE_ashrv2di3 (TARGET_SSE2)
#define HAVE_xop_vmfrczv4sf2 (TARGET_XOP)
#define HAVE_xop_vmfrczv2df2 ((TARGET_XOP) && (TARGET_SSE2))
#define HAVE_avx_vzeroall (TARGET_AVX)
#define HAVE_avx_vzeroupper (TARGET_AVX)
#define HAVE_avx512f_vpermilv8df ((TARGET_AVX && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_vpermilv8df_mask ((TARGET_AVX512F) && ((TARGET_AVX && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx_vpermilv4df ((TARGET_AVX && 1) && (TARGET_AVX))
#define HAVE_avx_vpermilv4df_mask ((TARGET_AVX512F) && ((TARGET_AVX && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX)))
#define HAVE_avx_vpermilv2df (TARGET_AVX && 1)
#define HAVE_avx_vpermilv2df_mask ((TARGET_AVX512F) && (TARGET_AVX && (16 == 64 || TARGET_AVX512VL)))
#define HAVE_avx512f_vpermilv16sf ((TARGET_AVX && 1) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_avx512f_vpermilv16sf_mask ((TARGET_AVX512F) && ((TARGET_AVX && (64 == 64 || TARGET_AVX512VL)) && (TARGET_AVX512F && TARGET_EVEX512)))
#define HAVE_avx_vpermilv8sf ((TARGET_AVX && 1) && (TARGET_AVX))
#define HAVE_avx_vpermilv8sf_mask ((TARGET_AVX512F) && ((TARGET_AVX && (32 == 64 || TARGET_AVX512VL)) && (TARGET_AVX)))
#define HAVE_avx_vpermilv4sf (TARGET_AVX && 1)
#define HAVE_avx_vpermilv4sf_mask ((TARGET_AVX512F) && (TARGET_AVX && (16 == 64 || TARGET_AVX512VL)))
#define HAVE_avx2_permv4di (TARGET_AVX2)
#define HAVE_avx2_permv4df (TARGET_AVX2)
#define HAVE_avx512vl_permv4di_mask (TARGET_AVX512VL)
#define HAVE_avx512vl_permv4df_mask (TARGET_AVX512VL)
#define HAVE_avx512f_permv8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_permv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_permv8df_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_permv8di_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermi2varv16si3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermi2varv16sf3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermi2varv8di3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermi2varv8df3_mask ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_vpermi2varv8si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermi2varv8sf3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermi2varv4di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermi2varv4df3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermi2varv4si3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermi2varv4sf3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermi2varv2di3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermi2varv2df3_mask ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512bw_vpermi2varv32hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512vl_vpermi2varv16hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_avx512vl_vpermi2varv8hi3_mask ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_avx512bw_vpermi2varv64qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_EVEX512))
#define HAVE_avx512vl_vpermi2varv32qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_AVX512VL))
#define HAVE_avx512vl_vpermi2varv16qi3_mask ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_AVX512VL))
#define HAVE_avx512f_vpermt2varv16si3_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermt2varv16sf3_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermt2varv8di3_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_vpermt2varv8df3_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_vpermt2varv8si3_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv8sf3_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4di3_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4df3_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4si3_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv4sf3_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv2di3_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv2df3_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512bw_vpermt2varv32hi3_maskz ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_avx512vl_vpermt2varv16hi3_maskz ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv8hi3_maskz ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_AVX512VL))
#define HAVE_avx512bw_vpermt2varv64qi3_maskz ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_EVEX512))
#define HAVE_avx512vl_vpermt2varv32qi3_maskz ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_AVX512VL))
#define HAVE_avx512vl_vpermt2varv16qi3_maskz ((TARGET_AVX512F) && (TARGET_AVX512VBMI && TARGET_AVX512VL))
#define HAVE_avx_vperm2f128v8si3 (TARGET_AVX)
#define HAVE_avx_vperm2f128v8sf3 (TARGET_AVX)
#define HAVE_avx_vperm2f128v4df3 (TARGET_AVX)
#define HAVE_avx512vl_vinsertv8si (TARGET_AVX512VL)
#define HAVE_avx512vl_vinsertv8sf (TARGET_AVX512VL)
#define HAVE_avx512vl_vinsertv4di (TARGET_AVX512VL)
#define HAVE_avx512vl_vinsertv4df (TARGET_AVX512VL)
#define HAVE_avx_vinsertf128v32qi (TARGET_AVX)
#define HAVE_avx_vinsertf128v16hi (TARGET_AVX)
#define HAVE_avx_vinsertf128v8si (TARGET_AVX)
#define HAVE_avx_vinsertf128v4di (TARGET_AVX)
#define HAVE_avx_vinsertf128v8sf (TARGET_AVX)
#define HAVE_avx_vinsertf128v4df (TARGET_AVX)
#define HAVE_avx_vinsertf128v16hf (TARGET_AVX)
#define HAVE_avx_vinsertf128v16bf (TARGET_AVX)
#define HAVE_maskloadv4sfv4si_1 (TARGET_AVX)
#define HAVE_maskloadv2dfv2di_1 (TARGET_AVX)
#define HAVE_maskloadv4div4di_1 (TARGET_AVX)
#define HAVE_maskloadv2div2di_1 (TARGET_AVX)
#define HAVE_maskloadv8sfv8si_1 (TARGET_AVX)
#define HAVE_maskloadv4dfv4di_1 (TARGET_AVX)
#define HAVE_maskloadv8siv8si_1 (TARGET_AVX)
#define HAVE_maskloadv4siv4si_1 (TARGET_AVX)
#define HAVE_maskloadv4sfv4si (TARGET_AVX)
#define HAVE_maskloadv2dfv2di (TARGET_AVX)
#define HAVE_maskloadv4div4di (TARGET_AVX)
#define HAVE_maskloadv2div2di (TARGET_AVX)
#define HAVE_maskloadv8sfv8si (TARGET_AVX)
#define HAVE_maskloadv4dfv4di (TARGET_AVX)
#define HAVE_maskloadv8siv8si (TARGET_AVX)
#define HAVE_maskloadv4siv4si (TARGET_AVX)
#define HAVE_maskloadv16sihi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_maskloadv8siqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskloadv4siqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskloadv8diqi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_maskloadv4diqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskloadv2diqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskloadv16sfhi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_maskloadv8sfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskloadv4sfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskloadv8dfqi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_maskloadv4dfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskloadv2dfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskloadv64qidi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_maskloadv16qihi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskloadv32qisi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskloadv32hisi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_maskloadv16hihi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskloadv8hiqi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskloadv32hfsi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_maskloadv16hfhi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskloadv8hfqi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskloadv32bfsi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_maskloadv16bfhi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskloadv8bfqi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskstorev4sfv4si (TARGET_AVX)
#define HAVE_maskstorev2dfv2di (TARGET_AVX)
#define HAVE_maskstorev4div4di (TARGET_AVX)
#define HAVE_maskstorev2div2di (TARGET_AVX)
#define HAVE_maskstorev8sfv8si (TARGET_AVX)
#define HAVE_maskstorev4dfv4di (TARGET_AVX)
#define HAVE_maskstorev8siv8si (TARGET_AVX)
#define HAVE_maskstorev4siv4si (TARGET_AVX)
#define HAVE_maskstorev16sihi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_maskstorev8siqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskstorev4siqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskstorev8diqi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_maskstorev4diqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskstorev2diqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskstorev16sfhi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_maskstorev8sfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskstorev4sfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskstorev8dfqi ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_maskstorev4dfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskstorev2dfqi ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_maskstorev64qidi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_maskstorev16qihi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskstorev32qisi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskstorev32hisi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_maskstorev16hihi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskstorev8hiqi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskstorev32hfsi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_maskstorev16hfhi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskstorev8hfqi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskstorev32bfsi ((TARGET_AVX512BW) && (TARGET_EVEX512))
#define HAVE_maskstorev16bfhi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_maskstorev8bfqi ((TARGET_AVX512BW) && (TARGET_AVX512VL))
#define HAVE_cbranchv64qi4 ((TARGET_SSE4_1 && (64 != 64 || !TARGET_PREFER_AVX256)) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cbranchv32qi4 ((TARGET_SSE4_1 && (32 != 64 || !TARGET_PREFER_AVX256)) && (TARGET_AVX))
#define HAVE_cbranchv16qi4 (TARGET_SSE4_1 && (16 != 64 || !TARGET_PREFER_AVX256))
#define HAVE_cbranchv32hi4 ((TARGET_SSE4_1 && (64 != 64 || !TARGET_PREFER_AVX256)) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cbranchv16hi4 ((TARGET_SSE4_1 && (32 != 64 || !TARGET_PREFER_AVX256)) && (TARGET_AVX))
#define HAVE_cbranchv8hi4 (TARGET_SSE4_1 && (16 != 64 || !TARGET_PREFER_AVX256))
#define HAVE_cbranchv16si4 ((TARGET_SSE4_1 && (64 != 64 || !TARGET_PREFER_AVX256)) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cbranchv8si4 ((TARGET_SSE4_1 && (32 != 64 || !TARGET_PREFER_AVX256)) && (TARGET_AVX))
#define HAVE_cbranchv4si4 (TARGET_SSE4_1 && (16 != 64 || !TARGET_PREFER_AVX256))
#define HAVE_cbranchv8di4 ((TARGET_SSE4_1 && (64 != 64 || !TARGET_PREFER_AVX256)) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cbranchv4di4 ((TARGET_SSE4_1 && (32 != 64 || !TARGET_PREFER_AVX256)) && (TARGET_AVX))
#define HAVE_cbranchv2di4 (TARGET_SSE4_1 && (16 != 64 || !TARGET_PREFER_AVX256))
#define HAVE_vec_initv64qiqi ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv32qiqi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv16qiqi (TARGET_SSE)
#define HAVE_vec_initv32hihi ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv16hihi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv8hihi (TARGET_SSE)
#define HAVE_vec_initv16sisi ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv8sisi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv4sisi (TARGET_SSE)
#define HAVE_vec_initv8didi ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv4didi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv2didi (TARGET_SSE)
#define HAVE_vec_initv32hfhf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv16hfhf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv8hfhf (TARGET_SSE)
#define HAVE_vec_initv32bfbf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv16bfbf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv8bfbf (TARGET_SSE)
#define HAVE_vec_initv16sfsf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv8sfsf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv4sfsf (TARGET_SSE)
#define HAVE_vec_initv8dfdf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv4dfdf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv2dfdf ((TARGET_SSE) && (TARGET_SSE2))
#define HAVE_vec_initv4titi ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv2titi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv64qiv32qi ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv32qiv16qi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv16qiv8qi (TARGET_SSE)
#define HAVE_vec_initv32hiv16hi ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv16hiv8hi ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv8hiv4hi (TARGET_SSE)
#define HAVE_vec_initv16siv8si ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv8siv4si ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv4siv2si (TARGET_SSE)
#define HAVE_vec_initv8div4di ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv4div2di ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv32hfv16hf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv16hfv8hf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv8hfv4hf (TARGET_SSE)
#define HAVE_vec_initv32bfv16bf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv16bfv8bf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv8bfv4bf (TARGET_SSE)
#define HAVE_vec_initv16sfv8sf ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv8sfv4sf ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv4sfv2sf (TARGET_SSE)
#define HAVE_vec_initv8dfv4df ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_initv4dfv2df ((TARGET_SSE) && (TARGET_AVX))
#define HAVE_vec_initv4tiv2ti ((TARGET_SSE) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_cond_ashlv32hi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_lshrv32hi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_ashrv32hi ((TARGET_AVX512F) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_cond_ashlv16hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_lshrv16hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_ashrv16hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_ashlv8hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_lshrv8hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_ashrv8hi ((TARGET_AVX512F) && (TARGET_AVX512VL && TARGET_AVX512BW))
#define HAVE_cond_ashlv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_lshrv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_ashrv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_ashlv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_lshrv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_ashrv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_ashlv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_lshrv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_ashrv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_ashlv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_lshrv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_ashrv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_cond_ashlv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_lshrv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_ashrv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_ashlv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_lshrv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_cond_ashrv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_vcvtps2ph_mask (TARGET_AVX512VL)
#define HAVE_vcvtps2ph (TARGET_F16C)
#define HAVE_avx512f_vcvtps2ph512_mask_sae (TARGET_AVX512F && TARGET_EVEX512)
#define HAVE_avx2_gathersiv2di (TARGET_AVX2)
#define HAVE_avx2_gathersiv2df (TARGET_AVX2)
#define HAVE_avx2_gathersiv4di (TARGET_AVX2)
#define HAVE_avx2_gathersiv4df (TARGET_AVX2)
#define HAVE_avx2_gathersiv4si (TARGET_AVX2)
#define HAVE_avx2_gathersiv4sf (TARGET_AVX2)
#define HAVE_avx2_gathersiv8si (TARGET_AVX2)
#define HAVE_avx2_gathersiv8sf (TARGET_AVX2)
#define HAVE_avx2_gatherdiv2di (TARGET_AVX2)
#define HAVE_avx2_gatherdiv2df (TARGET_AVX2)
#define HAVE_avx2_gatherdiv4di (TARGET_AVX2)
#define HAVE_avx2_gatherdiv4df (TARGET_AVX2)
#define HAVE_avx2_gatherdiv4si (TARGET_AVX2)
#define HAVE_avx2_gatherdiv4sf (TARGET_AVX2)
#define HAVE_avx2_gatherdiv8si (TARGET_AVX2)
#define HAVE_avx2_gatherdiv8sf (TARGET_AVX2)
#define HAVE_avx512f_gathersiv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_gathersiv16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_gathersiv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_gathersiv8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_gathersiv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gathersiv8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gathersiv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gathersiv4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gathersiv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gathersiv4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gathersiv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gathersiv2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_gatherdiv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_gatherdiv16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_gatherdiv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_gatherdiv8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_gatherdiv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gatherdiv8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gatherdiv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gatherdiv4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gatherdiv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gatherdiv4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gatherdiv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_gatherdiv2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_scattersiv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_scattersiv16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_scattersiv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_scattersiv8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_scattersiv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scattersiv8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scattersiv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scattersiv4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scattersiv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scattersiv4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scattersiv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scattersiv2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_scatterdiv16si ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_scatterdiv16sf ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_scatterdiv8di ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_scatterdiv8df ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_scatterdiv8si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scatterdiv8sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scatterdiv4di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scatterdiv4df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scatterdiv4si ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scatterdiv4sf ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scatterdiv2di ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_scatterdiv2df ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512f_expandv16si_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_expandv16sf_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_expandv8di_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512f_expandv8df_maskz ((TARGET_AVX512F) && (TARGET_EVEX512))
#define HAVE_avx512vl_expandv8si_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_expandv8sf_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_expandv4di_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_expandv4df_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_expandv4si_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_expandv4sf_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_expandv2di_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_avx512vl_expandv2df_maskz ((TARGET_AVX512F) && (TARGET_AVX512VL))
#define HAVE_expandv64qi_maskz ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_expandv16qi_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_expandv32qi_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_expandv32hi_maskz ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_expandv16hi_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_expandv8hi_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpmadd52huqv8di_maskz ((TARGET_AVX512IFMA) && (TARGET_EVEX512))
#define HAVE_vpmadd52huqv4di_maskz ((TARGET_AVX512IFMA) && (TARGET_AVX512VL))
#define HAVE_vpmadd52huqv2di_maskz ((TARGET_AVX512IFMA) && (TARGET_AVX512VL))
#define HAVE_vpmadd52luqv8di_maskz ((TARGET_AVX512IFMA) && (TARGET_EVEX512))
#define HAVE_vpmadd52luqv4di_maskz ((TARGET_AVX512IFMA) && (TARGET_AVX512VL))
#define HAVE_vpmadd52luqv2di_maskz ((TARGET_AVX512IFMA) && (TARGET_AVX512VL))
#define HAVE_popcountv16si2 ((TARGET_AVX512VPOPCNTDQ) && (TARGET_EVEX512))
#define HAVE_popcountv8si2 ((TARGET_AVX512VPOPCNTDQ) && (TARGET_AVX512VL))
#define HAVE_popcountv4si2 ((TARGET_AVX512VPOPCNTDQ) && (TARGET_AVX512VL))
#define HAVE_popcountv8di2 ((TARGET_AVX512VPOPCNTDQ) && (TARGET_EVEX512))
#define HAVE_popcountv4di2 ((TARGET_AVX512VPOPCNTDQ) && (TARGET_AVX512VL))
#define HAVE_popcountv2di2 ((TARGET_AVX512VPOPCNTDQ) && (TARGET_AVX512VL))
#define HAVE_popcountv64qi2 ((TARGET_AVX512BITALG) && (TARGET_EVEX512))
#define HAVE_popcountv16qi2 ((TARGET_AVX512BITALG) && (TARGET_AVX512VL))
#define HAVE_popcountv32qi2 ((TARGET_AVX512BITALG) && (TARGET_AVX512VL))
#define HAVE_popcountv32hi2 ((TARGET_AVX512BITALG) && (TARGET_EVEX512))
#define HAVE_popcountv16hi2 ((TARGET_AVX512BITALG) && (TARGET_AVX512VL))
#define HAVE_popcountv8hi2 ((TARGET_AVX512BITALG) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v32hi_maskz ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrdv_v16si_maskz ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrdv_v8di_maskz ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshrdv_v16hi_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v8si_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v4di_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v8hi_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v4si_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshrdv_v2di_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v32hi_maskz ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshldv_v16si_maskz ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshldv_v8di_maskz ((TARGET_AVX512VBMI2) && (TARGET_EVEX512))
#define HAVE_vpshldv_v16hi_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v8si_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v4di_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v8hi_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v4si_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_vpshldv_v2di_maskz ((TARGET_AVX512VBMI2) && (TARGET_AVX512VL))
#define HAVE_usdot_prodv16siv64qi ((TARGET_SSE2) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_usdot_prodv8siv32qi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_usdot_prodv4siv16qi (TARGET_SSE2)
#define HAVE_vpdpbusd_v16si_maskz ((TARGET_AVX512VNNI) && (TARGET_EVEX512))
#define HAVE_vpdpbusd_v8si_maskz ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpbusd_v4si_maskz ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpbusds_v16si_maskz ((TARGET_AVX512VNNI) && (TARGET_EVEX512))
#define HAVE_vpdpbusds_v8si_maskz ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpbusds_v4si_maskz ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpwssd_v16si_maskz ((TARGET_AVX512VNNI) && (TARGET_EVEX512))
#define HAVE_vpdpwssd_v8si_maskz ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpwssd_v4si_maskz ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpwssds_v16si_maskz ((TARGET_AVX512VNNI) && (TARGET_EVEX512))
#define HAVE_vpdpwssds_v8si_maskz ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_vpdpwssds_v4si_maskz ((TARGET_AVX512VNNI) && (TARGET_AVX512VL))
#define HAVE_movp2qi (TARGET_AVX512VP2INTERSECT)
#define HAVE_movp2hi (TARGET_AVX512VP2INTERSECT)
#define HAVE_avx512f_cvtne2ps2bf16_v32bf_maskz ((TARGET_AVX512BF16) && (TARGET_EVEX512))
#define HAVE_avx512f_cvtne2ps2bf16_v16bf_maskz ((TARGET_AVX512BF16) && (TARGET_AVX512VL))
#define HAVE_avx512f_cvtne2ps2bf16_v8bf_maskz ((TARGET_AVX512BF16) && (TARGET_AVX512VL))
#define HAVE_truncv4sfv4bf2 (TARGET_SSSE3 && !HONOR_NANS (BFmode) && !flag_rounding_math \
   && (flag_unsafe_math_optimizations \
       || TARGET_AVXNECONVERT \
       || (TARGET_AVX512BF16 && TARGET_AVX512VL)))
#define HAVE_vcvtneps2bf16_v4sf (TARGET_AVXNECONVERT || (TARGET_AVX512BF16 && TARGET_AVX512VL))
#define HAVE_avx512f_cvtneps2bf16_v4sf_maskz (TARGET_AVX512BF16 && TARGET_AVX512VL)
#define HAVE_avx512f_cvtneps2bf16_v4sf_mask (TARGET_AVX512BF16 && TARGET_AVX512VL)
#define HAVE_avx512f_cvtneps2bf16_v16sf_maskz ((TARGET_AVX512BF16) && (TARGET_EVEX512))
#define HAVE_avx512f_cvtneps2bf16_v8sf_maskz ((TARGET_AVX512BF16) && (TARGET_AVX512VL))
#define HAVE_truncv8sfv8bf2 (TARGET_AVX2 && !HONOR_NANS (BFmode) && !flag_rounding_math \
   && (flag_unsafe_math_optimizations \
       || TARGET_AVXNECONVERT \
       || (TARGET_AVX512BF16 && TARGET_AVX512VL)))
#define HAVE_truncv16sfv16bf2 (TARGET_AVX512BW && TARGET_EVEX512 \
   && !HONOR_NANS (BFmode) && !flag_rounding_math \
   && (flag_unsafe_math_optimizations || TARGET_AVX512BF16))
#define HAVE_extendv16bfv16sf2 ((TARGET_SSE2 && !HONOR_NANS (BFmode)) && (TARGET_AVX512BW && TARGET_EVEX512))
#define HAVE_extendv8bfv8sf2 ((TARGET_SSE2 && !HONOR_NANS (BFmode)) && (TARGET_AVX2))
#define HAVE_extendv4bfv4sf2 (TARGET_SSE2 && !HONOR_NANS (BFmode))
#define HAVE_avx512f_dpbf16ps_v16sf_maskz ((TARGET_AVX512BF16) && (TARGET_EVEX512))
#define HAVE_avx512f_dpbf16ps_v8sf_maskz ((TARGET_AVX512BF16) && (TARGET_AVX512VL))
#define HAVE_avx512f_dpbf16ps_v4sf_maskz ((TARGET_AVX512BF16) && (TARGET_AVX512VL))
#define HAVE_encodekey128u32 (TARGET_KL)
#define HAVE_encodekey256u32 (TARGET_KL)
#define HAVE_aesdecwide128klu8 (TARGET_WIDEKL)
#define HAVE_aesdecwide256klu8 (TARGET_WIDEKL)
#define HAVE_aesencwide128klu8 (TARGET_WIDEKL)
#define HAVE_aesencwide256klu8 (TARGET_WIDEKL)
#define HAVE_vec_duplicatev64qi ((TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_duplicatev32qi ((TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC) && (TARGET_AVX))
#define HAVE_vec_duplicatev16qi (TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC)
#define HAVE_vec_duplicatev32hi ((TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_duplicatev16hi ((TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC) && (TARGET_AVX))
#define HAVE_vec_duplicatev8hi (TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC)
#define HAVE_vec_duplicatev16si ((TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC) && (TARGET_AVX512F && TARGET_EVEX512))
#define HAVE_vec_duplicatev8si ((TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC) && (TARGET_AVX))
#define HAVE_vec_duplicatev4si (TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC)
#define HAVE_sdot_prodv16siv64qi ((TARGET_SSE2) && ((TARGET_AVX512BW || TARGET_AVX512VNNI) && TARGET_EVEX512))
#define HAVE_sdot_prodv8siv32qi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_sdot_prodv4siv16qi (TARGET_SSE2)
#define HAVE_udot_prodv16siv64qi ((TARGET_SSE2) && ((TARGET_AVX512BW || TARGET_AVX512VNNI) && TARGET_EVEX512))
#define HAVE_udot_prodv8siv32qi ((TARGET_SSE2) && (TARGET_AVX2))
#define HAVE_udot_prodv4siv16qi (TARGET_SSE2)
#define HAVE_vpdpbssd_v16si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbssds_v16si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbsud_v16si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbsuds_v16si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbuud_v16si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbuuds_v16si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbssd_v8si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbssds_v8si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbsud_v8si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbsuds_v8si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbuud_v8si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbuuds_v8si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbssd_v4si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbssds_v4si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbsud_v4si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbsuds_v4si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbuud_v4si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpbuuds_v4si_maskz (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2bf8v8hf (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2bf8sv8hf (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2hf8v8hf (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2hf8sv8hf (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2bf8v8hf_mask (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2bf8sv8hf_mask (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2hf8v8hf_mask (TARGET_AVX10_2)
#define HAVE_vcvtbiasph2hf8sv8hf_mask (TARGET_AVX10_2)
#define HAVE_vcvtph2bf8v8hf (TARGET_AVX10_2)
#define HAVE_vcvtph2bf8sv8hf (TARGET_AVX10_2)
#define HAVE_vcvtph2hf8v8hf (TARGET_AVX10_2)
#define HAVE_vcvtph2hf8sv8hf (TARGET_AVX10_2)
#define HAVE_vcvtph2bf8v8hf_mask (TARGET_AVX10_2)
#define HAVE_vcvtph2bf8sv8hf_mask (TARGET_AVX10_2)
#define HAVE_vcvtph2hf8v8hf_mask (TARGET_AVX10_2)
#define HAVE_vcvtph2hf8sv8hf_mask (TARGET_AVX10_2)
#define HAVE_usdot_prodv16siv32hi ((TARGET_AVXVNNIINT16 || TARGET_AVX10_2) && (TARGET_AVX10_2))
#define HAVE_usdot_prodv8siv16hi (TARGET_AVXVNNIINT16 || TARGET_AVX10_2)
#define HAVE_usdot_prodv4siv8hi (TARGET_AVXVNNIINT16 || TARGET_AVX10_2)
#define HAVE_udot_prodv16siv32hi ((TARGET_AVXVNNIINT16 || TARGET_AVX10_2) && (TARGET_AVX10_2))
#define HAVE_udot_prodv8siv16hi (TARGET_AVXVNNIINT16 || TARGET_AVX10_2)
#define HAVE_udot_prodv4siv8hi (TARGET_AVXVNNIINT16 || TARGET_AVX10_2)
#define HAVE_vpdpwusd_v16si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwusds_v16si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwsud_v16si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwsuds_v16si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwuud_v16si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwuuds_v16si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwusd_v8si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwusds_v8si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwsud_v8si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwsuds_v8si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwuud_v8si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwuuds_v8si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwusd_v4si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwusds_v4si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwsud_v4si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwsuds_v4si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwuud_v4si_maskz (TARGET_AVX10_2)
#define HAVE_vpdpwuuds_v4si_maskz (TARGET_AVX10_2)
#define HAVE_vdpphps_v16sf_maskz (TARGET_AVX10_2)
#define HAVE_vdpphps_v8sf_maskz (TARGET_AVX10_2)
#define HAVE_vdpphps_v4sf_maskz (TARGET_AVX10_2)
#define HAVE_smaxv32bf3 (TARGET_AVX10_2)
#define HAVE_sminv32bf3 (TARGET_AVX10_2)
#define HAVE_smaxv16bf3 (TARGET_AVX10_2)
#define HAVE_sminv16bf3 (TARGET_AVX10_2)
#define HAVE_smaxv8bf3 (TARGET_AVX10_2)
#define HAVE_sminv8bf3 (TARGET_AVX10_2)
#define HAVE_avx10_2_fmaddbf16_v32bf_maskz (TARGET_AVX10_2)
#define HAVE_avx10_2_fmaddbf16_v16bf_maskz (TARGET_AVX10_2)
#define HAVE_avx10_2_fmaddbf16_v8bf_maskz (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v32bf_maskz (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v16bf_maskz (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmaddbf16_v8bf_maskz (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v32bf_maskz (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v16bf_maskz (TARGET_AVX10_2)
#define HAVE_avx10_2_fmsubbf16_v8bf_maskz (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v32bf_maskz (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v16bf_maskz (TARGET_AVX10_2)
#define HAVE_avx10_2_fnmsubbf16_v8bf_maskz (TARGET_AVX10_2)
#define HAVE_sse2_lfence (TARGET_SSE2)
#define HAVE_sse_sfence (TARGET_SSE || TARGET_3DNOW_A)
#define HAVE_sse2_mfence (TARGET_SSE2)
#define HAVE_mem_thread_fence 1
#define HAVE_atomic_loadqi 1
#define HAVE_atomic_loadhi 1
#define HAVE_atomic_loadsi 1
#define HAVE_atomic_loaddi (TARGET_64BIT || (TARGET_CMPXCHG8B && (TARGET_80387 || TARGET_SSE)))
#define HAVE_atomic_storeqi 1
#define HAVE_atomic_storehi 1
#define HAVE_atomic_storesi 1
#define HAVE_atomic_storedi (TARGET_64BIT || (TARGET_CMPXCHG8B && (TARGET_80387 || TARGET_SSE)))
#define HAVE_atomic_compare_and_swapqi (TARGET_CMPXCHG)
#define HAVE_atomic_compare_and_swaphi (TARGET_CMPXCHG)
#define HAVE_atomic_compare_and_swapsi (TARGET_CMPXCHG)
#define HAVE_atomic_compare_and_swapdi ((TARGET_CMPXCHG) && (TARGET_64BIT || TARGET_CMPXCHG8B))
#define HAVE_atomic_fetch_andqi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_fetch_orqi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_fetch_xorqi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_fetch_andhi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_fetch_orhi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_fetch_xorhi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_fetch_andsi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_fetch_orsi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_fetch_xorsi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_and_fetchqi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_or_fetchqi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_xor_fetchqi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_and_fetchhi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_or_fetchhi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_xor_fetchhi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_and_fetchsi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_or_fetchsi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_xor_fetchsi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_fetch_nandqi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_fetch_nandhi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_fetch_nandsi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_nand_fetchqi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_nand_fetchhi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_nand_fetchsi (TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP)
#define HAVE_atomic_fetch_anddi ((TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP) && (TARGET_64BIT || TARGET_CMPXCHG8B))
#define HAVE_atomic_fetch_ordi ((TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP) && (TARGET_64BIT || TARGET_CMPXCHG8B))
#define HAVE_atomic_fetch_xordi ((TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP) && (TARGET_64BIT || TARGET_CMPXCHG8B))
#define HAVE_atomic_and_fetchdi ((TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP) && (TARGET_64BIT || TARGET_CMPXCHG8B))
#define HAVE_atomic_or_fetchdi ((TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP) && (TARGET_64BIT || TARGET_CMPXCHG8B))
#define HAVE_atomic_xor_fetchdi ((TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP) && (TARGET_64BIT || TARGET_CMPXCHG8B))
#define HAVE_atomic_fetch_nanddi ((TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP) && (TARGET_64BIT || TARGET_CMPXCHG8B))
#define HAVE_atomic_nand_fetchdi ((TARGET_CMPXCHG && TARGET_RELAX_CMPXCHG_LOOP) && (TARGET_64BIT || TARGET_CMPXCHG8B))
#define HAVE_atomic_bit_test_and_sethi 1
#define HAVE_atomic_bit_test_and_setsi 1
#define HAVE_atomic_bit_test_and_complementhi 1
#define HAVE_atomic_bit_test_and_complementsi 1
#define HAVE_atomic_bit_test_and_resethi 1
#define HAVE_atomic_bit_test_and_resetsi 1
#define HAVE_atomic_add_fetch_cmp_0qi 1
#define HAVE_atomic_sub_fetch_cmp_0qi 1
#define HAVE_atomic_add_fetch_cmp_0hi 1
#define HAVE_atomic_sub_fetch_cmp_0hi 1
#define HAVE_atomic_add_fetch_cmp_0si 1
#define HAVE_atomic_sub_fetch_cmp_0si 1
#define HAVE_atomic_and_fetch_cmp_0qi 1
#define HAVE_atomic_or_fetch_cmp_0qi 1
#define HAVE_atomic_xor_fetch_cmp_0qi 1
#define HAVE_atomic_and_fetch_cmp_0hi 1
#define HAVE_atomic_or_fetch_cmp_0hi 1
#define HAVE_atomic_xor_fetch_cmp_0hi 1
#define HAVE_atomic_and_fetch_cmp_0si 1
#define HAVE_atomic_or_fetch_cmp_0si 1
#define HAVE_atomic_xor_fetch_cmp_0si 1
extern rtx        gen_ccmpqi                                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ccmphi                                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ccmpsi                                          (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_ccmpdi                                          (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_ccmpdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_x86_sahf_1                                      (rtx);
extern rtx        gen_x86_stc                                         (void);
extern rtx        gen_pushflsi2                                       (rtx, rtx);
extern rtx        gen_pushfldi2                                       (rtx, rtx);
extern rtx        gen_popflsi1                                        (rtx, rtx);
extern rtx        gen_popfldi1                                        (rtx, rtx);
extern rtx        gen_swapsi                                          (rtx, rtx);
static inline rtx gen_swapdi                                          (rtx, rtx);
static inline rtx
gen_swapdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_insvhi_1                                        (rtx, rtx);
extern rtx        gen_insvsi_1                                        (rtx, rtx);
static inline rtx gen_insvdi_1                                        (rtx, rtx);
static inline rtx
gen_insvdi_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_push2_di                                        (rtx, rtx, rtx);
extern rtx        gen_pop2_di                                         (rtx, rtx, rtx);
static inline rtx gen_pushp_di                                        (rtx, rtx);
static inline rtx
gen_pushp_di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_popp_di                                         (rtx, rtx);
extern rtx        gen_push2p_di                                       (rtx, rtx, rtx);
extern rtx        gen_pop2p_di                                        (rtx, rtx, rtx);
static inline rtx gen_zero_extendditi2                                (rtx, rtx);
static inline rtx
gen_zero_extendditi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_zero_extendqidi2                                (rtx, rtx);
static inline rtx
gen_zero_extendqidi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_zero_extendhidi2                                (rtx, rtx);
static inline rtx
gen_zero_extendhidi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_zero_extendqisi2_and                            (rtx, rtx);
extern rtx        gen_zero_extendhisi2_and                            (rtx, rtx);
extern rtx        gen_zero_extendqihi2_and                            (rtx, rtx);
extern rtx        gen_extendsidi2_1                                   (rtx, rtx);
static inline rtx gen_extendditi2                                     (rtx, rtx);
static inline rtx
gen_extendditi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_extendqidi2                                     (rtx, rtx);
static inline rtx
gen_extendqidi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_extendhidi2                                     (rtx, rtx);
static inline rtx
gen_extendhidi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_extendhisi2                                     (rtx, rtx);
extern rtx        gen_extendqisi2                                     (rtx, rtx);
extern rtx        gen_extendqihi2                                     (rtx, rtx);
extern rtx        gen_extendbfsf2_1                                   (rtx, rtx);
extern rtx        gen_truncdfsf2                                      (rtx, rtx);
extern rtx        gen_truncxfsf2                                      (rtx, rtx);
extern rtx        gen_truncxfdf2                                      (rtx, rtx);
extern rtx        gen_truncsfbf2                                      (rtx, rtx);
extern rtx        gen_fix_trunchfsi2                                  (rtx, rtx);
extern rtx        gen_fixuns_trunchfsi2                               (rtx, rtx);
static inline rtx gen_fix_trunchfdi2                                  (rtx, rtx);
static inline rtx
gen_fix_trunchfdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_fixuns_trunchfdi2                               (rtx, rtx);
static inline rtx
gen_fixuns_trunchfdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_fixuns_truncsfdi2                               (rtx, rtx);
static inline rtx
gen_fixuns_truncsfdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_fixuns_truncdfdi2                               (rtx, rtx);
static inline rtx
gen_fixuns_truncdfdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_fixuns_truncsfsi2_avx512f                       (rtx, rtx);
extern rtx        gen_fixuns_truncdfsi2_avx512f                       (rtx, rtx);
extern rtx        gen_fix_truncsfsi_sse                               (rtx, rtx);
static inline rtx gen_fix_truncsfdi_sse                               (rtx, rtx);
static inline rtx
gen_fix_truncsfdi_sse(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_fix_truncdfsi_sse                               (rtx, rtx);
static inline rtx gen_fix_truncdfdi_sse                               (rtx, rtx);
static inline rtx
gen_fix_truncdfdi_sse(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_fix_trunchi_i387_fisttp                         (rtx, rtx);
extern rtx        gen_fix_truncsi_i387_fisttp                         (rtx, rtx);
extern rtx        gen_fix_truncdi_i387_fisttp                         (rtx, rtx);
extern rtx        gen_fix_truncdi_i387                                (rtx, rtx, rtx, rtx);
extern rtx        gen_fix_trunchi_i387                                (rtx, rtx, rtx, rtx);
extern rtx        gen_fix_truncsi_i387                                (rtx, rtx, rtx, rtx);
extern rtx        gen_x86_fnstcw_1                                    (rtx);
extern rtx        gen_floathisf2                                      (rtx, rtx);
extern rtx        gen_floathidf2                                      (rtx, rtx);
extern rtx        gen_floathixf2                                      (rtx, rtx);
extern rtx        gen_floatsixf2                                      (rtx, rtx);
extern rtx        gen_floatdixf2                                      (rtx, rtx);
extern rtx        gen_floatsihf2                                      (rtx, rtx);
extern rtx        gen_floatunssihf2                                   (rtx, rtx);
static inline rtx gen_floatdihf2                                      (rtx, rtx);
static inline rtx
gen_floatdihf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_floatunsdihf2                                   (rtx, rtx);
static inline rtx
gen_floatunsdihf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_floatdisf2_i387_with_xmm                        (rtx, rtx, rtx);
extern rtx        gen_floatdidf2_i387_with_xmm                        (rtx, rtx, rtx);
extern rtx        gen_floatdixf2_i387_with_xmm                        (rtx, rtx, rtx);
extern rtx        gen_floatunssisf2_i387_with_xmm                     (rtx, rtx, rtx);
extern rtx        gen_floatunssidf2_i387_with_xmm                     (rtx, rtx, rtx);
extern rtx        gen_floatunssixf2_i387_with_xmm                     (rtx, rtx, rtx);
static inline rtx gen_addsi_1_zext                                    (rtx, rtx, rtx);
static inline rtx
gen_addsi_1_zext(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_addvqi4_1                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_addvhi4_1                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_addvsi4_1                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_addvdi4_1                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_subvqi4_1                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_subvhi4_1                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_subvsi4_1                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_subvdi4_1                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_addqi3_carry                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addhi3_carry                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addsi3_carry                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_adddi3_carry                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addcarrysi                                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addcarrydi                                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subqi3_carry                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subhi3_carry                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subsi3_carry                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subdi3_carry                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subsi3_carry_ccc                                (rtx, rtx, rtx);
static inline rtx gen_subdi3_carry_ccc                                (rtx, rtx, rtx);
static inline rtx
gen_subdi3_carry_ccc(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_subsi3_carry_ccgz                               (rtx, rtx, rtx);
static inline rtx gen_subdi3_carry_ccgz                               (rtx, rtx, rtx);
static inline rtx
gen_subdi3_carry_ccgz(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_subborrowsi                                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subborrowdi                                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addqi3_cc_overflow_1                            (rtx, rtx, rtx);
extern rtx        gen_addhi3_cc_overflow_1                            (rtx, rtx, rtx);
extern rtx        gen_addsi3_cc_overflow_1                            (rtx, rtx, rtx);
extern rtx        gen_adddi3_cc_overflow_1                            (rtx, rtx, rtx);
extern rtx        gen_smulsi3_highpart                                (rtx, rtx, rtx);
extern rtx        gen_umulsi3_highpart                                (rtx, rtx, rtx);
static inline rtx gen_smuldi3_highpart                                (rtx, rtx, rtx);
static inline rtx
gen_smuldi3_highpart(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_umuldi3_highpart                                (rtx, rtx, rtx);
static inline rtx
gen_umuldi3_highpart(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_divmodsi4_1                                     (rtx, rtx, rtx, rtx);
static inline rtx gen_divmoddi4_1                                     (rtx, rtx, rtx, rtx);
static inline rtx
gen_divmoddi4_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_udivmodsi4_1                                    (rtx, rtx, rtx, rtx);
static inline rtx gen_udivmoddi4_1                                    (rtx, rtx, rtx, rtx);
static inline rtx
gen_udivmoddi4_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_divmodsi4_zext_1                                (rtx, rtx, rtx, rtx);
static inline rtx
gen_divmodsi4_zext_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_udivmodsi4_zext_1                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_udivmodsi4_zext_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_divmodsi4_zext_2                                (rtx, rtx, rtx, rtx);
static inline rtx
gen_divmodsi4_zext_2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_udivmodsi4_zext_2                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_udivmodsi4_zext_2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_divmodhiqi3_nf                                  (rtx, rtx, rtx);
extern rtx        gen_divmodhiqi3                                     (rtx, rtx, rtx);
extern rtx        gen_udivmodhiqi3_nf                                 (rtx, rtx, rtx);
extern rtx        gen_udivmodhiqi3                                    (rtx, rtx, rtx);
extern rtx        gen_ashldi3_doubleword                              (rtx, rtx, rtx);
static inline rtx gen_ashlti3_doubleword                              (rtx, rtx, rtx);
static inline rtx
gen_ashlti3_doubleword(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_x86_64_shld_nf                                  (rtx, rtx, rtx);
static inline rtx
gen_x86_64_shld_nf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_x86_64_shld                                     (rtx, rtx, rtx);
static inline rtx
gen_x86_64_shld(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_x86_64_shld_ndd_nf                              (rtx, rtx, rtx, rtx);
extern rtx        gen_x86_64_shld_ndd                                 (rtx, rtx, rtx, rtx);
static inline rtx gen_x86_64_shld_1_nf                                (rtx, rtx, rtx, rtx);
static inline rtx
gen_x86_64_shld_1_nf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_x86_64_shld_1                                   (rtx, rtx, rtx, rtx);
static inline rtx
gen_x86_64_shld_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_x86_64_shld_ndd_1_nf                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_x86_64_shld_ndd_1                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_x86_shld_nf                                     (rtx, rtx, rtx);
extern rtx        gen_x86_shld                                        (rtx, rtx, rtx);
extern rtx        gen_x86_shld_ndd_nf                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_x86_shld_ndd                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_x86_shld_1_nf                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_x86_shld_1                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_x86_shld_ndd_1_nf                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_x86_shld_ndd_1                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_lshrdi3_doubleword                              (rtx, rtx, rtx);
extern rtx        gen_ashrdi3_doubleword                              (rtx, rtx, rtx);
static inline rtx gen_lshrti3_doubleword                              (rtx, rtx, rtx);
static inline rtx
gen_lshrti3_doubleword(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_ashrti3_doubleword                              (rtx, rtx, rtx);
static inline rtx
gen_ashrti3_doubleword(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_lshrdi3_doubleword_lowpart_nf                   (rtx, rtx, rtx);
extern rtx        gen_ashrdi3_doubleword_lowpart_nf                   (rtx, rtx, rtx);
static inline rtx gen_lshrti3_doubleword_lowpart_nf                   (rtx, rtx, rtx);
static inline rtx
gen_lshrti3_doubleword_lowpart_nf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_ashrti3_doubleword_lowpart_nf                   (rtx, rtx, rtx);
static inline rtx
gen_ashrti3_doubleword_lowpart_nf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_lshrdi3_doubleword_lowpart                      (rtx, rtx, rtx);
extern rtx        gen_ashrdi3_doubleword_lowpart                      (rtx, rtx, rtx);
static inline rtx gen_lshrti3_doubleword_lowpart                      (rtx, rtx, rtx);
static inline rtx
gen_lshrti3_doubleword_lowpart(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_ashrti3_doubleword_lowpart                      (rtx, rtx, rtx);
static inline rtx
gen_ashrti3_doubleword_lowpart(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_x86_64_shrd_nf                                  (rtx, rtx, rtx);
static inline rtx
gen_x86_64_shrd_nf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_x86_64_shrd                                     (rtx, rtx, rtx);
static inline rtx
gen_x86_64_shrd(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_x86_64_shrd_ndd_nf                              (rtx, rtx, rtx, rtx);
extern rtx        gen_x86_64_shrd_ndd                                 (rtx, rtx, rtx, rtx);
static inline rtx gen_x86_64_shrd_1_nf                                (rtx, rtx, rtx, rtx);
static inline rtx
gen_x86_64_shrd_1_nf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_x86_64_shrd_1                                   (rtx, rtx, rtx, rtx);
static inline rtx
gen_x86_64_shrd_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_x86_64_shrd_ndd_1_nf                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_x86_64_shrd_ndd_1                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_x86_shrd_nf                                     (rtx, rtx, rtx);
extern rtx        gen_x86_shrd                                        (rtx, rtx, rtx);
extern rtx        gen_x86_shrd_ndd_nf                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_x86_shrd_ndd                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_x86_shrd_1_nf                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_x86_shrd_1                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_x86_shrd_ndd_1_nf                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_x86_shrd_ndd_1                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashrsi3_cvt_nf                                  (rtx, rtx, rtx);
extern rtx        gen_ashrsi3_cvt                                     (rtx, rtx, rtx);
extern rtx        gen_ashrdi3_cvt_nf                                  (rtx, rtx, rtx);
extern rtx        gen_ashrdi3_cvt                                     (rtx, rtx, rtx);
extern rtx        gen_ix86_rotldi3_doubleword                         (rtx, rtx, rtx);
static inline rtx gen_ix86_rotlti3_doubleword                         (rtx, rtx, rtx);
static inline rtx
gen_ix86_rotlti3_doubleword(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_ix86_rotrdi3_doubleword                         (rtx, rtx, rtx);
static inline rtx gen_ix86_rotrti3_doubleword                         (rtx, rtx, rtx);
static inline rtx
gen_ix86_rotrti3_doubleword(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_rotl32di2_doubleword                            (rtx, rtx);
extern rtx        gen_rotr32di2_doubleword                            (rtx, rtx);
static inline rtx gen_rotl64ti2_doubleword                            (rtx, rtx);
static inline rtx
gen_rotl64ti2_doubleword(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_rotr64ti2_doubleword                            (rtx, rtx);
static inline rtx
gen_rotr64ti2_doubleword(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_rcrsi2                                          (rtx, rtx);
static inline rtx gen_rcrdi2                                          (rtx, rtx);
static inline rtx
gen_rcrdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_lshrsi3_carry                                   (rtx, rtx);
extern rtx        gen_ashrsi3_carry                                   (rtx, rtx);
static inline rtx gen_lshrdi3_carry                                   (rtx, rtx);
static inline rtx
gen_lshrdi3_carry(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_ashrdi3_carry                                   (rtx, rtx);
static inline rtx
gen_ashrdi3_carry(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_setcc_sf_sse                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_setcc_df_sse                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_setcc_hf_mask                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_jump                                            (rtx);
extern rtx        gen_blockage                                        (void);
extern rtx        gen_prologue_use                                    (rtx);
extern rtx        gen_simple_return_internal                          (void);
extern rtx        gen_interrupt_return                                (void);
extern rtx        gen_simple_return_internal_long                     (void);
extern rtx        gen_simple_return_pop_internal                      (rtx);
extern rtx        gen_nop                                             (void);
extern rtx        gen_nops                                            (rtx);
extern rtx        gen_max_skip_align                                  (rtx, rtx);
static inline rtx gen_set_got_rex64                                   (rtx);
static inline rtx
gen_set_got_rex64(rtx ARG_UNUSED (a))
{
  return 0;
}
static inline rtx gen_set_rip_rex64                                   (rtx, rtx);
static inline rtx
gen_set_rip_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_set_got_offset_rex64                            (rtx, rtx);
extern rtx        gen_eh_return_internal                              (void);
extern rtx        gen_split_stack_return                              (rtx);
extern rtx        gen_ffssi2_no_cmove                                 (rtx, rtx);
extern rtx        gen_ctzsi2                                          (rtx, rtx);
static inline rtx gen_ctzdi2                                          (rtx, rtx);
static inline rtx
gen_ctzdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_bsr_rex64                                       (rtx, rtx);
static inline rtx
gen_bsr_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_bsr_rex64_1                                     (rtx, rtx);
static inline rtx
gen_bsr_rex64_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_bsr_rex64_1_zext                                (rtx, rtx);
static inline rtx
gen_bsr_rex64_1_zext(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_bsr                                             (rtx, rtx);
extern rtx        gen_bsr_1                                           (rtx, rtx);
static inline rtx gen_bsr_zext_1                                      (rtx, rtx);
static inline rtx
gen_bsr_zext_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_clzsi2_lzcnt_nf                                 (rtx, rtx);
static inline rtx gen_clzdi2_lzcnt_nf                                 (rtx, rtx);
static inline rtx
gen_clzdi2_lzcnt_nf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_clzsi2_lzcnt                                    (rtx, rtx);
static inline rtx gen_clzdi2_lzcnt                                    (rtx, rtx);
static inline rtx
gen_clzdi2_lzcnt(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_tzcnt_si_nf                                     (rtx, rtx);
extern rtx        gen_lzcnt_si_nf                                     (rtx, rtx);
static inline rtx gen_tzcnt_di_nf                                     (rtx, rtx);
static inline rtx
gen_tzcnt_di_nf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_lzcnt_di_nf                                     (rtx, rtx);
static inline rtx
gen_lzcnt_di_nf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_tzcnt_si                                        (rtx, rtx);
extern rtx        gen_lzcnt_si                                        (rtx, rtx);
static inline rtx gen_tzcnt_di                                        (rtx, rtx);
static inline rtx
gen_tzcnt_di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_lzcnt_di                                        (rtx, rtx);
static inline rtx
gen_lzcnt_di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_tzcnt_hi_nf                                     (rtx, rtx);
extern rtx        gen_tzcnt_hi                                        (rtx, rtx);
extern rtx        gen_lzcnt_hi_nf                                     (rtx, rtx);
extern rtx        gen_lzcnt_hi                                        (rtx, rtx);
extern rtx        gen_bmi_bextr_si                                    (rtx, rtx, rtx);
static inline rtx gen_bmi_bextr_di                                    (rtx, rtx, rtx);
static inline rtx
gen_bmi_bextr_di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_bmi2_pdep_si3                                   (rtx, rtx, rtx);
static inline rtx gen_bmi2_pdep_di3                                   (rtx, rtx, rtx);
static inline rtx
gen_bmi2_pdep_di3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_bmi2_pext_si3                                   (rtx, rtx, rtx);
static inline rtx gen_bmi2_pext_di3                                   (rtx, rtx, rtx);
static inline rtx
gen_bmi2_pext_di3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_tbm_bextri_si                                   (rtx, rtx, rtx, rtx);
static inline rtx gen_tbm_bextri_di                                   (rtx, rtx, rtx, rtx);
static inline rtx
gen_tbm_bextri_di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_popcountsi2_nf                                  (rtx, rtx);
static inline rtx gen_popcountdi2_nf                                  (rtx, rtx);
static inline rtx
gen_popcountdi2_nf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_popcountsi2                                     (rtx, rtx);
static inline rtx gen_popcountdi2                                     (rtx, rtx);
static inline rtx
gen_popcountdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_popcounthi2_nf                                  (rtx, rtx);
extern rtx        gen_popcounthi2                                     (rtx, rtx);
extern rtx        gen_bswaphisi2_lowpart                              (rtx, rtx);
extern rtx        gen_parityhi2_cmp                                   (rtx);
extern rtx        gen_parityqi2_cmp                                   (rtx);
extern rtx        gen_rcphf2                                          (rtx, rtx);
extern rtx        gen_truncxfsf2_i387_noop_unspec                     (rtx, rtx);
extern rtx        gen_truncxfdf2_i387_noop_unspec                     (rtx, rtx);
extern rtx        gen_sqrtxf2                                         (rtx, rtx);
extern rtx        gen_rsqrthf2                                        (rtx, rtx);
extern rtx        gen_sqrthf2                                         (rtx, rtx);
extern rtx        gen_x86_fnstsw_1                                    (rtx);
extern rtx        gen_fpremxf4_i387                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_fprem1xf4_i387                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_sinxf2                                          (rtx, rtx);
extern rtx        gen_cosxf2                                          (rtx, rtx);
extern rtx        gen_sincosxf3                                       (rtx, rtx, rtx);
extern rtx        gen_fptanxf4_i387                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_atan2xf3                                        (rtx, rtx, rtx);
extern rtx        gen_fyl2xxf3_i387                                   (rtx, rtx, rtx);
extern rtx        gen_fyl2xp1xf3_i387                                 (rtx, rtx, rtx);
extern rtx        gen_fxtractxf3_i387                                 (rtx, rtx, rtx);
extern rtx        gen_fscalexf4_i387                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scalefsf2                               (rtx, rtx, rtx);
extern rtx        gen_avx512f_scalefdf2                               (rtx, rtx, rtx);
extern rtx        gen_sse4_1_roundhf2                                 (rtx, rtx, rtx);
extern rtx        gen_sse4_1_roundsf2                                 (rtx, rtx, rtx);
extern rtx        gen_sse4_1_rounddf2                                 (rtx, rtx, rtx);
extern rtx        gen_rintxf2                                         (rtx, rtx);
extern rtx        gen_lrintxfdi2                                      (rtx, rtx);
extern rtx        gen_lrintxfhi2                                      (rtx, rtx);
extern rtx        gen_lrintxfsi2                                      (rtx, rtx);
extern rtx        gen_frndintxf2_roundeven                            (rtx, rtx);
extern rtx        gen_frndintxf2_floor                                (rtx, rtx);
extern rtx        gen_frndintxf2_ceil                                 (rtx, rtx);
extern rtx        gen_frndintxf2_trunc                                (rtx, rtx);
extern rtx        gen_frndintxf2_roundeven_i387                       (rtx, rtx, rtx, rtx);
extern rtx        gen_frndintxf2_floor_i387                           (rtx, rtx, rtx, rtx);
extern rtx        gen_frndintxf2_ceil_i387                            (rtx, rtx, rtx, rtx);
extern rtx        gen_frndintxf2_trunc_i387                           (rtx, rtx, rtx, rtx);
extern rtx        gen_fistdi2_floor                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_fistdi2_ceil                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_fisthi2_floor                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_fisthi2_ceil                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_fistsi2_floor                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_fistsi2_ceil                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_fxamsf2_i387                                    (rtx, rtx);
extern rtx        gen_fxamdf2_i387                                    (rtx, rtx);
extern rtx        gen_fxamxf2_i387                                    (rtx, rtx);
extern rtx        gen_movmsk_df                                       (rtx, rtx);
extern rtx        gen_cld                                             (void);
extern rtx        gen_movhf_mask                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_smaxsf3                                         (rtx, rtx, rtx);
extern rtx        gen_sminsf3                                         (rtx, rtx, rtx);
extern rtx        gen_smaxdf3                                         (rtx, rtx, rtx);
extern rtx        gen_smindf3                                         (rtx, rtx, rtx);
extern rtx        gen_smaxhf3                                         (rtx, rtx, rtx);
extern rtx        gen_sminhf3                                         (rtx, rtx, rtx);
extern rtx        gen_pro_epilogue_adjust_stack_add_si                (rtx, rtx, rtx);
extern rtx        gen_pro_epilogue_adjust_stack_add_di                (rtx, rtx, rtx);
extern rtx        gen_pro_epilogue_adjust_stack_sub_si                (rtx, rtx, rtx);
extern rtx        gen_pro_epilogue_adjust_stack_sub_di                (rtx, rtx, rtx);
extern rtx        gen_allocate_stack_worker_probe_si                  (rtx, rtx);
extern rtx        gen_allocate_stack_worker_probe_di                  (rtx, rtx);
extern rtx        gen_probe_stack_1_si                                (rtx, rtx);
extern rtx        gen_probe_stack_1_di                                (rtx, rtx);
extern rtx        gen_adjust_stack_and_probe_si                       (rtx, rtx, rtx);
extern rtx        gen_adjust_stack_and_probe_di                       (rtx, rtx, rtx);
extern rtx        gen_probe_stack_range_si                            (rtx, rtx, rtx);
extern rtx        gen_probe_stack_range_di                            (rtx, rtx, rtx);
extern rtx        gen_stack_protect_set_1_si_si                       (rtx, rtx, rtx);
extern rtx        gen_stack_protect_set_1_di_si                       (rtx, rtx, rtx);
extern rtx        gen_stack_protect_set_1_si_di                       (rtx, rtx, rtx);
extern rtx        gen_stack_protect_set_1_di_di                       (rtx, rtx, rtx);
extern rtx        gen_stack_protect_test_1_si                         (rtx, rtx, rtx);
extern rtx        gen_stack_protect_test_1_di                         (rtx, rtx, rtx);
extern rtx        gen_trap                                            (void);
extern rtx        gen_ud2                                             (void);
static inline rtx gen_prefetchi                                       (rtx, rtx);
static inline rtx
gen_prefetchi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_sse4_2_crc32qi                                  (rtx, rtx, rtx);
extern rtx        gen_sse4_2_crc32hi                                  (rtx, rtx, rtx);
extern rtx        gen_sse4_2_crc32si                                  (rtx, rtx, rtx);
static inline rtx gen_sse4_2_crc32di                                  (rtx, rtx, rtx);
static inline rtx
gen_sse4_2_crc32di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_rdpmc                                           (rtx, rtx);
static inline rtx gen_rdpmc_rex64                                     (rtx, rtx, rtx);
static inline rtx
gen_rdpmc_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_rdtsc                                           (rtx);
static inline rtx gen_rdtsc_rex64                                     (rtx, rtx);
static inline rtx
gen_rdtsc_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_rdtscp                                          (rtx, rtx);
static inline rtx gen_rdtscp_rex64                                    (rtx, rtx, rtx);
static inline rtx
gen_rdtscp_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_fxsave                                          (rtx);
static inline rtx gen_fxsave64                                        (rtx);
static inline rtx
gen_fxsave64(rtx ARG_UNUSED (a))
{
  return 0;
}
extern rtx        gen_fxrstor                                         (rtx);
static inline rtx gen_fxrstor64                                       (rtx);
static inline rtx
gen_fxrstor64(rtx ARG_UNUSED (a))
{
  return 0;
}
extern rtx        gen_xsave                                           (rtx, rtx);
extern rtx        gen_xsaveopt                                        (rtx, rtx);
extern rtx        gen_xsavec                                          (rtx, rtx);
extern rtx        gen_xsaves                                          (rtx, rtx);
static inline rtx gen_xsave_rex64                                     (rtx, rtx, rtx);
static inline rtx
gen_xsave_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xsaveopt_rex64                                  (rtx, rtx, rtx);
static inline rtx
gen_xsaveopt_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xsavec_rex64                                    (rtx, rtx, rtx);
static inline rtx
gen_xsavec_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xsaves_rex64                                    (rtx, rtx, rtx);
static inline rtx
gen_xsaves_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xsave64                                         (rtx, rtx, rtx);
static inline rtx
gen_xsave64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xsaveopt64                                      (rtx, rtx, rtx);
static inline rtx
gen_xsaveopt64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xsavec64                                        (rtx, rtx, rtx);
static inline rtx
gen_xsavec64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xsaves64                                        (rtx, rtx, rtx);
static inline rtx
gen_xsaves64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_xrstor                                          (rtx, rtx);
extern rtx        gen_xrstors                                         (rtx, rtx);
static inline rtx gen_xrstor_rex64                                    (rtx, rtx, rtx);
static inline rtx
gen_xrstor_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xrstors_rex64                                   (rtx, rtx, rtx);
static inline rtx
gen_xrstors_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xrstor64                                        (rtx, rtx, rtx);
static inline rtx
gen_xrstor64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xrstors64                                       (rtx, rtx, rtx);
static inline rtx
gen_xrstors64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_xsetbv                                          (rtx, rtx);
static inline rtx gen_xsetbv_rex64                                    (rtx, rtx, rtx);
static inline rtx
gen_xsetbv_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_xgetbv                                          (rtx, rtx);
static inline rtx gen_xgetbv_rex64                                    (rtx, rtx, rtx);
static inline rtx
gen_xgetbv_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_fnstenv                                         (rtx);
extern rtx        gen_fldenv                                          (rtx);
extern rtx        gen_fnstsw                                          (rtx);
extern rtx        gen_fnclex                                          (void);
extern rtx        gen_lwp_llwpcbsi                                    (rtx);
extern rtx        gen_lwp_llwpcbdi                                    (rtx);
extern rtx        gen_lwp_slwpcbsi                                    (rtx);
extern rtx        gen_lwp_slwpcbdi                                    (rtx);
extern rtx        gen_lwp_lwpvalsi                                    (rtx, rtx, rtx);
static inline rtx gen_lwp_lwpvaldi                                    (rtx, rtx, rtx);
static inline rtx
gen_lwp_lwpvaldi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_lwp_lwpinssi                                    (rtx, rtx, rtx);
static inline rtx gen_lwp_lwpinsdi                                    (rtx, rtx, rtx);
static inline rtx
gen_lwp_lwpinsdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_rdfsbasesi                                      (rtx);
static inline rtx
gen_rdfsbasesi(rtx ARG_UNUSED (a))
{
  return 0;
}
static inline rtx gen_rdgsbasesi                                      (rtx);
static inline rtx
gen_rdgsbasesi(rtx ARG_UNUSED (a))
{
  return 0;
}
static inline rtx gen_rdfsbasedi                                      (rtx);
static inline rtx
gen_rdfsbasedi(rtx ARG_UNUSED (a))
{
  return 0;
}
static inline rtx gen_rdgsbasedi                                      (rtx);
static inline rtx
gen_rdgsbasedi(rtx ARG_UNUSED (a))
{
  return 0;
}
static inline rtx gen_wrfsbasesi                                      (rtx);
static inline rtx
gen_wrfsbasesi(rtx ARG_UNUSED (a))
{
  return 0;
}
static inline rtx gen_wrgsbasesi                                      (rtx);
static inline rtx
gen_wrgsbasesi(rtx ARG_UNUSED (a))
{
  return 0;
}
static inline rtx gen_wrfsbasedi                                      (rtx);
static inline rtx
gen_wrfsbasedi(rtx ARG_UNUSED (a))
{
  return 0;
}
static inline rtx gen_wrgsbasedi                                      (rtx);
static inline rtx
gen_wrgsbasedi(rtx ARG_UNUSED (a))
{
  return 0;
}
extern rtx        gen_ptwritesi                                       (rtx);
static inline rtx gen_ptwritedi                                       (rtx);
static inline rtx
gen_ptwritedi(rtx ARG_UNUSED (a))
{
  return 0;
}
extern rtx        gen_rdrandhi                                        (rtx);
extern rtx        gen_rdrandsi                                        (rtx);
static inline rtx gen_rdranddi                                        (rtx);
static inline rtx
gen_rdranddi(rtx ARG_UNUSED (a))
{
  return 0;
}
extern rtx        gen_rdseedhi                                        (rtx);
extern rtx        gen_rdseedsi                                        (rtx);
static inline rtx gen_rdseeddi                                        (rtx);
static inline rtx
gen_rdseeddi(rtx ARG_UNUSED (a))
{
  return 0;
}
extern rtx        gen_rdsspsi                                         (rtx, rtx);
static inline rtx gen_rdsspdi                                         (rtx, rtx);
static inline rtx
gen_rdsspdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_incsspsi                                        (rtx);
static inline rtx gen_incsspdi                                        (rtx);
static inline rtx
gen_incsspdi(rtx ARG_UNUSED (a))
{
  return 0;
}
extern rtx        gen_saveprevssp                                     (void);
extern rtx        gen_rstorssp                                        (rtx);
extern rtx        gen_wrsssi                                          (rtx, rtx);
static inline rtx gen_wrssdi                                          (rtx, rtx);
static inline rtx
gen_wrssdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_wrusssi                                         (rtx, rtx);
static inline rtx gen_wrussdi                                         (rtx, rtx);
static inline rtx
gen_wrussdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_setssbsy                                        (void);
extern rtx        gen_clrssbsy                                        (rtx);
extern rtx        gen_nop_endbr                                       (void);
extern rtx        gen_xbegin_1                                        (rtx, rtx);
extern rtx        gen_xend                                            (void);
extern rtx        gen_xabort                                          (rtx);
extern rtx        gen_xtest_1                                         (void);
extern rtx        gen_clwb                                            (rtx);
extern rtx        gen_clflushopt                                      (rtx);
extern rtx        gen_mwaitx                                          (rtx, rtx, rtx);
extern rtx        gen_monitorx_si                                     (rtx, rtx, rtx);
extern rtx        gen_monitorx_di                                     (rtx, rtx, rtx);
extern rtx        gen_clzero_si                                       (rtx);
extern rtx        gen_clzero_di                                       (rtx);
extern rtx        gen_rdpid                                           (rtx);
static inline rtx gen_rdpid_rex64                                     (rtx);
static inline rtx
gen_rdpid_rex64(rtx ARG_UNUSED (a))
{
  return 0;
}
extern rtx        gen_wbinvd                                          (void);
extern rtx        gen_wbnoinvd                                        (void);
extern rtx        gen_movdirisi                                       (rtx, rtx);
static inline rtx gen_movdiridi                                       (rtx, rtx);
static inline rtx
gen_movdiridi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_movdir64b_si                                    (rtx, rtx);
extern rtx        gen_movdir64b_di                                    (rtx, rtx);
extern rtx        gen_xsusldtrk                                       (void);
extern rtx        gen_xresldtrk                                       (void);
extern rtx        gen_enqcmd_si                                       (rtx, rtx);
extern rtx        gen_enqcmds_si                                      (rtx, rtx);
extern rtx        gen_enqcmd_di                                       (rtx, rtx);
extern rtx        gen_enqcmds_di                                      (rtx, rtx);
static inline rtx gen_clui                                            (void);
static inline rtx
gen_clui(void)
{
  return 0;
}
static inline rtx gen_stui                                            (void);
static inline rtx
gen_stui(void)
{
  return 0;
}
static inline rtx gen_testui                                          (void);
static inline rtx
gen_testui(void)
{
  return 0;
}
static inline rtx gen_senduipi                                        (rtx);
static inline rtx
gen_senduipi(rtx ARG_UNUSED (a))
{
  return 0;
}
extern rtx        gen_umwait                                          (rtx, rtx);
static inline rtx gen_umwait_rex64                                    (rtx, rtx, rtx);
static inline rtx
gen_umwait_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_umonitor_si                                     (rtx);
extern rtx        gen_umonitor_di                                     (rtx);
extern rtx        gen_tpause                                          (rtx, rtx);
static inline rtx gen_tpause_rex64                                    (rtx, rtx, rtx);
static inline rtx
gen_tpause_rex64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_cldemote                                        (rtx);
extern rtx        gen_speculation_barrier                             (void);
extern rtx        gen_serialize                                       (void);
extern rtx        gen_patchable_area                                  (rtx, rtx);
extern rtx        gen_hreset                                          (rtx);
static inline rtx gen_urdmsr                                          (rtx, rtx);
static inline rtx
gen_urdmsr(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_uwrmsr                                          (rtx, rtx);
static inline rtx
gen_uwrmsr(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_ldtilecfg                                       (rtx);
extern rtx        gen_sttilecfg                                       (rtx);
static inline rtx gen_movrsqi                                         (rtx, rtx);
static inline rtx
gen_movrsqi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_movrshi                                         (rtx, rtx);
static inline rtx
gen_movrshi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_movrssi                                         (rtx, rtx);
static inline rtx
gen_movrssi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_movrsdi                                         (rtx, rtx);
static inline rtx
gen_movrsdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_sse_movntq                                      (rtx, rtx);
extern rtx        gen_mmx_ieee_maxv2sf3                               (rtx, rtx, rtx);
extern rtx        gen_mmx_ieee_minv2sf3                               (rtx, rtx, rtx);
extern rtx        gen_mmx_rcpv2sf2                                    (rtx, rtx);
extern rtx        gen_mmx_rcpit1v2sf3                                 (rtx, rtx, rtx);
extern rtx        gen_mmx_rcpit2v2sf3                                 (rtx, rtx, rtx);
extern rtx        gen_mmx_rsqrtv2sf2                                  (rtx, rtx);
extern rtx        gen_mmx_rsqit1v2sf3                                 (rtx, rtx, rtx);
extern rtx        gen_mmx_hsubv2sf3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_gtv2sf3                                     (rtx, rtx, rtx);
extern rtx        gen_mmx_gev2sf3                                     (rtx, rtx, rtx);
static inline rtx gen_sse4_1_insertps_v2sf                            (rtx, rtx, rtx, rtx);
static inline rtx
gen_sse4_1_insertps_v2sf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_sse4_1_insertps_v2si                            (rtx, rtx, rtx, rtx);
static inline rtx
gen_sse4_1_insertps_v2si(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_mmx_blendvps                                    (rtx, rtx, rtx, rtx);
static inline rtx
gen_mmx_blendvps(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_andv2sf3                                        (rtx, rtx, rtx);
static inline rtx
gen_andv2sf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_iorv2sf3                                        (rtx, rtx, rtx);
static inline rtx
gen_iorv2sf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xorv2sf3                                        (rtx, rtx, rtx);
static inline rtx
gen_xorv2sf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mmx_fix_truncv2sfv2si2                          (rtx, rtx);
extern rtx        gen_mmx_floatv2siv2sf2                              (rtx, rtx);
extern rtx        gen_mmx_pf2iw                                       (rtx, rtx);
extern rtx        gen_mmx_pi2fw                                       (rtx, rtx);
extern rtx        gen_mmx_pswapdv2sf2                                 (rtx, rtx);
extern rtx        gen_andv2bf3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv2bf3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv2bf3                                        (rtx, rtx, rtx);
static inline rtx gen_andv4bf3                                        (rtx, rtx, rtx);
static inline rtx
gen_andv4bf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_iorv4bf3                                        (rtx, rtx, rtx);
static inline rtx
gen_iorv4bf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xorv4bf3                                        (rtx, rtx, rtx);
static inline rtx
gen_xorv4bf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_andv2hf3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv2hf3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv2hf3                                        (rtx, rtx, rtx);
static inline rtx gen_andv4hf3                                        (rtx, rtx, rtx);
static inline rtx
gen_andv4hf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_iorv4hf3                                        (rtx, rtx, rtx);
static inline rtx
gen_iorv4hf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xorv4hf3                                        (rtx, rtx, rtx);
static inline rtx
gen_xorv4hf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_negv2qi2                                        (rtx, rtx);
extern rtx        gen_addv4qi3                                        (rtx, rtx, rtx);
extern rtx        gen_subv4qi3                                        (rtx, rtx, rtx);
extern rtx        gen_addv2hi3                                        (rtx, rtx, rtx);
extern rtx        gen_subv2hi3                                        (rtx, rtx, rtx);
extern rtx        gen_addv2qi3                                        (rtx, rtx, rtx);
extern rtx        gen_subv2qi3                                        (rtx, rtx, rtx);
extern rtx        gen_ssaddv4qi3                                      (rtx, rtx, rtx);
extern rtx        gen_usaddv4qi3                                      (rtx, rtx, rtx);
extern rtx        gen_sssubv4qi3                                      (rtx, rtx, rtx);
extern rtx        gen_ussubv4qi3                                      (rtx, rtx, rtx);
extern rtx        gen_ssaddv2qi3                                      (rtx, rtx, rtx);
extern rtx        gen_usaddv2qi3                                      (rtx, rtx, rtx);
extern rtx        gen_sssubv2qi3                                      (rtx, rtx, rtx);
extern rtx        gen_ussubv2qi3                                      (rtx, rtx, rtx);
extern rtx        gen_ssaddv2hi3                                      (rtx, rtx, rtx);
extern rtx        gen_usaddv2hi3                                      (rtx, rtx, rtx);
extern rtx        gen_sssubv2hi3                                      (rtx, rtx, rtx);
extern rtx        gen_ussubv2hi3                                      (rtx, rtx, rtx);
static inline rtx gen_mulv2si3                                        (rtx, rtx, rtx);
static inline rtx
gen_mulv2si3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mulv2hi3                                        (rtx, rtx, rtx);
extern rtx        gen_smulv2hi3_highpart                              (rtx, rtx, rtx);
extern rtx        gen_umulv2hi3_highpart                              (rtx, rtx, rtx);
static inline rtx gen_smaxv8qi3                                       (rtx, rtx, rtx);
static inline rtx
gen_smaxv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_sminv8qi3                                       (rtx, rtx, rtx);
static inline rtx
gen_sminv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_smaxv2si3                                       (rtx, rtx, rtx);
static inline rtx
gen_smaxv2si3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_sminv2si3                                       (rtx, rtx, rtx);
static inline rtx
gen_sminv2si3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_smaxv4qi3                                       (rtx, rtx, rtx);
extern rtx        gen_sminv4qi3                                       (rtx, rtx, rtx);
extern rtx        gen_smaxv2qi3                                       (rtx, rtx, rtx);
extern rtx        gen_sminv2qi3                                       (rtx, rtx, rtx);
extern rtx        gen_smaxv2hi3                                       (rtx, rtx, rtx);
extern rtx        gen_sminv2hi3                                       (rtx, rtx, rtx);
static inline rtx gen_umaxv4hi3                                       (rtx, rtx, rtx);
static inline rtx
gen_umaxv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_uminv4hi3                                       (rtx, rtx, rtx);
static inline rtx
gen_uminv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_umaxv2si3                                       (rtx, rtx, rtx);
static inline rtx
gen_umaxv2si3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_uminv2si3                                       (rtx, rtx, rtx);
static inline rtx
gen_uminv2si3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_umaxv4qi3                                       (rtx, rtx, rtx);
extern rtx        gen_uminv4qi3                                       (rtx, rtx, rtx);
extern rtx        gen_umaxv2qi3                                       (rtx, rtx, rtx);
extern rtx        gen_uminv2qi3                                       (rtx, rtx, rtx);
extern rtx        gen_umaxv2hi3                                       (rtx, rtx, rtx);
extern rtx        gen_uminv2hi3                                       (rtx, rtx, rtx);
extern rtx        gen_ssse3_absv8qi2                                  (rtx, rtx);
extern rtx        gen_ssse3_absv4hi2                                  (rtx, rtx);
extern rtx        gen_ssse3_absv2si2                                  (rtx, rtx);
extern rtx        gen_absv4qi2                                        (rtx, rtx);
extern rtx        gen_absv2qi2                                        (rtx, rtx);
extern rtx        gen_absv2hi2                                        (rtx, rtx);
extern rtx        gen_mmx_ashrv4hi3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_ashrv2si3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_ashlv4hi3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_lshrv4hi3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_ashlv2si3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_lshrv2si3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_ashlv1di3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_lshrv1di3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_ashlv1si3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_lshrv1si3                                   (rtx, rtx, rtx);
extern rtx        gen_ashlv2hi3                                       (rtx, rtx, rtx);
extern rtx        gen_lshrv2hi3                                       (rtx, rtx, rtx);
extern rtx        gen_ashrv2hi3                                       (rtx, rtx, rtx);
extern rtx        gen_ashlv2qi3                                       (rtx, rtx, rtx);
extern rtx        gen_lshrv2qi3                                       (rtx, rtx, rtx);
extern rtx        gen_ashrv2qi3                                       (rtx, rtx, rtx);
extern rtx        gen_mmx_gtv8qi3                                     (rtx, rtx, rtx);
extern rtx        gen_mmx_gtv4hi3                                     (rtx, rtx, rtx);
extern rtx        gen_mmx_gtv2si3                                     (rtx, rtx, rtx);
static inline rtx gen_mmx_pblendvb_v8qi                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_mmx_pblendvb_v8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_mmx_pblendvb_v4qi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_mmx_pblendvb_v2qi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_mmx_pblendvb_v2hi                               (rtx, rtx, rtx, rtx);
static inline rtx gen_mmx_ppermv64                                    (rtx, rtx, rtx, rtx);
static inline rtx
gen_mmx_ppermv64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_mmx_ppermv32                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_one_cmplv4qi2                                   (rtx, rtx);
extern rtx        gen_one_cmplv2qi2                                   (rtx, rtx);
extern rtx        gen_one_cmplv2hi2                                   (rtx, rtx);
extern rtx        gen_mmx_andnotv8qi3                                 (rtx, rtx, rtx);
extern rtx        gen_mmx_andnotv4hi3                                 (rtx, rtx, rtx);
extern rtx        gen_mmx_andnotv2si3                                 (rtx, rtx, rtx);
extern rtx        gen_mmx_packsswb                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_packuswb                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_packssdw                                    (rtx, rtx, rtx);
static inline rtx gen_mmx_packusdw                                    (rtx, rtx, rtx);
static inline rtx
gen_mmx_packusdw(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mmx_punpckhbw                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_punpckhbw_low                               (rtx, rtx, rtx);
extern rtx        gen_mmx_punpcklbw                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_punpcklbw_low                               (rtx, rtx, rtx);
extern rtx        gen_mmx_punpckhwd                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_punpcklwd                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_punpckhdq                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_punpckldq                                   (rtx, rtx, rtx);
static inline rtx gen_sse4_1_sign_extendv4qiv4hi2                     (rtx, rtx);
static inline rtx
gen_sse4_1_sign_extendv4qiv4hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_sse4_1_zero_extendv4qiv4hi2                     (rtx, rtx);
static inline rtx
gen_sse4_1_zero_extendv4qiv4hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_sse4_1_sign_extendv2hiv2si2                     (rtx, rtx);
static inline rtx
gen_sse4_1_sign_extendv2hiv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_sse4_1_zero_extendv2hiv2si2                     (rtx, rtx);
static inline rtx
gen_sse4_1_zero_extendv2hiv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_sse4_1_sign_extendv2qiv2si2                     (rtx, rtx);
static inline rtx
gen_sse4_1_sign_extendv2qiv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_sse4_1_zero_extendv2qiv2si2                     (rtx, rtx);
static inline rtx
gen_sse4_1_zero_extendv2qiv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_sse4_1_sign_extendv2qiv2hi2                     (rtx, rtx);
extern rtx        gen_sse4_1_zero_extendv2qiv2hi2                     (rtx, rtx);
static inline rtx gen_avx512vl_truncv4hiv4qi2                         (rtx, rtx);
static inline rtx
gen_avx512vl_truncv4hiv4qi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_avx512vl_truncv2hiv2qi2                         (rtx, rtx);
static inline rtx gen_avx512vl_truncv2siv2qi2                         (rtx, rtx);
static inline rtx
gen_avx512vl_truncv2siv2qi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx512vl_truncv2siv2hi2                         (rtx, rtx);
static inline rtx
gen_avx512vl_truncv2siv2hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_mmx_pshufbv8qi3                                 (rtx, rtx, rtx);
static inline rtx
gen_mmx_pshufbv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mmx_pshufbv4qi3                                 (rtx, rtx, rtx);
extern rtx        gen_mmx_pshufwv4hf_1                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mmx_pshufwv4bf_1                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mmx_pshufwv4hi_1                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mmx_pswapdv2si2                                 (rtx, rtx);
extern rtx        gen_uavgv4qi3_ceil                                  (rtx, rtx, rtx);
extern rtx        gen_uavgv2qi3_ceil                                  (rtx, rtx, rtx);
extern rtx        gen_uavgv2hi3_ceil                                  (rtx, rtx, rtx);
extern rtx        gen_mmx_pmovmskb                                    (rtx, rtx);
static inline rtx gen_popcountv8qi2                                   (rtx, rtx);
static inline rtx
gen_popcountv8qi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_popcountv4qi2                                   (rtx, rtx);
extern rtx        gen_popcountv2qi2                                   (rtx, rtx);
static inline rtx gen_popcountv4hi2                                   (rtx, rtx);
static inline rtx
gen_popcountv4hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_popcountv2hi2                                   (rtx, rtx);
static inline rtx gen_popcountv2si2                                   (rtx, rtx);
static inline rtx
gen_popcountv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_movv64qi_internal                               (rtx, rtx);
extern rtx        gen_movv32qi_internal                               (rtx, rtx);
extern rtx        gen_movv16qi_internal                               (rtx, rtx);
extern rtx        gen_movv32hi_internal                               (rtx, rtx);
extern rtx        gen_movv16hi_internal                               (rtx, rtx);
extern rtx        gen_movv8hi_internal                                (rtx, rtx);
extern rtx        gen_movv16si_internal                               (rtx, rtx);
extern rtx        gen_movv8si_internal                                (rtx, rtx);
extern rtx        gen_movv4si_internal                                (rtx, rtx);
extern rtx        gen_movv8di_internal                                (rtx, rtx);
extern rtx        gen_movv4di_internal                                (rtx, rtx);
extern rtx        gen_movv2di_internal                                (rtx, rtx);
extern rtx        gen_movv4ti_internal                                (rtx, rtx);
extern rtx        gen_movv2ti_internal                                (rtx, rtx);
extern rtx        gen_movv1ti_internal                                (rtx, rtx);
extern rtx        gen_movv32hf_internal                               (rtx, rtx);
extern rtx        gen_movv16hf_internal                               (rtx, rtx);
extern rtx        gen_movv8hf_internal                                (rtx, rtx);
extern rtx        gen_movv32bf_internal                               (rtx, rtx);
extern rtx        gen_movv16bf_internal                               (rtx, rtx);
extern rtx        gen_movv8bf_internal                                (rtx, rtx);
extern rtx        gen_movv16sf_internal                               (rtx, rtx);
extern rtx        gen_movv8sf_internal                                (rtx, rtx);
extern rtx        gen_movv4sf_internal                                (rtx, rtx);
extern rtx        gen_movv8df_internal                                (rtx, rtx);
extern rtx        gen_movv4df_internal                                (rtx, rtx);
extern rtx        gen_movv2df_internal                                (rtx, rtx);
extern rtx        gen_avx512f_movhf_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_movsf_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_movdf_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_storehf_mask                            (rtx, rtx, rtx);
extern rtx        gen_avx512f_storesf_mask                            (rtx, rtx, rtx);
extern rtx        gen_avx512f_storedf_mask                            (rtx, rtx, rtx);
extern rtx        gen_avx512f_blendmv16si                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv8si                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv4si                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_blendmv8di                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv4di                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv2di                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_blendmv16sf                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv8sf                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv4sf                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_blendmv8df                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv4df                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv2df                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_blendmv64qi                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv16qi                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv32qi                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_blendmv32hi                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv16hi                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv8hi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_blendmv32hf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv16hf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_blendmv8hf                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_blendmv32bf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv16bf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_blendmv8bf                             (rtx, rtx, rtx, rtx);
extern rtx        gen_movdi_to_sse                                    (rtx, rtx);
extern rtx        gen_avx_lddqu256                                    (rtx, rtx);
extern rtx        gen_sse3_lddqu                                      (rtx, rtx);
extern rtx        gen_sse2_movntisi                                   (rtx, rtx);
static inline rtx gen_sse2_movntidi                                   (rtx, rtx);
static inline rtx
gen_sse2_movntidi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_avx512f_movntv16sf                              (rtx, rtx);
extern rtx        gen_avx_movntv8sf                                   (rtx, rtx);
extern rtx        gen_sse_movntv4sf                                   (rtx, rtx);
extern rtx        gen_avx512f_movntv8df                               (rtx, rtx);
extern rtx        gen_avx_movntv4df                                   (rtx, rtx);
extern rtx        gen_sse2_movntv2df                                  (rtx, rtx);
extern rtx        gen_avx512f_movntv8di                               (rtx, rtx);
extern rtx        gen_avx_movntv4di                                   (rtx, rtx);
extern rtx        gen_sse2_movntv2di                                  (rtx, rtx);
extern rtx        gen_kandqi                                          (rtx, rtx, rtx);
extern rtx        gen_kiorqi                                          (rtx, rtx, rtx);
extern rtx        gen_kxorqi                                          (rtx, rtx, rtx);
extern rtx        gen_kandhi                                          (rtx, rtx, rtx);
extern rtx        gen_kiorhi                                          (rtx, rtx, rtx);
extern rtx        gen_kxorhi                                          (rtx, rtx, rtx);
extern rtx        gen_kandsi                                          (rtx, rtx, rtx);
extern rtx        gen_kiorsi                                          (rtx, rtx, rtx);
extern rtx        gen_kxorsi                                          (rtx, rtx, rtx);
extern rtx        gen_kanddi                                          (rtx, rtx, rtx);
extern rtx        gen_kiordi                                          (rtx, rtx, rtx);
extern rtx        gen_kxordi                                          (rtx, rtx, rtx);
extern rtx        gen_kandnqi                                         (rtx, rtx, rtx);
extern rtx        gen_kandnhi                                         (rtx, rtx, rtx);
extern rtx        gen_kandnsi                                         (rtx, rtx, rtx);
extern rtx        gen_kandndi                                         (rtx, rtx, rtx);
extern rtx        gen_kxnorqi                                         (rtx, rtx, rtx);
extern rtx        gen_kxnorhi                                         (rtx, rtx, rtx);
extern rtx        gen_kxnorsi                                         (rtx, rtx, rtx);
extern rtx        gen_kxnordi                                         (rtx, rtx, rtx);
extern rtx        gen_knotqi                                          (rtx, rtx);
extern rtx        gen_knothi                                          (rtx, rtx);
extern rtx        gen_knotsi                                          (rtx, rtx);
extern rtx        gen_knotdi                                          (rtx, rtx);
extern rtx        gen_kaddqi                                          (rtx, rtx, rtx);
extern rtx        gen_kaddhi                                          (rtx, rtx, rtx);
extern rtx        gen_kaddsi                                          (rtx, rtx, rtx);
extern rtx        gen_kadddi                                          (rtx, rtx, rtx);
extern rtx        gen_kashiftqi                                       (rtx, rtx, rtx);
extern rtx        gen_klshiftrtqi                                     (rtx, rtx, rtx);
extern rtx        gen_kashifthi                                       (rtx, rtx, rtx);
extern rtx        gen_klshiftrthi                                     (rtx, rtx, rtx);
extern rtx        gen_kashiftsi                                       (rtx, rtx, rtx);
extern rtx        gen_klshiftrtsi                                     (rtx, rtx, rtx);
extern rtx        gen_kashiftdi                                       (rtx, rtx, rtx);
extern rtx        gen_klshiftrtdi                                     (rtx, rtx, rtx);
extern rtx        gen_ktestqi                                         (rtx, rtx);
extern rtx        gen_ktesthi                                         (rtx, rtx);
extern rtx        gen_ktestsi                                         (rtx, rtx);
extern rtx        gen_ktestdi                                         (rtx, rtx);
extern rtx        gen_kortestqi_ccc                                   (rtx, rtx);
extern rtx        gen_kortesthi_ccc                                   (rtx, rtx);
extern rtx        gen_kortestsi_ccc                                   (rtx, rtx);
extern rtx        gen_kortestdi_ccc                                   (rtx, rtx);
extern rtx        gen_kortestqi_ccz                                   (rtx, rtx);
extern rtx        gen_kortesthi_ccz                                   (rtx, rtx);
extern rtx        gen_kortestsi_ccz                                   (rtx, rtx);
extern rtx        gen_kortestdi_ccz                                   (rtx, rtx);
extern rtx        gen_kunpckhi                                        (rtx, rtx, rtx);
extern rtx        gen_kunpcksi                                        (rtx, rtx, rtx);
extern rtx        gen_kunpckdi                                        (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmaddv8hf3                           (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmaddv8hf3_round                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmaddv8hf3_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmaddv8hf3_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmsubv8hf3                           (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmsubv8hf3_round                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmsubv8hf3_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmsubv8hf3_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmaddv4sf3                                  (rtx, rtx, rtx);
extern rtx        gen_sse_vmaddv4sf3_round                            (rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmaddv4sf3_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmaddv4sf3_mask_round                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmsubv4sf3                                  (rtx, rtx, rtx);
extern rtx        gen_sse_vmsubv4sf3_round                            (rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmsubv4sf3_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmsubv4sf3_mask_round                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmaddv2df3                                 (rtx, rtx, rtx);
extern rtx        gen_sse2_vmaddv2df3_round                           (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmaddv2df3_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmaddv2df3_mask_round                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmsubv2df3                                 (rtx, rtx, rtx);
extern rtx        gen_sse2_vmsubv2df3_round                           (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmsubv2df3_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmsubv2df3_mask_round                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmmulv8hf3                           (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmmulv8hf3_round                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmmulv8hf3_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmmulv8hf3_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmdivv8hf3                           (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmdivv8hf3_round                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmdivv8hf3_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmdivv8hf3_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmmulv4sf3                                  (rtx, rtx, rtx);
extern rtx        gen_sse_vmmulv4sf3_round                            (rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmmulv4sf3_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmmulv4sf3_mask_round                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmdivv4sf3                                  (rtx, rtx, rtx);
extern rtx        gen_sse_vmdivv4sf3_round                            (rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmdivv4sf3_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmdivv4sf3_mask_round                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmmulv2df3                                 (rtx, rtx, rtx);
extern rtx        gen_sse2_vmmulv2df3_round                           (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmmulv2df3_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmmulv2df3_mask_round                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmdivv2df3                                 (rtx, rtx, rtx);
extern rtx        gen_sse2_vmdivv2df3_round                           (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmdivv2df3_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmdivv2df3_mask_round                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_divv32hf3                            (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_divv32hf3_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_divv32hf3_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_divv32hf3_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_divv16hf3                            (rtx, rtx, rtx);
static inline rtx gen_avx512fp16_divv16hf3_round                      (rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_divv16hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_avx512fp16_divv16hf3_mask                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_divv16hf3_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_divv16hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512fp16_divv8hf3                             (rtx, rtx, rtx);
static inline rtx gen_avx512fp16_divv8hf3_round                       (rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_divv8hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_avx512fp16_divv8hf3_mask                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_divv8hf3_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_divv8hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_divv16sf3                               (rtx, rtx, rtx);
extern rtx        gen_avx512f_divv16sf3_round                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_divv16sf3_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_divv16sf3_mask_round                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_divv8sf3                                    (rtx, rtx, rtx);
static inline rtx gen_avx_divv8sf3_round                              (rtx, rtx, rtx, rtx);
static inline rtx
gen_avx_divv8sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_avx_divv8sf3_mask                               (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx_divv8sf3_mask_round                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx_divv8sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_sse_divv4sf3                                    (rtx, rtx, rtx);
static inline rtx gen_sse_divv4sf3_round                              (rtx, rtx, rtx, rtx);
static inline rtx
gen_sse_divv4sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_sse_divv4sf3_mask                               (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_sse_divv4sf3_mask_round                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_sse_divv4sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_divv8df3                                (rtx, rtx, rtx);
extern rtx        gen_avx512f_divv8df3_round                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_divv8df3_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_divv8df3_mask_round                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_divv4df3                                    (rtx, rtx, rtx);
static inline rtx gen_avx_divv4df3_round                              (rtx, rtx, rtx, rtx);
static inline rtx
gen_avx_divv4df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_avx_divv4df3_mask                               (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx_divv4df3_mask_round                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx_divv4df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_sse2_divv2df3                                   (rtx, rtx, rtx);
static inline rtx gen_sse2_divv2df3_round                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_sse2_divv2df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_sse2_divv2df3_mask                              (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_sse2_divv2df3_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_sse2_divv2df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx_rcpv8sf2                                    (rtx, rtx);
extern rtx        gen_sse_rcpv4sf2                                    (rtx, rtx);
extern rtx        gen_sse_vmrcpv4sf2                                  (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_rcpv32hf2                            (rtx, rtx);
extern rtx        gen_avx512fp16_rcpv32hf2_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_rcpv16hf2                            (rtx, rtx);
extern rtx        gen_avx512fp16_rcpv16hf2_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_rcpv8hf2                             (rtx, rtx);
extern rtx        gen_avx512fp16_rcpv8hf2_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmrcpv8hf2                           (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmrcpv8hf2_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_rcp14v16sf_mask                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_rcp14v8sf_mask                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_rcp14v4sf_mask                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_rcp14v8df_mask                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_rcp14v4df_mask                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_rcp14v2df_mask                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_srcp14v4sf                                      (rtx, rtx, rtx);
extern rtx        gen_srcp14v2df                                      (rtx, rtx, rtx);
extern rtx        gen_srcp14v4sf_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_srcp14v2df_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_sqrtv32hf2                           (rtx, rtx);
extern rtx        gen_avx512fp16_sqrtv32hf2_round                     (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_sqrtv32hf2_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_sqrtv32hf2_mask_round                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_sqrtv16hf2                           (rtx, rtx);
static inline rtx gen_avx512fp16_sqrtv16hf2_round                     (rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_sqrtv16hf2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx512fp16_sqrtv16hf2_mask                      (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_sqrtv16hf2_mask_round                (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_sqrtv16hf2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512fp16_sqrtv8hf2                            (rtx, rtx);
static inline rtx gen_avx512fp16_sqrtv8hf2_round                      (rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_sqrtv8hf2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx512fp16_sqrtv8hf2_mask                       (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_sqrtv8hf2_mask_round                 (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_sqrtv8hf2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512f_sqrtv16sf2                              (rtx, rtx);
extern rtx        gen_avx512f_sqrtv16sf2_round                        (rtx, rtx, rtx);
extern rtx        gen_avx512f_sqrtv16sf2_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sqrtv16sf2_mask_round                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_sqrtv8sf2                                   (rtx, rtx);
static inline rtx gen_avx_sqrtv8sf2_round                             (rtx, rtx, rtx);
static inline rtx
gen_avx_sqrtv8sf2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx_sqrtv8sf2_mask                              (rtx, rtx, rtx, rtx);
static inline rtx gen_avx_sqrtv8sf2_mask_round                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx_sqrtv8sf2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_sse_sqrtv4sf2                                   (rtx, rtx);
static inline rtx gen_sse_sqrtv4sf2_round                             (rtx, rtx, rtx);
static inline rtx
gen_sse_sqrtv4sf2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_sse_sqrtv4sf2_mask                              (rtx, rtx, rtx, rtx);
static inline rtx gen_sse_sqrtv4sf2_mask_round                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_sse_sqrtv4sf2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512f_sqrtv8df2                               (rtx, rtx);
extern rtx        gen_avx512f_sqrtv8df2_round                         (rtx, rtx, rtx);
extern rtx        gen_avx512f_sqrtv8df2_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sqrtv8df2_mask_round                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_sqrtv4df2                                   (rtx, rtx);
static inline rtx gen_avx_sqrtv4df2_round                             (rtx, rtx, rtx);
static inline rtx
gen_avx_sqrtv4df2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx_sqrtv4df2_mask                              (rtx, rtx, rtx, rtx);
static inline rtx gen_avx_sqrtv4df2_mask_round                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx_sqrtv4df2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_sse2_sqrtv2df2                                  (rtx, rtx);
static inline rtx gen_sse2_sqrtv2df2_round                            (rtx, rtx, rtx);
static inline rtx
gen_sse2_sqrtv2df2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_sse2_sqrtv2df2_mask                             (rtx, rtx, rtx, rtx);
static inline rtx gen_sse2_sqrtv2df2_mask_round                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_sse2_sqrtv2df2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512fp16_vmsqrtv8hf2                          (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmsqrtv8hf2_round                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmsqrtv8hf2_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmsqrtv8hf2_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmsqrtv4sf2                                 (rtx, rtx, rtx);
extern rtx        gen_sse_vmsqrtv4sf2_round                           (rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmsqrtv4sf2_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmsqrtv4sf2_mask_round                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmsqrtv2df2                                (rtx, rtx, rtx);
extern rtx        gen_sse2_vmsqrtv2df2_round                          (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmsqrtv2df2_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmsqrtv2df2_mask_round                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_rsqrtv8sf2                                  (rtx, rtx);
extern rtx        gen_sse_rsqrtv4sf2                                  (rtx, rtx);
extern rtx        gen_avx512fp16_rsqrtv32hf2                          (rtx, rtx);
extern rtx        gen_avx512fp16_rsqrtv32hf2_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_rsqrtv16hf2                          (rtx, rtx);
extern rtx        gen_avx512fp16_rsqrtv16hf2_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_rsqrtv8hf2                           (rtx, rtx);
extern rtx        gen_avx512fp16_rsqrtv8hf2_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_rsqrt14v16sf_mask                               (rtx, rtx, rtx, rtx);
extern rtx        gen_rsqrt14v8sf_mask                                (rtx, rtx, rtx, rtx);
extern rtx        gen_rsqrt14v4sf_mask                                (rtx, rtx, rtx, rtx);
extern rtx        gen_rsqrt14v8df_mask                                (rtx, rtx, rtx, rtx);
extern rtx        gen_rsqrt14v4df_mask                                (rtx, rtx, rtx, rtx);
extern rtx        gen_rsqrt14v2df_mask                                (rtx, rtx, rtx, rtx);
extern rtx        gen_rsqrt14v4sf                                     (rtx, rtx, rtx);
extern rtx        gen_rsqrt14v2df                                     (rtx, rtx, rtx);
extern rtx        gen_rsqrt14_v4sf_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_rsqrt14_v2df_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmrsqrtv4sf2                                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmrsqrtv8hf2                         (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmrsqrtv8hf2_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_maxv32hf3                                  (rtx, rtx, rtx);
extern rtx        gen_ieee_maxv32hf3_round                            (rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_maxv32hf3_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_maxv32hf3_mask_round                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_minv32hf3                                  (rtx, rtx, rtx);
extern rtx        gen_ieee_minv32hf3_round                            (rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_minv32hf3_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_minv32hf3_mask_round                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_maxv16hf3                                  (rtx, rtx, rtx);
static inline rtx gen_ieee_maxv16hf3_round                            (rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_maxv16hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_ieee_maxv16hf3_mask                             (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_ieee_maxv16hf3_mask_round                       (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_maxv16hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_ieee_minv16hf3                                  (rtx, rtx, rtx);
static inline rtx gen_ieee_minv16hf3_round                            (rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_minv16hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_ieee_minv16hf3_mask                             (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_ieee_minv16hf3_mask_round                       (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_minv16hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_ieee_maxv8hf3                                   (rtx, rtx, rtx);
static inline rtx gen_ieee_maxv8hf3_round                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_maxv8hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_ieee_maxv8hf3_mask                              (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_ieee_maxv8hf3_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_maxv8hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_ieee_minv8hf3                                   (rtx, rtx, rtx);
static inline rtx gen_ieee_minv8hf3_round                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_minv8hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_ieee_minv8hf3_mask                              (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_ieee_minv8hf3_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_minv8hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_ieee_maxv16sf3                                  (rtx, rtx, rtx);
extern rtx        gen_ieee_maxv16sf3_round                            (rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_maxv16sf3_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_maxv16sf3_mask_round                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_minv16sf3                                  (rtx, rtx, rtx);
extern rtx        gen_ieee_minv16sf3_round                            (rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_minv16sf3_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_minv16sf3_mask_round                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_maxv8sf3                                   (rtx, rtx, rtx);
static inline rtx gen_ieee_maxv8sf3_round                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_maxv8sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_ieee_maxv8sf3_mask                              (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_ieee_maxv8sf3_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_maxv8sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_ieee_minv8sf3                                   (rtx, rtx, rtx);
static inline rtx gen_ieee_minv8sf3_round                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_minv8sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_ieee_minv8sf3_mask                              (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_ieee_minv8sf3_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_minv8sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_ieee_maxv4sf3                                   (rtx, rtx, rtx);
static inline rtx gen_ieee_maxv4sf3_round                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_maxv4sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_ieee_maxv4sf3_mask                              (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_ieee_maxv4sf3_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_maxv4sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_ieee_minv4sf3                                   (rtx, rtx, rtx);
static inline rtx gen_ieee_minv4sf3_round                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_minv4sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_ieee_minv4sf3_mask                              (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_ieee_minv4sf3_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_minv4sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_ieee_maxv8df3                                   (rtx, rtx, rtx);
extern rtx        gen_ieee_maxv8df3_round                             (rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_maxv8df3_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_maxv8df3_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_minv8df3                                   (rtx, rtx, rtx);
extern rtx        gen_ieee_minv8df3_round                             (rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_minv8df3_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_minv8df3_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ieee_maxv4df3                                   (rtx, rtx, rtx);
static inline rtx gen_ieee_maxv4df3_round                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_maxv4df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_ieee_maxv4df3_mask                              (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_ieee_maxv4df3_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_maxv4df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_ieee_minv4df3                                   (rtx, rtx, rtx);
static inline rtx gen_ieee_minv4df3_round                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_minv4df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_ieee_minv4df3_mask                              (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_ieee_minv4df3_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_minv4df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_ieee_maxv2df3                                   (rtx, rtx, rtx);
static inline rtx gen_ieee_maxv2df3_round                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_maxv2df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_ieee_maxv2df3_mask                              (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_ieee_maxv2df3_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_maxv2df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_ieee_minv2df3                                   (rtx, rtx, rtx);
static inline rtx gen_ieee_minv2df3_round                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_minv2df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_ieee_minv2df3_mask                              (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_ieee_minv2df3_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_ieee_minv2df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512fp16_ieee_vmmaxv8hf3                      (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_ieee_vmmaxv8hf3_mask                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_ieee_vmmaxv8hf3_round                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_ieee_vmmaxv8hf3_mask_round           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_ieee_vmminv8hf3                      (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_ieee_vmminv8hf3_mask                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_ieee_vmminv8hf3_round                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_ieee_vmminv8hf3_mask_round           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_ieee_vmmaxv4sf3                             (rtx, rtx, rtx);
extern rtx        gen_sse_ieee_vmmaxv4sf3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_ieee_vmmaxv4sf3_round                       (rtx, rtx, rtx, rtx);
extern rtx        gen_sse_ieee_vmmaxv4sf3_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_ieee_vmminv4sf3                             (rtx, rtx, rtx);
extern rtx        gen_sse_ieee_vmminv4sf3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_ieee_vmminv4sf3_round                       (rtx, rtx, rtx, rtx);
extern rtx        gen_sse_ieee_vmminv4sf3_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_ieee_vmmaxv2df3                            (rtx, rtx, rtx);
extern rtx        gen_sse2_ieee_vmmaxv2df3_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_ieee_vmmaxv2df3_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_ieee_vmmaxv2df3_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_ieee_vmminv2df3                            (rtx, rtx, rtx);
extern rtx        gen_sse2_ieee_vmminv2df3_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_ieee_vmminv2df3_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_ieee_vmminv2df3_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_addsubv8sf3                                 (rtx, rtx, rtx);
extern rtx        gen_vec_addsubv4sf3                                 (rtx, rtx, rtx);
extern rtx        gen_vec_addsubv4df3                                 (rtx, rtx, rtx);
extern rtx        gen_vec_addsubv2df3                                 (rtx, rtx, rtx);
extern rtx        gen_avx_haddv4df3                                   (rtx, rtx, rtx);
extern rtx        gen_avx_hsubv4df3                                   (rtx, rtx, rtx);
extern rtx        gen_sse3_hsubv2df3                                  (rtx, rtx, rtx);
extern rtx        gen_avx_haddv8sf3                                   (rtx, rtx, rtx);
extern rtx        gen_avx_hsubv8sf3                                   (rtx, rtx, rtx);
extern rtx        gen_sse3_haddv4sf3                                  (rtx, rtx, rtx);
extern rtx        gen_sse3_hsubv4sf3                                  (rtx, rtx, rtx);
extern rtx        gen_reducepv32hf_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv32hf_mask_round                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv16hf_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv16hf_mask_round                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv8hf_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv8hf_mask_round                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv16sf_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv16sf_mask_round                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv8sf_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv8sf_mask_round                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv4sf_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv4sf_mask_round                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv8df_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv8df_mask_round                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv4df_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv4df_mask_round                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv2df_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducepv2df_mask_round                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducesv8hf                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_reducesv8hf_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducesv8hf_round                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducesv8hf_mask_round                          (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducesv4sf                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_reducesv4sf_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducesv4sf_round                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducesv4sf_mask_round                          (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducesv2df                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_reducesv2df_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducesv2df_round                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_reducesv2df_mask_round                          (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_cmpv8sf3                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_cmpv4sf3                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_cmpv4df3                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_cmpv2df3                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vmcmpv4sf3                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vmcmpv2df3                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_maskcmpv8sf3                                (rtx, rtx, rtx, rtx);
extern rtx        gen_sse_maskcmpv4sf3                                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_maskcmpv4df3                                (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_maskcmpv2df3                               (rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmmaskcmpv4sf3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmmaskcmpv2df3                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cmpv16si3                               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cmpv16si3_round                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cmpv16si3_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cmpv16si3_mask_round                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_cmpv8si3                               (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv8si3_round                         (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv8si3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512vl_cmpv8si3_mask                          (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv8si3_mask_round                    (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv8si3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_cmpv4si3                               (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv4si3_round                         (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv4si3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512vl_cmpv4si3_mask                          (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv4si3_mask_round                    (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv4si3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_cmpv8di3                                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cmpv8di3_round                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cmpv8di3_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cmpv8di3_mask_round                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_cmpv4di3                               (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv4di3_round                         (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv4di3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512vl_cmpv4di3_mask                          (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv4di3_mask_round                    (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv4di3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_cmpv2di3                               (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv2di3_round                         (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv2di3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512vl_cmpv2di3_mask                          (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv2di3_mask_round                    (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv2di3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512bw_cmpv32hf3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_cmpv32hf3_round                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_cmpv32hf3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_cmpv32hf3_mask_round                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_cmpv16hf3                              (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv16hf3_round                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv16hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512vl_cmpv16hf3_mask                         (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv16hf3_mask_round                   (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv16hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512fp16_cmpv8hf3                             (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_cmpv8hf3_round                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_cmpv8hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512fp16_cmpv8hf3_mask                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_cmpv8hf3_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_cmpv8hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_cmpv16sf3                               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cmpv16sf3_round                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cmpv16sf3_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cmpv16sf3_mask_round                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_cmpv8sf3                               (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv8sf3_round                         (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv8sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512vl_cmpv8sf3_mask                          (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv8sf3_mask_round                    (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv8sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_cmpv4sf3                               (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv4sf3_round                         (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv4sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512vl_cmpv4sf3_mask                          (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv4sf3_mask_round                    (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv4sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_cmpv8df3                                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cmpv8df3_round                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cmpv8df3_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cmpv8df3_mask_round                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_cmpv4df3                               (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv4df3_round                         (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv4df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512vl_cmpv4df3_mask                          (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv4df3_mask_round                    (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv4df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_cmpv2df3                               (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv2df3_round                         (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv2df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512vl_cmpv2df3_mask                          (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_cmpv2df3_mask_round                    (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_cmpv2df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512bw_cmpv64qi3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_cmpv64qi3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_cmpv16qi3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_cmpv16qi3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_cmpv32qi3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_cmpv32qi3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_cmpv32hi3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_cmpv32hi3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_cmpv16hi3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_cmpv16hi3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_cmpv8hi3                               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_cmpv8hi3_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_ucmpv64qi3                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_ucmpv64qi3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv16qi3                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv16qi3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv32qi3                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv32qi3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_ucmpv32hi3                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_ucmpv32hi3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv16hi3                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv16hi3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv8hi3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv8hi3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_ucmpv16si3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_ucmpv16si3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv8si3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv8si3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv4si3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv4si3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_ucmpv8di3                               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_ucmpv8di3_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv4di3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv4di3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv2di3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ucmpv2di3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmcmpv8hf3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmcmpv8hf3_round                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmcmpv4sf3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmcmpv4sf3_round                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmcmpv2df3                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmcmpv2df3_round                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmcmpv8hf3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmcmpv8hf3_mask_round                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmcmpv4sf3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmcmpv4sf3_mask_round                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmcmpv2df3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmcmpv2df3_mask_round                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_comxhf                                  (rtx, rtx);
extern rtx        gen_avx10_2_comxhf_round                            (rtx, rtx, rtx);
extern rtx        gen_avx10_2_ucomxhf                                 (rtx, rtx);
extern rtx        gen_avx10_2_ucomxhf_round                           (rtx, rtx, rtx);
extern rtx        gen_avx10_2_comxsf                                  (rtx, rtx);
extern rtx        gen_avx10_2_comxsf_round                            (rtx, rtx, rtx);
extern rtx        gen_avx10_2_ucomxsf                                 (rtx, rtx);
extern rtx        gen_avx10_2_ucomxsf_round                           (rtx, rtx, rtx);
extern rtx        gen_avx10_2_comxdf                                  (rtx, rtx);
extern rtx        gen_avx10_2_comxdf_round                            (rtx, rtx, rtx);
extern rtx        gen_avx10_2_ucomxdf                                 (rtx, rtx);
extern rtx        gen_avx10_2_ucomxdf_round                           (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_comi                                 (rtx, rtx);
extern rtx        gen_avx512fp16_comi_round                           (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_ucomi                                (rtx, rtx);
extern rtx        gen_avx512fp16_ucomi_round                          (rtx, rtx, rtx);
extern rtx        gen_sse_comi                                        (rtx, rtx);
extern rtx        gen_sse_comi_round                                  (rtx, rtx, rtx);
extern rtx        gen_sse_ucomi                                       (rtx, rtx);
extern rtx        gen_sse_ucomi_round                                 (rtx, rtx, rtx);
extern rtx        gen_sse2_comi                                       (rtx, rtx);
extern rtx        gen_sse2_comi_round                                 (rtx, rtx, rtx);
extern rtx        gen_sse2_ucomi                                      (rtx, rtx);
extern rtx        gen_sse2_ucomi_round                                (rtx, rtx, rtx);
extern rtx        gen_avx10_2_comisbf16_v8bf                          (rtx, rtx);
extern rtx        gen_avx512bf16_andnotv16bf3                         (rtx, rtx, rtx);
static inline rtx gen_avx512bf16_andnotv16bf3_mask                    (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512bf16_andnotv16bf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512bf16_andnotv8bf3                          (rtx, rtx, rtx);
static inline rtx gen_avx512bf16_andnotv8bf3_mask                     (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512bf16_andnotv8bf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512fp16_andnotv16hf3                         (rtx, rtx, rtx);
static inline rtx gen_avx512fp16_andnotv16hf3_mask                    (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_andnotv16hf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512fp16_andnotv8hf3                          (rtx, rtx, rtx);
static inline rtx gen_avx512fp16_andnotv8hf3_mask                     (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_andnotv8hf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx_andnotv8sf3                                 (rtx, rtx, rtx);
extern rtx        gen_avx_andnotv8sf3_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_andnotv4sf3                                 (rtx, rtx, rtx);
extern rtx        gen_sse_andnotv4sf3_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_andnotv4df3                                 (rtx, rtx, rtx);
extern rtx        gen_avx_andnotv4df3_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_andnotv2df3                                (rtx, rtx, rtx);
extern rtx        gen_sse2_andnotv2df3_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bf16_andnotv32bf3                         (rtx, rtx, rtx);
static inline rtx gen_avx512bf16_andnotv32bf3_mask                    (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512bf16_andnotv32bf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512fp16_andnotv32hf3                         (rtx, rtx, rtx);
static inline rtx gen_avx512fp16_andnotv32hf3_mask                    (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_andnotv32hf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512f_andnotv16sf3                            (rtx, rtx, rtx);
extern rtx        gen_avx512f_andnotv16sf3_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_andnotv8df3                             (rtx, rtx, rtx);
extern rtx        gen_avx512f_andnotv8df3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_andbf3                                          (rtx, rtx, rtx);
extern rtx        gen_iorbf3                                          (rtx, rtx, rtx);
extern rtx        gen_xorbf3                                          (rtx, rtx, rtx);
extern rtx        gen_andhf3                                          (rtx, rtx, rtx);
extern rtx        gen_iorhf3                                          (rtx, rtx, rtx);
extern rtx        gen_xorhf3                                          (rtx, rtx, rtx);
extern rtx        gen_andsf3                                          (rtx, rtx, rtx);
extern rtx        gen_iorsf3                                          (rtx, rtx, rtx);
extern rtx        gen_xorsf3                                          (rtx, rtx, rtx);
extern rtx        gen_anddf3                                          (rtx, rtx, rtx);
extern rtx        gen_iordf3                                          (rtx, rtx, rtx);
extern rtx        gen_xordf3                                          (rtx, rtx, rtx);
extern rtx        gen_fma_fmadd_v32hf_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmadd_v32hf_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmadd_v16hf_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmadd_v16hf_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmadd_v16hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmadd_v8hf_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmadd_v8hf_maskz_1_round                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmadd_v8hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmadd_v16sf_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmadd_v16sf_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmadd_v8sf_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmadd_v8sf_maskz_1_round                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmadd_v8sf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmadd_v4sf_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmadd_v4sf_maskz_1_round                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmadd_v4sf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmadd_v8df_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmadd_v8df_maskz_1_round                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmadd_v4df_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmadd_v4df_maskz_1_round                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmadd_v4df_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmadd_v2df_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmadd_v2df_maskz_1_round                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmadd_v2df_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_avx512bw_fmadd_v32hf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmadd_v32hf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v16hf_mask                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fmadd_v16hf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fmadd_v16hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512fp16_fmadd_v8hf_mask                      (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_fmadd_v8hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_fmadd_v8hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_fmadd_v16sf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmadd_v16sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v8sf_mask                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fmadd_v8sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fmadd_v8sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_fmadd_v4sf_mask                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fmadd_v4sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fmadd_v4sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_fmadd_v8df_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmadd_v8df_mask_round                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v4df_mask                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fmadd_v4df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fmadd_v4df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_fmadd_v2df_mask                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fmadd_v2df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fmadd_v2df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512bw_fmadd_v32hf_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmadd_v32hf_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v16hf_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v16hf_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmadd_v8hf_mask3                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmadd_v8hf_mask3_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmadd_v16sf_mask3                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmadd_v16sf_mask3_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v8sf_mask3                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v8sf_mask3_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v4sf_mask3                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v4sf_mask3_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmadd_v8df_mask3                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmadd_v8df_mask3_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v4df_mask3                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v4df_mask3_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v2df_mask3                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v2df_mask3_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmsub_v32hf_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmsub_v32hf_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmsub_v16hf_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmsub_v16hf_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmsub_v16hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmsub_v8hf_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmsub_v8hf_maskz_1_round                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmsub_v8hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmsub_v16sf_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmsub_v16sf_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmsub_v8sf_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmsub_v8sf_maskz_1_round                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmsub_v8sf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmsub_v4sf_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmsub_v4sf_maskz_1_round                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmsub_v4sf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmsub_v8df_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmsub_v8df_maskz_1_round                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmsub_v4df_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmsub_v4df_maskz_1_round                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmsub_v4df_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmsub_v2df_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmsub_v2df_maskz_1_round                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmsub_v2df_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_avx512bw_fmsub_v32hf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmsub_v32hf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v16hf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v16hf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmsub_v8hf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmsub_v8hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsub_v16sf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsub_v16sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v8sf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v8sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v4sf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v4sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsub_v8df_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsub_v8df_mask_round                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v4df_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v4df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v2df_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v2df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmsub_v32hf_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmsub_v32hf_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v16hf_mask3                      (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fmsub_v16hf_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fmsub_v16hf_mask3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512fp16_fmsub_v8hf_mask3                     (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_fmsub_v8hf_mask3_round               (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_fmsub_v8hf_mask3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_fmsub_v16sf_mask3                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsub_v16sf_mask3_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v8sf_mask3                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fmsub_v8sf_mask3_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fmsub_v8sf_mask3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_fmsub_v4sf_mask3                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fmsub_v4sf_mask3_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fmsub_v4sf_mask3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_fmsub_v8df_mask3                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsub_v8df_mask3_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v4df_mask3                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fmsub_v4df_mask3_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fmsub_v4df_mask3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_fmsub_v2df_mask3                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fmsub_v2df_mask3_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fmsub_v2df_mask3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_fma_fnmadd_v32hf_maskz_1                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fnmadd_v32hf_maskz_1_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fnmadd_v16hf_maskz_1                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fnmadd_v16hf_maskz_1_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fnmadd_v16hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fnmadd_v8hf_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fnmadd_v8hf_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fnmadd_v8hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fnmadd_v16sf_maskz_1                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fnmadd_v16sf_maskz_1_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fnmadd_v8sf_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fnmadd_v8sf_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fnmadd_v8sf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fnmadd_v4sf_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fnmadd_v4sf_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fnmadd_v4sf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fnmadd_v8df_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fnmadd_v8df_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fnmadd_v4df_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fnmadd_v4df_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fnmadd_v4df_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fnmadd_v2df_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fnmadd_v2df_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fnmadd_v2df_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_avx512bw_fnmadd_v32hf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fnmadd_v32hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v16hf_mask                      (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmadd_v16hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmadd_v16hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512fp16_fnmadd_v8hf_mask                     (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_fnmadd_v8hf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_fnmadd_v8hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_fnmadd_v16sf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmadd_v16sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v8sf_mask                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmadd_v8sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmadd_v8sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_fnmadd_v4sf_mask                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmadd_v4sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmadd_v4sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_fnmadd_v8df_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmadd_v8df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v4df_mask                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmadd_v4df_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmadd_v4df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_fnmadd_v2df_mask                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmadd_v2df_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmadd_v2df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512bw_fnmadd_v32hf_mask3                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fnmadd_v32hf_mask3_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v16hf_mask3                     (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmadd_v16hf_mask3_round               (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmadd_v16hf_mask3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512fp16_fnmadd_v8hf_mask3                    (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_fnmadd_v8hf_mask3_round              (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_fnmadd_v8hf_mask3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_fnmadd_v16sf_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmadd_v16sf_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v8sf_mask3                      (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmadd_v8sf_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmadd_v8sf_mask3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_fnmadd_v4sf_mask3                      (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmadd_v4sf_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmadd_v4sf_mask3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_fnmadd_v8df_mask3                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmadd_v8df_mask3_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v4df_mask3                      (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmadd_v4df_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmadd_v4df_mask3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_fnmadd_v2df_mask3                      (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmadd_v2df_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmadd_v2df_mask3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_fma_fnmsub_v32hf_maskz_1                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fnmsub_v32hf_maskz_1_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fnmsub_v16hf_maskz_1                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fnmsub_v16hf_maskz_1_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fnmsub_v16hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fnmsub_v8hf_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fnmsub_v8hf_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fnmsub_v8hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fnmsub_v16sf_maskz_1                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fnmsub_v16sf_maskz_1_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fnmsub_v8sf_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fnmsub_v8sf_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fnmsub_v8sf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fnmsub_v4sf_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fnmsub_v4sf_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fnmsub_v4sf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fnmsub_v8df_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fnmsub_v8df_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fnmsub_v4df_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fnmsub_v4df_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fnmsub_v4df_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fnmsub_v2df_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fnmsub_v2df_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fnmsub_v2df_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_avx512bw_fnmsub_v32hf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fnmsub_v32hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v16hf_mask                      (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmsub_v16hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmsub_v16hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512fp16_fnmsub_v8hf_mask                     (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_fnmsub_v8hf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_fnmsub_v8hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_fnmsub_v16sf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmsub_v16sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v8sf_mask                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmsub_v8sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmsub_v8sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_fnmsub_v4sf_mask                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmsub_v4sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmsub_v4sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512f_fnmsub_v8df_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmsub_v8df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v4df_mask                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmsub_v4df_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmsub_v4df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_fnmsub_v2df_mask                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fnmsub_v2df_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fnmsub_v2df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512bw_fnmsub_v32hf_mask3                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fnmsub_v32hf_mask3_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v16hf_mask3                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v16hf_mask3_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fnmsub_v8hf_mask3                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fnmsub_v8hf_mask3_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmsub_v16sf_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmsub_v16sf_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v8sf_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v8sf_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v4sf_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v4sf_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmsub_v8df_mask3                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmsub_v8df_mask3_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v4df_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v4df_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v2df_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v2df_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddsub_v32hf_maskz_1                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddsub_v32hf_maskz_1_round                (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddsub_v16hf_maskz_1                      (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmaddsub_v16hf_maskz_1_round                (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmaddsub_v16hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmaddsub_v8hf_maskz_1                       (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmaddsub_v8hf_maskz_1_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmaddsub_v8hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmaddsub_v16sf_maskz_1                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddsub_v16sf_maskz_1_round                (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddsub_v8sf_maskz_1                       (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmaddsub_v8sf_maskz_1_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmaddsub_v8sf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmaddsub_v4sf_maskz_1                       (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmaddsub_v4sf_maskz_1_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmaddsub_v4sf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmaddsub_v8df_maskz_1                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddsub_v8df_maskz_1_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddsub_v4df_maskz_1                       (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmaddsub_v4df_maskz_1_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmaddsub_v4df_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmaddsub_v2df_maskz_1                       (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmaddsub_v2df_maskz_1_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmaddsub_v2df_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_avx512bw_fmaddsub_v32hf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmaddsub_v32hf_mask_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v16hf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v16hf_mask_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddsub_v8hf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddsub_v8hf_mask_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmaddsub_v16sf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmaddsub_v16sf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v8sf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v8sf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v4sf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v4sf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmaddsub_v8df_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmaddsub_v8df_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v4df_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v4df_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v2df_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v2df_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmaddsub_v32hf_mask3                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmaddsub_v32hf_mask3_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v16hf_mask3                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v16hf_mask3_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddsub_v8hf_mask3                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddsub_v8hf_mask3_round            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmaddsub_v16sf_mask3                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmaddsub_v16sf_mask3_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v8sf_mask3                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v8sf_mask3_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v4sf_mask3                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v4sf_mask3_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmaddsub_v8df_mask3                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmaddsub_v8df_mask3_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v4df_mask3                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v4df_mask3_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v2df_mask3                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v2df_mask3_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmsubadd_v32hf_maskz_1                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmsubadd_v32hf_maskz_1_round                (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmsubadd_v16hf_maskz_1                      (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmsubadd_v16hf_maskz_1_round                (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmsubadd_v16hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmsubadd_v8hf_maskz_1                       (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmsubadd_v8hf_maskz_1_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmsubadd_v8hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmsubadd_v16sf_maskz_1                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmsubadd_v16sf_maskz_1_round                (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmsubadd_v8sf_maskz_1                       (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmsubadd_v8sf_maskz_1_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmsubadd_v8sf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmsubadd_v4sf_maskz_1                       (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmsubadd_v4sf_maskz_1_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmsubadd_v4sf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmsubadd_v8df_maskz_1                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmsubadd_v8df_maskz_1_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmsubadd_v4df_maskz_1                       (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmsubadd_v4df_maskz_1_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmsubadd_v4df_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmsubadd_v2df_maskz_1                       (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmsubadd_v2df_maskz_1_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmsubadd_v2df_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_avx512bw_fmsubadd_v32hf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmsubadd_v32hf_mask_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v16hf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v16hf_mask_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmsubadd_v8hf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmsubadd_v8hf_mask_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsubadd_v16sf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsubadd_v16sf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v8sf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v8sf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v4sf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v4sf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsubadd_v8df_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsubadd_v8df_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v4df_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v4df_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v2df_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v2df_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmsubadd_v32hf_mask3                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmsubadd_v32hf_mask3_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v16hf_mask3                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v16hf_mask3_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmsubadd_v8hf_mask3                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmsubadd_v8hf_mask3_round            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsubadd_v16sf_mask3                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsubadd_v16sf_mask3_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v8sf_mask3                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v8sf_mask3_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v4sf_mask3                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v4sf_mask3_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsubadd_v8df_mask3                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsubadd_v8df_mask3_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v4df_mask3                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v4df_mask3_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v2df_mask3                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v2df_mask3_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v8hf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v8hf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v4sf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v4sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v2df_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v2df_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v8hf_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v8hf_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v4sf_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v4sf_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v2df_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v2df_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v8hf_maskz_1                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v8hf_maskz_1_round              (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v4sf_maskz_1                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v4sf_maskz_1_round              (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v2df_maskz_1                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v2df_maskz_1_round              (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmsub_v8hf_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmsub_v8hf_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmsub_v4sf_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmsub_v4sf_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmsub_v2df_mask3                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmsub_v2df_mask3_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v8hf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v8hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v4sf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v4sf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v2df_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v2df_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v8hf_mask3                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v8hf_mask3_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v4sf_mask3                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v4sf_mask3_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v2df_mask3                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v2df_mask3_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v8hf_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v8hf_maskz_1_round             (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v4sf_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v4sf_maskz_1_round             (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v2df_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v2df_maskz_1_round             (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddc_v32hf                                (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddc_v32hf_round                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddc_v32hf_maskz_1                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddc_v32hf_maskz_1_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fcmaddc_v32hf                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fcmaddc_v32hf_round                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fcmaddc_v32hf_maskz_1                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fcmaddc_v32hf_maskz_1_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddc_v16hf                                (rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmaddc_v16hf_round                          (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmaddc_v16hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_fma_fmaddc_v16hf_maskz_1                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmaddc_v16hf_maskz_1_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmaddc_v16hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fcmaddc_v16hf                               (rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fcmaddc_v16hf_round                         (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fcmaddc_v16hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_fma_fcmaddc_v16hf_maskz_1                       (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fcmaddc_v16hf_maskz_1_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fcmaddc_v16hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fmaddc_v8hf                                 (rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmaddc_v8hf_round                           (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmaddc_v8hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_fma_fmaddc_v8hf_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fmaddc_v8hf_maskz_1_round                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fmaddc_v8hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_fcmaddc_v8hf                                (rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fcmaddc_v8hf_round                          (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fcmaddc_v8hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_fma_fcmaddc_v8hf_maskz_1                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_fma_fcmaddc_v8hf_maskz_1_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fma_fcmaddc_v8hf_maskz_1_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_fma_v32hf_fadd_fmul                             (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_v16hf_fadd_fmul                             (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_v8hf_fadd_fmul                              (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_v32hf_fadd_fcmul                            (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_v16hf_fadd_fcmul                            (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_v8hf_fadd_fcmul                             (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddc_v32hf_fma_zero                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fcmaddc_v32hf_fma_zero                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddc_v16hf_fma_zero                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fcmaddc_v16hf_fma_zero                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddc_v8hf_fma_zero                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fcmaddc_v8hf_fma_zero                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddc_v16sf_pair                           (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fcmaddc_v16sf_pair                          (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddc_v8sf_pair                            (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fcmaddc_v8sf_pair                           (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fmaddc_v4sf_pair                            (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_fcmaddc_v4sf_pair                           (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_v32hf_fmaddc_bcst                           (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_v16hf_fmaddc_bcst                           (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_v8hf_fmaddc_bcst                            (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_v32hf_fcmaddc_bcst                          (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_v16hf_fcmaddc_bcst                          (rtx, rtx, rtx, rtx);
extern rtx        gen_fma_v8hf_fcmaddc_bcst                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmaddc_v32hf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmaddc_v32hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fcmaddc_v32hf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fcmaddc_v32hf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddc_v16hf_mask                      (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fmaddc_v16hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fmaddc_v16hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_fcmaddc_v16hf_mask                     (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fcmaddc_v16hf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fcmaddc_v16hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512fp16_fmaddc_v8hf_mask                     (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_fmaddc_v8hf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_fmaddc_v8hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512fp16_fcmaddc_v8hf_mask                    (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_fcmaddc_v8hf_mask_round              (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_fcmaddc_v8hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512bw_fmulc_v32hf                            (rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmulc_v32hf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmulc_v32hf_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmulc_v32hf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fcmulc_v32hf                           (rtx, rtx, rtx);
extern rtx        gen_avx512bw_fcmulc_v32hf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fcmulc_v32hf_round                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fcmulc_v32hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmulc_v16hf                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmulc_v16hf_mask                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fmulc_v16hf_round                      (rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fmulc_v16hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_avx512vl_fmulc_v16hf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fmulc_v16hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512vl_fcmulc_v16hf                           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_fcmulc_v16hf_mask                      (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512vl_fcmulc_v16hf_round                     (rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fcmulc_v16hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_avx512vl_fcmulc_v16hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512vl_fcmulc_v16hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512fp16_fmulc_v8hf                           (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmulc_v8hf_mask                      (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_fmulc_v8hf_round                     (rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_fmulc_v8hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_avx512fp16_fmulc_v8hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_fmulc_v8hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512fp16_fcmulc_v8hf                          (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmulc_v8hf_mask                     (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_fcmulc_v8hf_round                    (rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_fcmulc_v8hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_avx512fp16_fcmulc_v8hf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_fcmulc_v8hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512fp16_fma_fmaddcsh_v8hf                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fma_fmaddcsh_v8hf_maskz              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fma_fmaddcsh_v8hf_round              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fma_fmaddcsh_v8hf_maskz_round        (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fma_fcmaddcsh_v8hf                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fma_fcmaddcsh_v8hf_maskz             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fma_fcmaddcsh_v8hf_round             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fma_fcmaddcsh_v8hf_maskz_round       (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddcsh_v8hf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddcsh_v8hf_mask_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmaddcsh_v8hf_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmaddcsh_v8hf_mask_round            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmulcsh_v8hf                         (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmulcsh_v8hf_round                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmulcsh_v8hf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmulcsh_v8hf_mask_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmulcsh_v8hf                        (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmulcsh_v8hf_round                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmulcsh_v8hf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmulcsh_v8hf_mask_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uw_v32hi                      (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uw_v32hi_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uw_v32hi_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uw_v32hi_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2w_v32hi                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2w_v32hi_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2w_v32hi_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2w_v32hi_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2udq_v16si                     (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2udq_v16si_round               (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2udq_v16si_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2udq_v16si_mask_round          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2dq_v16si                      (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2dq_v16si_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2dq_v16si_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2dq_v16si_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uqq_v8di                      (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uqq_v8di_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uqq_v8di_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uqq_v8di_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2qq_v8di                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2qq_v8di_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2qq_v8di_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2qq_v8di_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uw_v16hi                      (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uw_v16hi_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uw_v16hi_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uw_v16hi_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2w_v16hi                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2w_v16hi_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2w_v16hi_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2w_v16hi_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2udq_v8si                      (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2udq_v8si_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2udq_v8si_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2udq_v8si_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2dq_v8si                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2dq_v8si_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2dq_v8si_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2dq_v8si_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uqq_v4di                      (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uqq_v4di_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uqq_v4di_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uqq_v4di_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2qq_v4di                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2qq_v4di_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2qq_v4di_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2qq_v4di_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uw_v8hi                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uw_v8hi_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uw_v8hi_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uw_v8hi_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2w_v8hi                        (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2w_v8hi_round                  (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2w_v8hi_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2w_v8hi_mask_round             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2udq_v4si                      (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2udq_v4si_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2udq_v4si_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2udq_v4si_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2dq_v4si                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2dq_v4si_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2dq_v4si_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2dq_v4si_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uqq_v2di                      (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uqq_v2di_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uqq_v2di_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2uqq_v2di_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2qq_v2di                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2qq_v2di_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2qq_v2di_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtph2qq_v2di_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtw2ph_v8hi                        (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtw2ph_v8hi_round                  (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtw2ph_v8hi_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtw2ph_v8hi_mask_round             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuw2ph_v8hi                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuw2ph_v8hi_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuw2ph_v8hi_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuw2ph_v8hi_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtw2ph_v16hi                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtw2ph_v16hi_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtw2ph_v16hi_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtw2ph_v16hi_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuw2ph_v16hi                      (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuw2ph_v16hi_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuw2ph_v16hi_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuw2ph_v16hi_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtw2ph_v32hi                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtw2ph_v32hi_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtw2ph_v32hi_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtw2ph_v32hi_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuw2ph_v32hi                      (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuw2ph_v32hi_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuw2ph_v32hi_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuw2ph_v32hi_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtdq2ph_v8si                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtdq2ph_v8si_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtdq2ph_v8si_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtdq2ph_v8si_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtudq2ph_v8si                      (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtudq2ph_v8si_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtudq2ph_v8si_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtudq2ph_v8si_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtdq2ph_v16si                      (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtdq2ph_v16si_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtdq2ph_v16si_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtdq2ph_v16si_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtudq2ph_v16si                     (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtudq2ph_v16si_round               (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtudq2ph_v16si_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtudq2ph_v16si_mask_round          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtqq2ph_v8di                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtqq2ph_v8di_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtqq2ph_v8di_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtqq2ph_v8di_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuqq2ph_v8di                      (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuqq2ph_v8di_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuqq2ph_v8di_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuqq2ph_v8di_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2usi                           (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2usi_round                     (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2si                            (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2si_round                      (rtx, rtx, rtx);
static inline rtx gen_avx512fp16_vcvtsh2usiq                          (rtx, rtx);
static inline rtx
gen_avx512fp16_vcvtsh2usiq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx512fp16_vcvtsh2usiq_round                    (rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_vcvtsh2usiq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_avx512fp16_vcvtsh2siq                           (rtx, rtx);
static inline rtx
gen_avx512fp16_vcvtsh2siq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx512fp16_vcvtsh2siq_round                     (rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_vcvtsh2siq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx512fp16_vcvtsh2usi_2                         (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2si_2                          (rtx, rtx);
static inline rtx gen_avx512fp16_vcvtsh2usiq_2                        (rtx, rtx);
static inline rtx
gen_avx512fp16_vcvtsh2usiq_2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx512fp16_vcvtsh2siq_2                         (rtx, rtx);
static inline rtx
gen_avx512fp16_vcvtsh2siq_2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_avx512fp16_vcvtsi2sh                            (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsi2sh_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtusi2sh                           (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtusi2sh_round                     (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512fp16_vcvtsi2shq                           (rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_vcvtsi2shq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_avx512fp16_vcvtsi2shq_round                     (rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_vcvtsi2shq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_avx512fp16_vcvtusi2shq                          (rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_vcvtusi2shq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_avx512fp16_vcvtusi2shq_round                    (rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_vcvtusi2shq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_unspec_avx512fp16_fix_truncv8hi2                (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv8hi2_round          (rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv8hi2_mask           (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv8hi2_mask_round     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv8hi2             (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv8hi2_round       (rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv8hi2_mask        (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv8hi2_mask_round  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv16hi2               (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv16hi2_round         (rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv16hi2_mask          (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv16hi2_mask_round    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv16hi2            (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv16hi2_round      (rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv16hi2_mask       (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv16hi2_mask_round (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv32hi2               (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv32hi2_round         (rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv32hi2_mask          (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv32hi2_mask_round    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv32hi2            (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv32hi2_round      (rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv32hi2_mask       (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv32hi2_mask_round (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv8si2                (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv8si2_round          (rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv8si2_mask           (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv8si2_mask_round     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv8si2             (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv8si2_round       (rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv8si2_mask        (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv8si2_mask_round  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv16si2               (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv16si2_round         (rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv16si2_mask          (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv16si2_mask_round    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv16si2            (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv16si2_round      (rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv16si2_mask       (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv16si2_mask_round (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv8di2                (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv8di2_round          (rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv8di2_mask           (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv8di2_mask_round     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv8di2             (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv8di2_round       (rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv8di2_mask        (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv8di2_mask_round  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv8hi2                       (rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv8hi2_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv8hi2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv8hi2_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv8hi2                    (rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv8hi2_round              (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv8hi2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv8hi2_mask_round         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv16hi2                      (rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv16hi2_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv16hi2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv16hi2_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv16hi2                   (rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv16hi2_round             (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv16hi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv16hi2_mask_round        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv32hi2                      (rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv32hi2_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv32hi2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv32hi2_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv32hi2                   (rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv32hi2_round             (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv32hi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv32hi2_mask_round        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv8si2                       (rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv8si2_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv8si2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv8si2_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv8si2                    (rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv8si2_round              (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv8si2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv8si2_mask_round         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv16si2                      (rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv16si2_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv16si2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv16si2_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv16si2                   (rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv16si2_round             (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv16si2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv16si2_mask_round        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv8di2                       (rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv8di2_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv8di2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv8di2_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv8di2                    (rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv8di2_round              (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv8di2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv8di2_mask_round         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv4si2                (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv4si2_mask           (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv4si2             (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv4si2_mask        (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv4di2                (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv4di2_mask           (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv4di2             (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv4di2_mask        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv4si2                       (rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv4si2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv4si2                    (rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv4si2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv4di2                       (rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv4di2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv4di2                    (rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv4di2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv2di2                (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncv2di2_mask           (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv2di2             (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncv2di2_mask        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv2di2                       (rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncv2di2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv2di2                    (rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncv2di2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncsi2                  (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fix_truncsi2_round            (rtx, rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncsi2               (rtx, rtx);
extern rtx        gen_unspec_avx512fp16_fixuns_truncsi2_round         (rtx, rtx, rtx);
static inline rtx gen_unspec_avx512fp16_fix_truncdi2                  (rtx, rtx);
static inline rtx
gen_unspec_avx512fp16_fix_truncdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_unspec_avx512fp16_fix_truncdi2_round            (rtx, rtx, rtx);
static inline rtx
gen_unspec_avx512fp16_fix_truncdi2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_unspec_avx512fp16_fixuns_truncdi2               (rtx, rtx);
static inline rtx
gen_unspec_avx512fp16_fixuns_truncdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_unspec_avx512fp16_fixuns_truncdi2_round         (rtx, rtx, rtx);
static inline rtx
gen_unspec_avx512fp16_fixuns_truncdi2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx512fp16_fix_truncsi2                         (rtx, rtx);
extern rtx        gen_avx512fp16_fix_truncsi2_round                   (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncsi2                      (rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncsi2_round                (rtx, rtx, rtx);
static inline rtx gen_avx512fp16_fix_truncdi2                         (rtx, rtx);
static inline rtx
gen_avx512fp16_fix_truncdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx512fp16_fix_truncdi2_round                   (rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_fix_truncdi2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_avx512fp16_fixuns_truncdi2                      (rtx, rtx);
static inline rtx
gen_avx512fp16_fixuns_truncdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx512fp16_fixuns_truncdi2_round                (rtx, rtx, rtx);
static inline rtx
gen_avx512fp16_fixuns_truncdi2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx512fp16_fix_truncsi2_mem                     (rtx, rtx);
extern rtx        gen_avx512fp16_fixuns_truncsi2_mem                  (rtx, rtx);
static inline rtx gen_avx512fp16_fix_truncdi2_mem                     (rtx, rtx);
static inline rtx
gen_avx512fp16_fix_truncdi2_mem(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx512fp16_fixuns_truncdi2_mem                  (rtx, rtx);
static inline rtx
gen_avx512fp16_fixuns_truncdi2_mem(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_avx512fp16_float_extend_phv8df2                 (rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv8df2_round           (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv8df2_mask            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv8df2_mask_round      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv16sf2                (rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv16sf2_round          (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv16sf2_mask           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv16sf2_mask_round     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv8sf2                 (rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv8sf2_round           (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv8sf2_mask            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv8sf2_mask_round      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv4df2                 (rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv4df2_mask            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv4sf2                 (rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv4sf2_mask            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv2df2                 (rtx, rtx);
extern rtx        gen_avx512fp16_float_extend_phv2df2_mask            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtpd2ph_v8df                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtpd2ph_v8df_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtpd2ph_v8df_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtpd2ph_v8df_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtps2ph_v16sf                      (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtps2ph_v16sf_round                (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtps2ph_v16sf_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtps2ph_v16sf_mask_round           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtps2ph_v8sf                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtps2ph_v8sf_round                 (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtps2ph_v8sf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtps2ph_v8sf_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2sd                            (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2sd_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2sd_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2sd_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2ss                            (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2ss_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2ss_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2ss_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2sd_mem                        (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2sd_mask_mem                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2ss_mem                        (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsh2ss_mask_mem                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsd2sh                            (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsd2sh_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsd2sh_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsd2sh_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtss2sh                            (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtss2sh_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtss2sh_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtss2sh_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtss2sh_mem                        (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtss2sh_mask_mem                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsd2sh_mem                        (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtsd2sh_mask_mem                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_cvtpi2ps                                    (rtx, rtx, rtx);
extern rtx        gen_sse_cvtps2pi                                    (rtx, rtx);
extern rtx        gen_unspec_sse_cvttps2pi                            (rtx, rtx);
extern rtx        gen_sse_cvttps2pi                                   (rtx, rtx);
extern rtx        gen_sse_cvtsi2ss                                    (rtx, rtx, rtx);
extern rtx        gen_sse_cvtsi2ss_round                              (rtx, rtx, rtx, rtx);
static inline rtx gen_sse_cvtsi2ssq                                   (rtx, rtx, rtx);
static inline rtx
gen_sse_cvtsi2ssq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_sse_cvtsi2ssq_round                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_sse_cvtsi2ssq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_sse_cvtss2si                                    (rtx, rtx);
extern rtx        gen_sse_cvtss2si_round                              (rtx, rtx, rtx);
static inline rtx gen_sse_cvtss2siq                                   (rtx, rtx);
static inline rtx
gen_sse_cvtss2siq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_sse_cvtss2siq_round                             (rtx, rtx, rtx);
static inline rtx
gen_sse_cvtss2siq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_sse_cvtss2si_2                                  (rtx, rtx);
static inline rtx gen_sse_cvtss2siq_2                                 (rtx, rtx);
static inline rtx
gen_sse_cvtss2siq_2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_unspec_sse_cvttss2si                            (rtx, rtx);
extern rtx        gen_unspec_sse_cvttss2si_round                      (rtx, rtx, rtx);
static inline rtx gen_unspec_sse_cvttss2siq                           (rtx, rtx);
static inline rtx
gen_unspec_sse_cvttss2siq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_unspec_sse_cvttss2siq_round                     (rtx, rtx, rtx);
static inline rtx
gen_unspec_sse_cvttss2siq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_sse_cvttss2si                                   (rtx, rtx);
extern rtx        gen_sse_cvttss2si_round                             (rtx, rtx, rtx);
static inline rtx gen_sse_cvttss2siq                                  (rtx, rtx);
static inline rtx
gen_sse_cvttss2siq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_sse_cvttss2siq_round                            (rtx, rtx, rtx);
static inline rtx
gen_sse_cvttss2siq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_cvtusi2ss32                                     (rtx, rtx, rtx);
extern rtx        gen_cvtusi2ss32_round                               (rtx, rtx, rtx, rtx);
extern rtx        gen_cvtusi2sd32                                     (rtx, rtx, rtx);
static inline rtx gen_cvtusi2sd32_round                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_cvtusi2sd32_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_cvtusi2ss64                                     (rtx, rtx, rtx);
static inline rtx
gen_cvtusi2ss64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_cvtusi2ss64_round                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_cvtusi2ss64_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_cvtusi2sd64                                     (rtx, rtx, rtx);
static inline rtx
gen_cvtusi2sd64(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_cvtusi2sd64_round                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_cvtusi2sd64_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_floatv16siv16sf2                                (rtx, rtx);
extern rtx        gen_floatv16siv16sf2_round                          (rtx, rtx, rtx);
extern rtx        gen_floatv16siv16sf2_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_floatv16siv16sf2_mask_round                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_floatv8siv8sf2                                  (rtx, rtx);
static inline rtx gen_floatv8siv8sf2_round                            (rtx, rtx, rtx);
static inline rtx
gen_floatv8siv8sf2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_floatv8siv8sf2_mask                             (rtx, rtx, rtx, rtx);
static inline rtx gen_floatv8siv8sf2_mask_round                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_floatv8siv8sf2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_floatv4siv4sf2                                  (rtx, rtx);
static inline rtx gen_floatv4siv4sf2_round                            (rtx, rtx, rtx);
static inline rtx
gen_floatv4siv4sf2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_floatv4siv4sf2_mask                             (rtx, rtx, rtx, rtx);
static inline rtx gen_floatv4siv4sf2_mask_round                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_floatv4siv4sf2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_floatunsv16siv16sf2_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv16siv16sf2_mask_round                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv8siv8sf2_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv8siv8sf2_mask_round                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv4siv4sf2_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv4siv4sf2_mask_round                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_fix_notruncv8sfv8si                         (rtx, rtx);
extern rtx        gen_avx_fix_notruncv8sfv8si_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_fix_notruncv4sfv4si                        (rtx, rtx);
extern rtx        gen_sse2_fix_notruncv4sfv4si_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fix_notruncv16sfv16si                   (rtx, rtx);
extern rtx        gen_avx512f_fix_notruncv16sfv16si_round             (rtx, rtx, rtx);
extern rtx        gen_avx512f_fix_notruncv16sfv16si_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fix_notruncv16sfv16si_mask_round        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixuns_notruncv16sfv16si_mask           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixuns_notruncv16sfv16si_mask_round     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixuns_notruncv8sfv8si_mask            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixuns_notruncv8sfv8si_mask_round      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixuns_notruncv4sfv4si_mask            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixuns_notruncv4sfv4si_mask_round      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_cvtps2qqv8di_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_cvtps2qqv8di_mask_round                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_cvtps2qqv4di_mask                      (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512dq_cvtps2qqv4di_mask_round                (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512dq_cvtps2qqv4di_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512dq_cvtps2qqv2di_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_cvtps2uqqv8di_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_cvtps2uqqv8di_mask_round               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_cvtps2uqqv4di_mask                     (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512dq_cvtps2uqqv4di_mask_round               (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512dq_cvtps2uqqv4di_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512dq_cvtps2uqqv2di_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv16sfv16si2                     (rtx, rtx);
extern rtx        gen_unspec_fix_truncv16sfv16si2_round               (rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv16sfv16si2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv16sfv16si2_mask_round          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv16sfv16si2                  (rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv16sfv16si2_round            (rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv16sfv16si2_mask             (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv16sfv16si2_mask_round       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fix_truncv16sfv16si2                            (rtx, rtx);
extern rtx        gen_fix_truncv16sfv16si2_round                      (rtx, rtx, rtx);
extern rtx        gen_fix_truncv16sfv16si2_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fix_truncv16sfv16si2_mask_round                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv16sfv16si2                         (rtx, rtx);
extern rtx        gen_fixuns_truncv16sfv16si2_round                   (rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv16sfv16si2_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv16sfv16si2_mask_round              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv8sfv8si2                       (rtx, rtx);
extern rtx        gen_unspec_fix_truncv8sfv8si2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_fix_truncv8sfv8si2                              (rtx, rtx);
extern rtx        gen_fix_truncv8sfv8si2_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv4sfv4si2                       (rtx, rtx);
extern rtx        gen_unspec_fix_truncv4sfv4si2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_fix_truncv4sfv4si2                              (rtx, rtx);
extern rtx        gen_fix_truncv4sfv4si2_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_cvtpi2pd                                   (rtx, rtx);
static inline rtx gen_floatunsv2siv2df2                               (rtx, rtx);
static inline rtx
gen_floatunsv2siv2df2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_sse2_cvtpd2pi                                   (rtx, rtx);
extern rtx        gen_unspec_sse2_cvttpd2pi                           (rtx, rtx);
extern rtx        gen_sse2_cvttpd2pi                                  (rtx, rtx);
static inline rtx gen_unspec_fixuns_truncv2dfv2si2                    (rtx, rtx);
static inline rtx
gen_unspec_fixuns_truncv2dfv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_fixuns_truncv2dfv2si2                           (rtx, rtx);
static inline rtx
gen_fixuns_truncv2dfv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_sse2_cvtsi2sd                                   (rtx, rtx, rtx);
static inline rtx gen_sse2_cvtsi2sdq                                  (rtx, rtx, rtx);
static inline rtx
gen_sse2_cvtsi2sdq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_sse2_cvtsi2sdq_round                            (rtx, rtx, rtx, rtx);
static inline rtx
gen_sse2_cvtsi2sdq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_avx512f_vcvtss2usi                              (rtx, rtx);
extern rtx        gen_avx512f_vcvtss2usi_round                        (rtx, rtx, rtx);
static inline rtx gen_avx512f_vcvtss2usiq                             (rtx, rtx);
static inline rtx
gen_avx512f_vcvtss2usiq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx512f_vcvtss2usiq_round                       (rtx, rtx, rtx);
static inline rtx
gen_avx512f_vcvtss2usiq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_unspec_avx512f_vcvttss2usi                      (rtx, rtx);
extern rtx        gen_unspec_avx512f_vcvttss2usi_round                (rtx, rtx, rtx);
static inline rtx gen_unspec_avx512f_vcvttss2usiq                     (rtx, rtx);
static inline rtx
gen_unspec_avx512f_vcvttss2usiq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_unspec_avx512f_vcvttss2usiq_round               (rtx, rtx, rtx);
static inline rtx
gen_unspec_avx512f_vcvttss2usiq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx512f_vcvttss2usi                             (rtx, rtx);
extern rtx        gen_avx512f_vcvttss2usi_round                       (rtx, rtx, rtx);
static inline rtx gen_avx512f_vcvttss2usiq                            (rtx, rtx);
static inline rtx
gen_avx512f_vcvttss2usiq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx512f_vcvttss2usiq_round                      (rtx, rtx, rtx);
static inline rtx
gen_avx512f_vcvttss2usiq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx512f_vcvtsd2usi                              (rtx, rtx);
extern rtx        gen_avx512f_vcvtsd2usi_round                        (rtx, rtx, rtx);
static inline rtx gen_avx512f_vcvtsd2usiq                             (rtx, rtx);
static inline rtx
gen_avx512f_vcvtsd2usiq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx512f_vcvtsd2usiq_round                       (rtx, rtx, rtx);
static inline rtx
gen_avx512f_vcvtsd2usiq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_unspec_avx512f_vcvttsd2usi                      (rtx, rtx);
extern rtx        gen_unspec_avx512f_vcvttsd2usi_round                (rtx, rtx, rtx);
static inline rtx gen_unspec_avx512f_vcvttsd2usiq                     (rtx, rtx);
static inline rtx
gen_unspec_avx512f_vcvttsd2usiq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_unspec_avx512f_vcvttsd2usiq_round               (rtx, rtx, rtx);
static inline rtx
gen_unspec_avx512f_vcvttsd2usiq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx512f_vcvttsd2usi                             (rtx, rtx);
extern rtx        gen_avx512f_vcvttsd2usi_round                       (rtx, rtx, rtx);
static inline rtx gen_avx512f_vcvttsd2usiq                            (rtx, rtx);
static inline rtx
gen_avx512f_vcvttsd2usiq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx512f_vcvttsd2usiq_round                      (rtx, rtx, rtx);
static inline rtx
gen_avx512f_vcvttsd2usiq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_sse2_cvtsd2si                                   (rtx, rtx);
extern rtx        gen_sse2_cvtsd2si_round                             (rtx, rtx, rtx);
static inline rtx gen_sse2_cvtsd2siq                                  (rtx, rtx);
static inline rtx
gen_sse2_cvtsd2siq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_sse2_cvtsd2siq_round                            (rtx, rtx, rtx);
static inline rtx
gen_sse2_cvtsd2siq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_sse2_cvtsd2si_2                                 (rtx, rtx);
static inline rtx gen_sse2_cvtsd2siq_2                                (rtx, rtx);
static inline rtx
gen_sse2_cvtsd2siq_2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_unspec_sse2_cvttsd2si                           (rtx, rtx);
extern rtx        gen_unspec_sse2_cvttsd2si_round                     (rtx, rtx, rtx);
static inline rtx gen_unspec_sse2_cvttsd2siq                          (rtx, rtx);
static inline rtx
gen_unspec_sse2_cvttsd2siq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_unspec_sse2_cvttsd2siq_round                    (rtx, rtx, rtx);
static inline rtx
gen_unspec_sse2_cvttsd2siq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_sse2_cvttsd2si                                  (rtx, rtx);
extern rtx        gen_sse2_cvttsd2si_round                            (rtx, rtx, rtx);
static inline rtx gen_sse2_cvttsd2siq                                 (rtx, rtx);
static inline rtx
gen_sse2_cvttsd2siq(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_sse2_cvttsd2siq_round                           (rtx, rtx, rtx);
static inline rtx
gen_sse2_cvttsd2siq_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_floatv8siv8df2                                  (rtx, rtx);
extern rtx        gen_floatv8siv8df2_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_floatv4siv4df2                                  (rtx, rtx);
extern rtx        gen_floatv4siv4df2_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_floatv8div8df2                                  (rtx, rtx);
extern rtx        gen_floatv8div8df2_round                            (rtx, rtx, rtx);
extern rtx        gen_floatv8div8df2_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_floatv8div8df2_mask_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv8div8df2                               (rtx, rtx);
extern rtx        gen_floatunsv8div8df2_round                         (rtx, rtx, rtx);
extern rtx        gen_floatunsv8div8df2_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv8div8df2_mask_round                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_floatv4div4df2                                  (rtx, rtx);
extern rtx        gen_floatv4div4df2_round                            (rtx, rtx, rtx);
extern rtx        gen_floatv4div4df2_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_floatv4div4df2_mask_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv4div4df2                               (rtx, rtx);
extern rtx        gen_floatunsv4div4df2_round                         (rtx, rtx, rtx);
extern rtx        gen_floatunsv4div4df2_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv4div4df2_mask_round                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_floatv2div2df2                                  (rtx, rtx);
extern rtx        gen_floatv2div2df2_round                            (rtx, rtx, rtx);
extern rtx        gen_floatv2div2df2_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_floatv2div2df2_mask_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv2div2df2                               (rtx, rtx);
extern rtx        gen_floatunsv2div2df2_round                         (rtx, rtx, rtx);
extern rtx        gen_floatunsv2div2df2_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv2div2df2_mask_round                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_floatv8div8sf2                                  (rtx, rtx);
extern rtx        gen_floatv8div8sf2_round                            (rtx, rtx, rtx);
extern rtx        gen_floatv8div8sf2_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_floatv8div8sf2_mask_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv8div8sf2                               (rtx, rtx);
extern rtx        gen_floatunsv8div8sf2_round                         (rtx, rtx, rtx);
extern rtx        gen_floatunsv8div8sf2_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv8div8sf2_mask_round                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_floatv4div4sf2                                  (rtx, rtx);
static inline rtx gen_floatv4div4sf2_round                            (rtx, rtx, rtx);
static inline rtx
gen_floatv4div4sf2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_floatv4div4sf2_mask                             (rtx, rtx, rtx, rtx);
static inline rtx gen_floatv4div4sf2_mask_round                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_floatv4div4sf2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_floatunsv4div4sf2                               (rtx, rtx);
static inline rtx gen_floatunsv4div4sf2_round                         (rtx, rtx, rtx);
static inline rtx
gen_floatunsv4div4sf2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_floatunsv4div4sf2_mask                          (rtx, rtx, rtx, rtx);
static inline rtx gen_floatunsv4div4sf2_mask_round                    (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_floatunsv4div4sf2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_floatunsv8siv8df2                               (rtx, rtx);
extern rtx        gen_floatunsv8siv8df2_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv4siv4df2                               (rtx, rtx);
extern rtx        gen_floatunsv4siv4df2_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv2siv2df2_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtdq2pd512_2                           (rtx, rtx);
extern rtx        gen_avx_cvtdq2pd256_2                               (rtx, rtx);
extern rtx        gen_sse2_cvtdq2pd                                   (rtx, rtx);
extern rtx        gen_sse2_cvtdq2pd_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtpd2dq512                             (rtx, rtx);
extern rtx        gen_avx512f_cvtpd2dq512_round                       (rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtpd2dq512_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtpd2dq512_mask_round                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_cvtpd2dq256                                 (rtx, rtx);
extern rtx        gen_avx_cvtpd2dq256_mask                            (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_cvtpd2dq                                   (rtx, rtx);
extern rtx        gen_sse2_cvtpd2dq_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_notruncv8dfv8si2                         (rtx, rtx);
extern rtx        gen_fixuns_notruncv8dfv8si2_round                   (rtx, rtx, rtx);
extern rtx        gen_fixuns_notruncv8dfv8si2_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_notruncv8dfv8si2_mask_round              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_notruncv4dfv4si2                         (rtx, rtx);
extern rtx        gen_fixuns_notruncv4dfv4si2_round                   (rtx, rtx, rtx);
extern rtx        gen_fixuns_notruncv4dfv4si2_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_notruncv4dfv4si2_mask_round              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_notruncv2dfv2si2                         (rtx, rtx);
extern rtx        gen_fixuns_notruncv2dfv2si2_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv8dfv8si2                       (rtx, rtx);
extern rtx        gen_unspec_fix_truncv8dfv8si2_round                 (rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv8dfv8si2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv8dfv8si2_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv8dfv8si2                    (rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv8dfv8si2_round              (rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv8dfv8si2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv8dfv8si2_mask_round         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fix_truncv8dfv8si2                              (rtx, rtx);
extern rtx        gen_fix_truncv8dfv8si2_round                        (rtx, rtx, rtx);
extern rtx        gen_fix_truncv8dfv8si2_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_fix_truncv8dfv8si2_mask_round                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv8dfv8si2                           (rtx, rtx);
extern rtx        gen_fixuns_truncv8dfv8si2_round                     (rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv8dfv8si2_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv8dfv8si2_mask_round                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv2dfv2si2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv2dfv2si2_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv4dfv4si2                       (rtx, rtx);
extern rtx        gen_unspec_fix_truncv4dfv4si2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_fix_truncv4dfv4si2                              (rtx, rtx);
extern rtx        gen_fix_truncv4dfv4si2_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv4dfv4si2                    (rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv4dfv4si2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv4dfv4si2                           (rtx, rtx);
extern rtx        gen_fixuns_truncv4dfv4si2_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv8dfv8di2                       (rtx, rtx);
extern rtx        gen_unspec_fix_truncv8dfv8di2_round                 (rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv8dfv8di2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv8dfv8di2_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv8dfv8di2                    (rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv8dfv8di2_round              (rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv8dfv8di2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv8dfv8di2_mask_round         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv4dfv4di2                       (rtx, rtx);
static inline rtx gen_unspec_fix_truncv4dfv4di2_round                 (rtx, rtx, rtx);
static inline rtx
gen_unspec_fix_truncv4dfv4di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_unspec_fix_truncv4dfv4di2_mask                  (rtx, rtx, rtx, rtx);
static inline rtx gen_unspec_fix_truncv4dfv4di2_mask_round            (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_unspec_fix_truncv4dfv4di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_unspec_fixuns_truncv4dfv4di2                    (rtx, rtx);
static inline rtx gen_unspec_fixuns_truncv4dfv4di2_round              (rtx, rtx, rtx);
static inline rtx
gen_unspec_fixuns_truncv4dfv4di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_unspec_fixuns_truncv4dfv4di2_mask               (rtx, rtx, rtx, rtx);
static inline rtx gen_unspec_fixuns_truncv4dfv4di2_mask_round         (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_unspec_fixuns_truncv4dfv4di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_unspec_fix_truncv2dfv2di2                       (rtx, rtx);
static inline rtx gen_unspec_fix_truncv2dfv2di2_round                 (rtx, rtx, rtx);
static inline rtx
gen_unspec_fix_truncv2dfv2di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_unspec_fix_truncv2dfv2di2_mask                  (rtx, rtx, rtx, rtx);
static inline rtx gen_unspec_fix_truncv2dfv2di2_mask_round            (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_unspec_fix_truncv2dfv2di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_unspec_fixuns_truncv2dfv2di2                    (rtx, rtx);
static inline rtx gen_unspec_fixuns_truncv2dfv2di2_round              (rtx, rtx, rtx);
static inline rtx
gen_unspec_fixuns_truncv2dfv2di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_unspec_fixuns_truncv2dfv2di2_mask               (rtx, rtx, rtx, rtx);
static inline rtx gen_unspec_fixuns_truncv2dfv2di2_mask_round         (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_unspec_fixuns_truncv2dfv2di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_fix_truncv8dfv8di2                              (rtx, rtx);
extern rtx        gen_fix_truncv8dfv8di2_round                        (rtx, rtx, rtx);
extern rtx        gen_fix_truncv8dfv8di2_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_fix_truncv8dfv8di2_mask_round                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv8dfv8di2                           (rtx, rtx);
extern rtx        gen_fixuns_truncv8dfv8di2_round                     (rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv8dfv8di2_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv8dfv8di2_mask_round                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fix_truncv4dfv4di2                              (rtx, rtx);
static inline rtx gen_fix_truncv4dfv4di2_round                        (rtx, rtx, rtx);
static inline rtx
gen_fix_truncv4dfv4di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_fix_truncv4dfv4di2_mask                         (rtx, rtx, rtx, rtx);
static inline rtx gen_fix_truncv4dfv4di2_mask_round                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fix_truncv4dfv4di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_fixuns_truncv4dfv4di2                           (rtx, rtx);
static inline rtx gen_fixuns_truncv4dfv4di2_round                     (rtx, rtx, rtx);
static inline rtx
gen_fixuns_truncv4dfv4di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_fixuns_truncv4dfv4di2_mask                      (rtx, rtx, rtx, rtx);
static inline rtx gen_fixuns_truncv4dfv4di2_mask_round                (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fixuns_truncv4dfv4di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_fix_truncv2dfv2di2                              (rtx, rtx);
static inline rtx gen_fix_truncv2dfv2di2_round                        (rtx, rtx, rtx);
static inline rtx
gen_fix_truncv2dfv2di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_fix_truncv2dfv2di2_mask                         (rtx, rtx, rtx, rtx);
static inline rtx gen_fix_truncv2dfv2di2_mask_round                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fix_truncv2dfv2di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_fixuns_truncv2dfv2di2                           (rtx, rtx);
static inline rtx gen_fixuns_truncv2dfv2di2_round                     (rtx, rtx, rtx);
static inline rtx
gen_fixuns_truncv2dfv2di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_fixuns_truncv2dfv2di2_mask                      (rtx, rtx, rtx, rtx);
static inline rtx gen_fixuns_truncv2dfv2di2_mask_round                (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fixuns_truncv2dfv2di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_fix_notruncv8dfv8di2                            (rtx, rtx);
extern rtx        gen_fix_notruncv8dfv8di2_round                      (rtx, rtx, rtx);
extern rtx        gen_fix_notruncv8dfv8di2_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fix_notruncv8dfv8di2_mask_round                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fix_notruncv4dfv4di2                            (rtx, rtx);
static inline rtx gen_fix_notruncv4dfv4di2_round                      (rtx, rtx, rtx);
static inline rtx
gen_fix_notruncv4dfv4di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_fix_notruncv4dfv4di2_mask                       (rtx, rtx, rtx, rtx);
static inline rtx gen_fix_notruncv4dfv4di2_mask_round                 (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fix_notruncv4dfv4di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_fix_notruncv2dfv2di2                            (rtx, rtx);
static inline rtx gen_fix_notruncv2dfv2di2_round                      (rtx, rtx, rtx);
static inline rtx
gen_fix_notruncv2dfv2di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_fix_notruncv2dfv2di2_mask                       (rtx, rtx, rtx, rtx);
static inline rtx gen_fix_notruncv2dfv2di2_mask_round                 (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fix_notruncv2dfv2di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_fixuns_notruncv8dfv8di2                         (rtx, rtx);
extern rtx        gen_fixuns_notruncv8dfv8di2_round                   (rtx, rtx, rtx);
extern rtx        gen_fixuns_notruncv8dfv8di2_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_notruncv8dfv8di2_mask_round              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_notruncv4dfv4di2                         (rtx, rtx);
static inline rtx gen_fixuns_notruncv4dfv4di2_round                   (rtx, rtx, rtx);
static inline rtx
gen_fixuns_notruncv4dfv4di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_fixuns_notruncv4dfv4di2_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_fixuns_notruncv4dfv4di2_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fixuns_notruncv4dfv4di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_fixuns_notruncv2dfv2di2                         (rtx, rtx);
static inline rtx gen_fixuns_notruncv2dfv2di2_round                   (rtx, rtx, rtx);
static inline rtx
gen_fixuns_notruncv2dfv2di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_fixuns_notruncv2dfv2di2_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_fixuns_notruncv2dfv2di2_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fixuns_notruncv2dfv2di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_unspec_fix_truncv8sfv8di2                       (rtx, rtx);
extern rtx        gen_unspec_fix_truncv8sfv8di2_round                 (rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv8sfv8di2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv8sfv8di2_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv8sfv8di2                    (rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv8sfv8di2_round              (rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv8sfv8di2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv8sfv8di2_mask_round         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fix_truncv4sfv4di2                       (rtx, rtx);
static inline rtx gen_unspec_fix_truncv4sfv4di2_round                 (rtx, rtx, rtx);
static inline rtx
gen_unspec_fix_truncv4sfv4di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_unspec_fix_truncv4sfv4di2_mask                  (rtx, rtx, rtx, rtx);
static inline rtx gen_unspec_fix_truncv4sfv4di2_mask_round            (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_unspec_fix_truncv4sfv4di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_unspec_fixuns_truncv4sfv4di2                    (rtx, rtx);
static inline rtx gen_unspec_fixuns_truncv4sfv4di2_round              (rtx, rtx, rtx);
static inline rtx
gen_unspec_fixuns_truncv4sfv4di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_unspec_fixuns_truncv4sfv4di2_mask               (rtx, rtx, rtx, rtx);
static inline rtx gen_unspec_fixuns_truncv4sfv4di2_mask_round         (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_unspec_fixuns_truncv4sfv4di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_fix_truncv8sfv8di2                              (rtx, rtx);
extern rtx        gen_fix_truncv8sfv8di2_round                        (rtx, rtx, rtx);
extern rtx        gen_fix_truncv8sfv8di2_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_fix_truncv8sfv8di2_mask_round                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv8sfv8di2                           (rtx, rtx);
extern rtx        gen_fixuns_truncv8sfv8di2_round                     (rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv8sfv8di2_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv8sfv8di2_mask_round                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fix_truncv4sfv4di2                              (rtx, rtx);
static inline rtx gen_fix_truncv4sfv4di2_round                        (rtx, rtx, rtx);
static inline rtx
gen_fix_truncv4sfv4di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_fix_truncv4sfv4di2_mask                         (rtx, rtx, rtx, rtx);
static inline rtx gen_fix_truncv4sfv4di2_mask_round                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fix_truncv4sfv4di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_fixuns_truncv4sfv4di2                           (rtx, rtx);
static inline rtx gen_fixuns_truncv4sfv4di2_round                     (rtx, rtx, rtx);
static inline rtx
gen_fixuns_truncv4sfv4di2_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_fixuns_truncv4sfv4di2_mask                      (rtx, rtx, rtx, rtx);
static inline rtx gen_fixuns_truncv4sfv4di2_mask_round                (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_fixuns_truncv4sfv4di2_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_unspec_avx512dq_fix_truncv2sfv2di2              (rtx, rtx);
extern rtx        gen_unspec_avx512dq_fix_truncv2sfv2di2_mask         (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_avx512dq_fixuns_truncv2sfv2di2           (rtx, rtx);
extern rtx        gen_unspec_avx512dq_fixuns_truncv2sfv2di2_mask      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_fix_truncv2sfv2di2                     (rtx, rtx);
extern rtx        gen_avx512dq_fix_truncv2sfv2di2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_fixuns_truncv2sfv2di2                  (rtx, rtx);
extern rtx        gen_avx512dq_fixuns_truncv2sfv2di2_mask             (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv8sfv8si2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_fixuns_truncv4sfv4si2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv8sfv8si2_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_fixuns_truncv4sfv4si2_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_unspec_sse2_cvttpd2dq                           (rtx, rtx);
extern rtx        gen_unspec_sse2_cvttpd2dq_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_cvttpd2dq                                  (rtx, rtx);
extern rtx        gen_sse2_cvttpd2dq_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_cvtsd2ss                                   (rtx, rtx, rtx);
extern rtx        gen_sse2_cvtsd2ss_round                             (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_cvtsd2ss_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_cvtsd2ss_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_cvtss2sd                                   (rtx, rtx, rtx);
extern rtx        gen_sse2_cvtss2sd_round                             (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_cvtss2sd_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_cvtss2sd_mask_round                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtpd2ps512_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtpd2ps512_mask_round                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_cvtpd2ps256                                 (rtx, rtx);
extern rtx        gen_avx_cvtpd2ps256_mask                            (rtx, rtx, rtx, rtx);
static inline rtx gen_truncv2dfv2sf2                                  (rtx, rtx);
static inline rtx
gen_truncv2dfv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_avx512f_cvtps2pd512                             (rtx, rtx);
extern rtx        gen_avx512f_cvtps2pd512_round                       (rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtps2pd512_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtps2pd512_mask_round                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_cvtps2pd256                                 (rtx, rtx);
static inline rtx gen_avx_cvtps2pd256_round                           (rtx, rtx, rtx);
static inline rtx
gen_avx_cvtps2pd256_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx_cvtps2pd256_mask                            (rtx, rtx, rtx, rtx);
static inline rtx gen_avx_cvtps2pd256_mask_round                      (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx_cvtps2pd256_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_vec_unpacks_lo_v16sf                            (rtx, rtx);
extern rtx        gen_avx512bw_cvtb2maskv64qi                         (rtx, rtx);
extern rtx        gen_avx512vl_cvtb2maskv16qi                         (rtx, rtx);
extern rtx        gen_avx512vl_cvtb2maskv32qi                         (rtx, rtx);
extern rtx        gen_avx512bw_cvtw2maskv32hi                         (rtx, rtx);
extern rtx        gen_avx512vl_cvtw2maskv16hi                         (rtx, rtx);
extern rtx        gen_avx512vl_cvtw2maskv8hi                          (rtx, rtx);
extern rtx        gen_avx512f_cvtd2maskv16si                          (rtx, rtx);
extern rtx        gen_avx512vl_cvtd2maskv8si                          (rtx, rtx);
extern rtx        gen_avx512vl_cvtd2maskv4si                          (rtx, rtx);
extern rtx        gen_avx512f_cvtq2maskv8di                           (rtx, rtx);
extern rtx        gen_avx512vl_cvtq2maskv4di                          (rtx, rtx);
extern rtx        gen_avx512vl_cvtq2maskv2di                          (rtx, rtx);
extern rtx        gen_sse2_cvtps2pd                                   (rtx, rtx);
extern rtx        gen_sse2_cvtps2pd_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_cvtps2pd_1                                 (rtx, rtx);
extern rtx        gen_sse2_cvtps2pd_mask_1                            (rtx, rtx, rtx, rtx);
extern rtx        gen_sse_movhlps                                     (rtx, rtx, rtx);
extern rtx        gen_sse_movlhps                                     (rtx, rtx, rtx);
extern rtx        gen_sse_movlhps_v8hi                                (rtx, rtx, rtx);
extern rtx        gen_sse_movlhps_v8hf                                (rtx, rtx, rtx);
extern rtx        gen_sse_movlhps_v8bf                                (rtx, rtx, rtx);
extern rtx        gen_avx512f_unpckhps512_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_unpckhps256                                 (rtx, rtx, rtx);
extern rtx        gen_avx_unpckhps256_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv4sf                         (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv4sf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_unpcklps512_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_unpcklps256                                 (rtx, rtx, rtx);
extern rtx        gen_avx_unpcklps256_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_unpcklps128_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv4sf                          (rtx, rtx, rtx);
extern rtx        gen_avx_movshdup256                                 (rtx, rtx);
extern rtx        gen_avx_movshdup256_mask                            (rtx, rtx, rtx, rtx);
extern rtx        gen_sse3_movshdup                                   (rtx, rtx);
extern rtx        gen_sse3_movshdup_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_movshdup512_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_movsldup256                                 (rtx, rtx);
extern rtx        gen_avx_movsldup256_mask                            (rtx, rtx, rtx, rtx);
extern rtx        gen_sse3_movsldup                                   (rtx, rtx);
extern rtx        gen_sse3_movsldup_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_movsldup512_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_shufps256_1                                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_shufps256_1_mask                            (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_shufps_v4sf_mask                            (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_shufps_v4si                                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_shufps_v4sf                                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_storehps                                    (rtx, rtx);
extern rtx        gen_sse_loadhps                                     (rtx, rtx, rtx);
extern rtx        gen_sse_storelps                                    (rtx, rtx);
extern rtx        gen_sse_storelps_unalign                            (rtx, rtx);
extern rtx        gen_sse_loadlps                                     (rtx, rtx, rtx);
extern rtx        gen_sse_movss_v4si                                  (rtx, rtx, rtx);
extern rtx        gen_sse_movss_v4sf                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_vec_dupv8sf                                (rtx, rtx);
extern rtx        gen_avx2_vec_dupv4sf                                (rtx, rtx);
extern rtx        gen_avx2_vec_dupv8sf_1                              (rtx, rtx);
extern rtx        gen_avx512f_vec_dupv16sf_1                          (rtx, rtx);
extern rtx        gen_avx512f_vec_dupv8df_1                           (rtx, rtx);
extern rtx        gen_vec_setv4si_0                                   (rtx, rtx, rtx);
extern rtx        gen_vec_setv4sf_0                                   (rtx, rtx, rtx);
extern rtx        gen_vec_setv8hi_0                                   (rtx, rtx, rtx);
extern rtx        gen_vec_setv8hf_0                                   (rtx, rtx, rtx);
extern rtx        gen_vec_setv8bf_0                                   (rtx, rtx, rtx);
extern rtx        gen_vec_setv16hi_0                                  (rtx, rtx, rtx);
extern rtx        gen_vec_setv32hi_0                                  (rtx, rtx, rtx);
extern rtx        gen_vec_setv16hf_0                                  (rtx, rtx, rtx);
extern rtx        gen_vec_setv32hf_0                                  (rtx, rtx, rtx);
extern rtx        gen_vec_setv16bf_0                                  (rtx, rtx, rtx);
extern rtx        gen_vec_setv32bf_0                                  (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_movv8hi                              (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_movv8hf                              (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_movv8bf                              (rtx, rtx, rtx);
extern rtx        gen_vec_setv8si_0                                   (rtx, rtx, rtx);
extern rtx        gen_vec_setv8sf_0                                   (rtx, rtx, rtx);
extern rtx        gen_vec_setv16si_0                                  (rtx, rtx, rtx);
extern rtx        gen_vec_setv16sf_0                                  (rtx, rtx, rtx);
extern rtx        gen_sse4_1_insertps_v4si                            (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_insertps_v4sf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_setv2df_0                                   (rtx, rtx, rtx);
extern rtx        gen_avx512dq_vextractf64x2_1_mask                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_vextracti64x2_1_mask                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vextractf32x4_1_mask                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vextracti32x4_1_mask                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_lo_v8df_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_lo_v8di_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_lo_v8df                             (rtx, rtx);
extern rtx        gen_vec_extract_lo_v8di                             (rtx, rtx);
extern rtx        gen_vec_extract_hi_v8df_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_hi_v8di_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_hi_v8df                             (rtx, rtx);
extern rtx        gen_vec_extract_hi_v8di                             (rtx, rtx);
extern rtx        gen_vec_extract_hi_v16sf_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_hi_v16si_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_hi_v16sf                            (rtx, rtx);
extern rtx        gen_vec_extract_hi_v16si                            (rtx, rtx);
extern rtx        gen_vec_extract_lo_v16sf_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_lo_v16si_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_lo_v16sf                            (rtx, rtx);
extern rtx        gen_vec_extract_lo_v16si                            (rtx, rtx);
extern rtx        gen_vec_extract_lo_v4di_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_lo_v4df_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_lo_v4di                             (rtx, rtx);
extern rtx        gen_vec_extract_lo_v4df                             (rtx, rtx);
extern rtx        gen_vec_extract_hi_v4di_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_hi_v4df_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_hi_v4di                             (rtx, rtx);
extern rtx        gen_vec_extract_hi_v4df                             (rtx, rtx);
extern rtx        gen_vec_extract_lo_v8si_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_lo_v8sf_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_lo_v8si                             (rtx, rtx);
extern rtx        gen_vec_extract_lo_v8sf                             (rtx, rtx);
extern rtx        gen_vec_extract_hi_v8si_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_hi_v8sf_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_extract_hi_v8si                             (rtx, rtx);
extern rtx        gen_vec_extract_hi_v8sf                             (rtx, rtx);
extern rtx        gen_vec_extract_lo_v32hi                            (rtx, rtx);
extern rtx        gen_vec_extract_lo_v32hf                            (rtx, rtx);
extern rtx        gen_vec_extract_lo_v32bf                            (rtx, rtx);
extern rtx        gen_vec_extract_hi_v32hi                            (rtx, rtx);
extern rtx        gen_vec_extract_hi_v32hf                            (rtx, rtx);
extern rtx        gen_vec_extract_hi_v32bf                            (rtx, rtx);
extern rtx        gen_vec_extract_lo_v16hi                            (rtx, rtx);
extern rtx        gen_vec_extract_lo_v16hf                            (rtx, rtx);
extern rtx        gen_vec_extract_lo_v16bf                            (rtx, rtx);
extern rtx        gen_vec_extract_hi_v16hi                            (rtx, rtx);
extern rtx        gen_vec_extract_hi_v16hf                            (rtx, rtx);
extern rtx        gen_vec_extract_hi_v16bf                            (rtx, rtx);
extern rtx        gen_vec_extract_lo_v64qi                            (rtx, rtx);
extern rtx        gen_vec_extract_hi_v64qi                            (rtx, rtx);
extern rtx        gen_vec_extract_lo_v32qi                            (rtx, rtx);
extern rtx        gen_vec_extract_hi_v32qi                            (rtx, rtx);
extern rtx        gen_avx512f_unpckhpd512_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_unpckhpd256                                 (rtx, rtx, rtx);
extern rtx        gen_avx_unpckhpd256_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_unpckhpd128_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_movddup512                              (rtx, rtx);
extern rtx        gen_avx512f_movddup512_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_unpcklpd512                             (rtx, rtx, rtx);
extern rtx        gen_avx512f_unpcklpd512_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_movddup256                                  (rtx, rtx);
extern rtx        gen_avx_movddup256_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_unpcklpd256                                 (rtx, rtx, rtx);
extern rtx        gen_avx_unpcklpd256_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_unpcklpd128_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmscalefv8hf                            (rtx, rtx, rtx);
extern rtx        gen_avx512f_vmscalefv8hf_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmscalefv8hf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmscalefv8hf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmscalefv4sf                            (rtx, rtx, rtx);
extern rtx        gen_avx512f_vmscalefv4sf_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmscalefv4sf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmscalefv4sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmscalefv2df                            (rtx, rtx, rtx);
extern rtx        gen_avx512f_vmscalefv2df_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmscalefv2df_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmscalefv2df_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_scalefv32hf                            (rtx, rtx, rtx);
extern rtx        gen_avx512bw_scalefv32hf_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_scalefv32hf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_scalefv32hf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv16hf                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv16hf_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv16hf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv16hf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_scalefv8hf                           (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_scalefv8hf_round                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_scalefv8hf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_scalefv8hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scalefv16sf                             (rtx, rtx, rtx);
extern rtx        gen_avx512f_scalefv16sf_round                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scalefv16sf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scalefv16sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv8sf                             (rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv8sf_round                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv8sf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv8sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv4sf                             (rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv4sf_round                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv4sf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv4sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scalefv8df                              (rtx, rtx, rtx);
extern rtx        gen_avx512f_scalefv8df_round                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scalefv8df_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scalefv8df_mask_round                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv4df                             (rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv4df_round                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv4df_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv4df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv2df                             (rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv2df_round                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv2df_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scalefv2df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vternlogv16si                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vternlogv16si_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv8si                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv8si_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv4si                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv4si_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vternlogv8di                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vternlogv8di_maskz_1                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv4di                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv4di_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv2di                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv2di_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_getexpv32hf                            (rtx, rtx);
extern rtx        gen_avx512bw_getexpv32hf_round                      (rtx, rtx, rtx);
extern rtx        gen_avx512bw_getexpv32hf_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_getexpv32hf_mask_round                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv16hf                            (rtx, rtx);
extern rtx        gen_avx512vl_getexpv16hf_round                      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv16hf_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv16hf_mask_round                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_getexpv8hf                           (rtx, rtx);
extern rtx        gen_avx512fp16_getexpv8hf_round                     (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_getexpv8hf_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_getexpv8hf_mask_round                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_getexpv16sf                             (rtx, rtx);
extern rtx        gen_avx512f_getexpv16sf_round                       (rtx, rtx, rtx);
extern rtx        gen_avx512f_getexpv16sf_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_getexpv16sf_mask_round                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv8sf                             (rtx, rtx);
extern rtx        gen_avx512vl_getexpv8sf_round                       (rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv8sf_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv8sf_mask_round                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv4sf                             (rtx, rtx);
extern rtx        gen_avx512vl_getexpv4sf_round                       (rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv4sf_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv4sf_mask_round                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_getexpv8df                              (rtx, rtx);
extern rtx        gen_avx512f_getexpv8df_round                        (rtx, rtx, rtx);
extern rtx        gen_avx512f_getexpv8df_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_getexpv8df_mask_round                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv4df                             (rtx, rtx);
extern rtx        gen_avx512vl_getexpv4df_round                       (rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv4df_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv4df_mask_round                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv2df                             (rtx, rtx);
extern rtx        gen_avx512vl_getexpv2df_round                       (rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv2df_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getexpv2df_mask_round                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sgetexpv8hf                             (rtx, rtx, rtx);
extern rtx        gen_avx512f_sgetexpv8hf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sgetexpv8hf_round                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sgetexpv8hf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sgetexpv4sf                             (rtx, rtx, rtx);
extern rtx        gen_avx512f_sgetexpv4sf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sgetexpv4sf_round                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sgetexpv4sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sgetexpv2df                             (rtx, rtx, rtx);
extern rtx        gen_avx512f_sgetexpv2df_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sgetexpv2df_round                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sgetexpv2df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_alignv16si_mask                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_alignv8si_mask                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_alignv4si_mask                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_alignv8di_mask                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_alignv4di_mask                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_alignv2di_mask                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv16sf                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv16sf_round                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv16sf_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv16sf_maskz_1_round             (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv8sf                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv8sf_round                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv8sf_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv8sf_maskz_1_round             (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4sf                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4sf_round                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4sf_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4sf_maskz_1_round             (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv8df                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv8df_round                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv8df_maskz_1                    (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv8df_maskz_1_round              (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4df                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4df_round                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4df_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4df_maskz_1_round             (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv2df                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv2df_round                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv2df_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv2df_maskz_1_round             (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv16sf_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv16sf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv8sf_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv8sf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4sf_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4sf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv8df_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv8df_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4df_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4df_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv2df_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv2df_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv4sf                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv4sf_round                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv4sf_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv4sf_maskz_1_round             (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv2df                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv2df_round                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv2df_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv2df_maskz_1_round             (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv4sf_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv4sf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv2df_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv2df_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_rndscalev32hf                          (rtx, rtx, rtx);
extern rtx        gen_avx512bw_rndscalev32hf_round                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_rndscalev32hf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_rndscalev32hf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev16hf                          (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev16hf_round                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev16hf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev16hf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_rndscalev8hf                         (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_rndscalev8hf_round                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_rndscalev8hf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_rndscalev8hf_mask_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev16sf                           (rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev16sf_round                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev16sf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev16sf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev8sf                           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev8sf_round                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev8sf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev8sf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev4sf                           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev4sf_round                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev4sf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev4sf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev8df                            (rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev8df_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev8df_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev8df_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev4df                           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev4df_round                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev4df_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev4df_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev2df                           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev2df_round                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev2df_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rndscalev2df_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev8hf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev8hf_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev8hf_round                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev8hf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev4sf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev4sf_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev4sf_round                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev4sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev2df                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev2df_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev2df_round                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rndscalev2df_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shufps512_1                             (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shufps512_1_mask                        (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shufpd512_1                             (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shufpd512_1_mask                        (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_shufpd256_1                                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_shufpd256_1_mask                            (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_shufpd_v2df_mask                           (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_highv4di                        (rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_highv4di_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_interleave_highv8di_mask                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv2di                         (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv2di_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_lowv4di                         (rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_lowv4di_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_interleave_lowv8di_mask                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv2di                          (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv2di_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_shufpd_v2di                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_shufpd_v2df                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_storehpd                                   (rtx, rtx);
extern rtx        gen_sse2_storelpd                                   (rtx, rtx);
extern rtx        gen_sse2_loadhpd                                    (rtx, rtx, rtx);
extern rtx        gen_sse2_loadlpd                                    (rtx, rtx, rtx);
extern rtx        gen_sse2_movsd_v2di                                 (rtx, rtx, rtx);
extern rtx        gen_sse2_movsd_v2df                                 (rtx, rtx, rtx);
extern rtx        gen_vec_dupv2df                                     (rtx, rtx);
extern rtx        gen_vec_dupv2df_mask                                (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_concatv2df                                  (rtx, rtx, rtx);
extern rtx        gen_vec_setv8df_0                                   (rtx, rtx, rtx);
extern rtx        gen_vec_setv4df_0                                   (rtx, rtx, rtx);
extern rtx        gen_avx512f_ss_truncatev16siv16qi2_mask             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_truncatev16siv16qi2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_us_truncatev16siv16qi2_mask             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_ss_truncatev16siv16hi2_mask             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_truncatev16siv16hi2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_us_truncatev16siv16hi2_mask             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_ss_truncatev8div8si2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_truncatev8div8si2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_us_truncatev8div8si2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_ss_truncatev8div8hi2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_truncatev8div8hi2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_us_truncatev8div8hi2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_ss_truncatev32hiv32qi2                 (rtx, rtx);
extern rtx        gen_avx512bw_truncatev32hiv32qi2                    (rtx, rtx);
extern rtx        gen_avx512bw_us_truncatev32hiv32qi2                 (rtx, rtx);
extern rtx        gen_avx512bw_ss_truncatev32hiv32qi2_mask            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_truncatev32hiv32qi2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_us_truncatev32hiv32qi2_mask            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4div4si2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4div4si2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4div4si2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev8siv8hi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev8siv8hi2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev8siv8hi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev16hiv16qi2_mask            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev16hiv16qi2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev16hiv16qi2_mask            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4div4qi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4div4qi2                      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4div4qi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev2div2qi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev2div2qi2                      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev2div2qi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev8siv8qi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev8siv8qi2                      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev8siv8qi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4siv4qi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4siv4qi2                      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4siv4qi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev8hiv8qi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev8hiv8qi2                      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev8hiv8qi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev2div2qi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev2div2qi2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev2div2qi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev2div2qi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev2div2qi2_mask_store_1         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev2div2qi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4siv4qi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4siv4qi2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4siv4qi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4div4qi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4div4qi2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4div4qi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4siv4qi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4siv4qi2_mask_store_1         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4siv4qi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4div4qi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4div4qi2_mask_store_1         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4div4qi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev8hiv8qi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev8hiv8qi2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev8hiv8qi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev8siv8qi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev8siv8qi2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev8siv8qi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev8hiv8qi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev8hiv8qi2_mask_store_1         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev8hiv8qi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev8siv8qi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev8siv8qi2_mask_store_1         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev8siv8qi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4div4hi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4div4hi2                      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4div4hi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev2div2hi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev2div2hi2                      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev2div2hi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4siv4hi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4siv4hi2                      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4siv4hi2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4siv4hi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4siv4hi2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4siv4hi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4div4hi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4div4hi2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4div4hi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4siv4hi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4siv4hi2_mask_store_1         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4siv4hi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4div4hi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4div4hi2_mask_store_1         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4div4hi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev2div2hi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev2div2hi2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev2div2hi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev2div2hi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev2div2hi2_mask_store_1         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev2div2hi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev2div2si2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev2div2si2                      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev2div2si2                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev2div2si2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev2div2si2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev2div2si2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev2div2si2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev2div2si2_mask_store_1         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev2div2si2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512f_ss_truncatev8div16qi2                   (rtx, rtx);
extern rtx        gen_avx512f_truncatev8div16qi2                      (rtx, rtx);
extern rtx        gen_avx512f_us_truncatev8div16qi2                   (rtx, rtx);
extern rtx        gen_avx512f_ss_truncatev8div16qi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_truncatev8div16qi2_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_us_truncatev8div16qi2_mask              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_ss_truncatev8div16qi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512f_truncatev8div16qi2_mask_store_1         (rtx, rtx, rtx);
extern rtx        gen_avx512f_us_truncatev8div16qi2_mask_store_1      (rtx, rtx, rtx);
extern rtx        gen_avx512bw_pmaddwd512v32hi                        (rtx, rtx, rtx);
extern rtx        gen_avx512bw_pmaddwd512v32hi_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_pmaddwd512v16hi                        (rtx, rtx, rtx);
extern rtx        gen_avx512bw_pmaddwd512v16hi_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_pmaddwd512v8hi                         (rtx, rtx, rtx);
extern rtx        gen_avx512bw_pmaddwd512v8hi_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashrv16hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashrv8hi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashrv8si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashrv4si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashrv2di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashrv16hi3                                      (rtx, rtx, rtx);
extern rtx        gen_ashrv8hi3                                       (rtx, rtx, rtx);
extern rtx        gen_ashrv8si3                                       (rtx, rtx, rtx);
extern rtx        gen_ashrv4si3                                       (rtx, rtx, rtx);
extern rtx        gen_ashrv32hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashrv4di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashrv16si3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashrv8di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashlv16hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_lshrv16hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashlv8hi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_lshrv8hi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashlv8si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_lshrv8si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashlv4si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_lshrv4si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashlv4di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_lshrv4di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashlv2di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_lshrv2di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashlv16hi3                                      (rtx, rtx, rtx);
extern rtx        gen_lshrv16hi3                                      (rtx, rtx, rtx);
extern rtx        gen_ashlv8hi3                                       (rtx, rtx, rtx);
extern rtx        gen_lshrv8hi3                                       (rtx, rtx, rtx);
extern rtx        gen_ashlv8si3                                       (rtx, rtx, rtx);
extern rtx        gen_lshrv8si3                                       (rtx, rtx, rtx);
extern rtx        gen_ashlv4si3                                       (rtx, rtx, rtx);
extern rtx        gen_lshrv4si3                                       (rtx, rtx, rtx);
extern rtx        gen_ashlv4di3                                       (rtx, rtx, rtx);
extern rtx        gen_lshrv4di3                                       (rtx, rtx, rtx);
extern rtx        gen_ashlv2di3                                       (rtx, rtx, rtx);
extern rtx        gen_lshrv2di3                                       (rtx, rtx, rtx);
extern rtx        gen_ashlv32hi3                                      (rtx, rtx, rtx);
extern rtx        gen_ashlv32hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_lshrv32hi3                                      (rtx, rtx, rtx);
extern rtx        gen_lshrv32hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashlv16si3                                      (rtx, rtx, rtx);
extern rtx        gen_ashlv16si3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_lshrv16si3                                      (rtx, rtx, rtx);
extern rtx        gen_lshrv16si3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ashlv8di3                                       (rtx, rtx, rtx);
extern rtx        gen_ashlv8di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_lshrv8di3                                       (rtx, rtx, rtx);
extern rtx        gen_lshrv8di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_ashlv4ti3                              (rtx, rtx, rtx);
extern rtx        gen_avx512bw_lshrv4ti3                              (rtx, rtx, rtx);
extern rtx        gen_avx512bw_ashlv2ti3                              (rtx, rtx, rtx);
extern rtx        gen_avx512bw_lshrv2ti3                              (rtx, rtx, rtx);
extern rtx        gen_avx512bw_ashlv1ti3                              (rtx, rtx, rtx);
extern rtx        gen_avx512bw_lshrv1ti3                              (rtx, rtx, rtx);
extern rtx        gen_avx2_ashlv2ti3                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_lshrv2ti3                                  (rtx, rtx, rtx);
extern rtx        gen_sse2_ashlv1ti3                                  (rtx, rtx, rtx);
extern rtx        gen_sse2_lshrv1ti3                                  (rtx, rtx, rtx);
extern rtx        gen_avx512f_rolvv16si                               (rtx, rtx, rtx);
extern rtx        gen_avx512f_rolvv16si_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rorvv16si                               (rtx, rtx, rtx);
extern rtx        gen_avx512f_rorvv16si_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolvv8si                               (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolvv8si_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorvv8si                               (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorvv8si_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolvv4si                               (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolvv4si_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorvv4si                               (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorvv4si_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rolvv8di                                (rtx, rtx, rtx);
extern rtx        gen_avx512f_rolvv8di_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rorvv8di                                (rtx, rtx, rtx);
extern rtx        gen_avx512f_rorvv8di_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolvv4di                               (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolvv4di_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorvv4di                               (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorvv4di_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolvv2di                               (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolvv2di_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorvv2di                               (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorvv2di_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rolv16si                                (rtx, rtx, rtx);
extern rtx        gen_avx512f_rolv16si_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rorv16si                                (rtx, rtx, rtx);
extern rtx        gen_avx512f_rorv16si_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolv8si                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolv8si_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorv8si                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorv8si_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolv4si                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolv4si_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorv4si                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorv4si_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rolv8di                                 (rtx, rtx, rtx);
extern rtx        gen_avx512f_rolv8di_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_rorv8di                                 (rtx, rtx, rtx);
extern rtx        gen_avx512f_rorv8di_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolv4di                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolv4di_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorv4di                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorv4di_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolv2di                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rolv2di_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorv2di                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_rorv2di_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_2_gtv2di3                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_gtv32qi3                                   (rtx, rtx, rtx);
extern rtx        gen_avx2_gtv16hi3                                   (rtx, rtx, rtx);
extern rtx        gen_avx2_gtv8si3                                    (rtx, rtx, rtx);
extern rtx        gen_avx2_gtv4di3                                    (rtx, rtx, rtx);
extern rtx        gen_one_cmplv16si2_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_one_cmplv8di2_mask                              (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_one_cmplv64qi2_mask                             (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_one_cmplv64qi2_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
static inline rtx gen_one_cmplv32qi2_mask                             (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_one_cmplv32qi2_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
static inline rtx gen_one_cmplv16qi2_mask                             (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_one_cmplv16qi2_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
static inline rtx gen_one_cmplv32hi2_mask                             (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_one_cmplv32hi2_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
static inline rtx gen_one_cmplv16hi2_mask                             (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_one_cmplv16hi2_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
static inline rtx gen_one_cmplv8hi2_mask                              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_one_cmplv8hi2_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_one_cmplv8si2_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_one_cmplv4si2_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_one_cmplv4di2_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_one_cmplv2di2_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_andv1ti3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv1ti3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv1ti3                                        (rtx, rtx, rtx);
extern rtx        gen_avx512bw_testmv64qi3                            (rtx, rtx, rtx);
extern rtx        gen_avx512bw_testmv64qi3_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv32qi3                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv32qi3_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv16qi3                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv16qi3_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_testmv32hi3                            (rtx, rtx, rtx);
extern rtx        gen_avx512bw_testmv32hi3_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv16hi3                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv16hi3_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv8hi3                             (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv8hi3_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_testmv16si3                             (rtx, rtx, rtx);
extern rtx        gen_avx512f_testmv16si3_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv8si3                             (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv8si3_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv4si3                             (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv4si3_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_testmv8di3                              (rtx, rtx, rtx);
extern rtx        gen_avx512f_testmv8di3_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv4di3                             (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv4di3_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv2di3                             (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testmv2di3_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_testnmv64qi3                           (rtx, rtx, rtx);
extern rtx        gen_avx512bw_testnmv64qi3_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv32qi3                           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv32qi3_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv16qi3                           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv16qi3_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_testnmv32hi3                           (rtx, rtx, rtx);
extern rtx        gen_avx512bw_testnmv32hi3_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv16hi3                           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv16hi3_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv8hi3                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv8hi3_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_testnmv16si3                            (rtx, rtx, rtx);
extern rtx        gen_avx512f_testnmv16si3_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv8si3                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv8si3_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv4si3                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv4si3_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_testnmv8di3                             (rtx, rtx, rtx);
extern rtx        gen_avx512f_testnmv8di3_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv4di3                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv4di3_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv2di3                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_testnmv2di3_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_packsswb                                   (rtx, rtx, rtx);
extern rtx        gen_sse2_packsswb_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_packsswb                                   (rtx, rtx, rtx);
extern rtx        gen_avx2_packsswb_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_packsswb                               (rtx, rtx, rtx);
extern rtx        gen_avx512bw_packsswb_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_packssdw                                   (rtx, rtx, rtx);
extern rtx        gen_sse2_packssdw_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_packssdw                                   (rtx, rtx, rtx);
extern rtx        gen_avx2_packssdw_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_packssdw                               (rtx, rtx, rtx);
extern rtx        gen_avx512bw_packssdw_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_packuswb                               (rtx, rtx, rtx);
extern rtx        gen_avx512bw_packuswb_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_packuswb                                   (rtx, rtx, rtx);
extern rtx        gen_avx2_packuswb_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_packuswb                                   (rtx, rtx, rtx);
extern rtx        gen_sse2_packuswb_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_interleave_highv64qi                   (rtx, rtx, rtx);
extern rtx        gen_avx512bw_interleave_highv64qi_mask              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_highv32qi                       (rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_highv32qi_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv16qi                        (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv16qi_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_interleave_lowv64qi                    (rtx, rtx, rtx);
extern rtx        gen_avx512bw_interleave_lowv64qi_mask               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_lowv32qi                        (rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_lowv32qi_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv16qi                         (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv16qi_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_interleave_highv32hi                   (rtx, rtx, rtx);
extern rtx        gen_avx512bw_interleave_highv32hi_mask              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_interleave_highv32hf                   (rtx, rtx, rtx);
extern rtx        gen_avx512bw_interleave_highv32hf_mask              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_interleave_highv32bf                   (rtx, rtx, rtx);
extern rtx        gen_avx512bw_interleave_highv32bf_mask              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_highv16hi                       (rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_highv16hi_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_highv16hf                       (rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_highv16hf_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_highv16bf                       (rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_highv16bf_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv8hi                         (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv8hi_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv8hf                         (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv8hf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv8bf                         (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv8bf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_interleave_lowv32hi_mask               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_interleave_lowv32hf_mask               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_interleave_lowv32bf_mask               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_lowv16hi                        (rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_lowv16hi_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_lowv16hf                        (rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_lowv16hf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_lowv16bf                        (rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_lowv16bf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv8hi                          (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv8hi_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv8hf                          (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv8hf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv8bf                          (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv8bf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_highv8si                        (rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_highv8si_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_interleave_highv16si_mask               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv4si                         (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv4si_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_lowv8si                         (rtx, rtx, rtx);
extern rtx        gen_avx2_interleave_lowv8si_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_interleave_lowv16si_mask                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv4si                          (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv4si_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_pinsrb                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_pinsrw                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_pinsrph                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_pinsrbf                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_pinsrd                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_pinsrq                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_vinsertf64x2_1_mask                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_vinserti64x2_1_mask                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vinsertf32x4_1_mask                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vinserti32x4_1_mask                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v16sf                                (rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v16sf_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v16si                                (rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v16si_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v16sf                                (rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v16sf_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v16si                                (rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v16si_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v8df                                 (rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v8df_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v8di                                 (rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v8di_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v8df                                 (rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v8df_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v8di                                 (rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v8di_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_shuf_i64x2_1_mask                      (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_shuf_f64x2_1_mask                      (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shuf_f64x2_1                            (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shuf_f64x2_1_mask                       (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shuf_i64x2_1                            (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shuf_i64x2_1_mask                       (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_shuf_i32x4_1                           (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_shuf_i32x4_1_mask                      (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_shuf_f32x4_1                           (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_shuf_f32x4_1_mask                      (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shuf_f32x4_1                            (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shuf_f32x4_1_mask                       (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shuf_i32x4_1                            (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shuf_i32x4_1_mask                       (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_pshufd_1                                (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_pshufd_1_mask                           (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pshufd_1                                   (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pshufd_1_mask                              (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_pshufd_1                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_pshufd_1_mask                              (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_pshuflwv32hi_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pshuflw_1                                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pshuflw_1_mask                             (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_pshuflw_1                                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_pshuflw_1_mask                             (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_pshufhwv32hi_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pshufhw_1                                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pshufhw_1_mask                             (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_pshufhw_1                                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_pshufhw_1_mask                             (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_loadld                                     (rtx, rtx, rtx);
extern rtx        gen_vec_concatv2di                                  (rtx, rtx, rtx);
extern rtx        gen_vec_setv8di_0                                   (rtx, rtx, rtx);
extern rtx        gen_vec_setv4di_0                                   (rtx, rtx, rtx);
extern rtx        gen_avx_movmskps256                                 (rtx, rtx);
extern rtx        gen_sse_movmskps                                    (rtx, rtx);
extern rtx        gen_avx_movmskpd256                                 (rtx, rtx);
extern rtx        gen_sse2_movmskpd                                   (rtx, rtx);
extern rtx        gen_avx2_pmovmskb                                   (rtx, rtx);
extern rtx        gen_sse2_pmovmskb                                   (rtx, rtx);
extern rtx        gen_sse_ldmxcsr                                     (rtx);
extern rtx        gen_sse_stmxcsr                                     (rtx);
extern rtx        gen_sse2_clflush                                    (rtx);
extern rtx        gen_sse3_mwait                                      (rtx, rtx);
extern rtx        gen_sse3_monitor_si                                 (rtx, rtx, rtx);
extern rtx        gen_sse3_monitor_di                                 (rtx, rtx, rtx);
extern rtx        gen_avx2_phaddwv16hi3                               (rtx, rtx, rtx);
extern rtx        gen_avx2_phaddswv16hi3                              (rtx, rtx, rtx);
extern rtx        gen_avx2_phsubwv16hi3                               (rtx, rtx, rtx);
extern rtx        gen_avx2_phsubswv16hi3                              (rtx, rtx, rtx);
extern rtx        gen_ssse3_phaddwv8hi3                               (rtx, rtx, rtx);
extern rtx        gen_ssse3_phaddswv8hi3                              (rtx, rtx, rtx);
extern rtx        gen_ssse3_phsubwv8hi3                               (rtx, rtx, rtx);
extern rtx        gen_ssse3_phsubswv8hi3                              (rtx, rtx, rtx);
extern rtx        gen_ssse3_phaddwv4hi3                               (rtx, rtx, rtx);
extern rtx        gen_ssse3_phaddswv4hi3                              (rtx, rtx, rtx);
extern rtx        gen_ssse3_phsubwv4hi3                               (rtx, rtx, rtx);
extern rtx        gen_ssse3_phsubswv4hi3                              (rtx, rtx, rtx);
extern rtx        gen_avx2_phadddv8si3                                (rtx, rtx, rtx);
extern rtx        gen_avx2_phsubdv8si3                                (rtx, rtx, rtx);
extern rtx        gen_ssse3_phadddv4si3                               (rtx, rtx, rtx);
extern rtx        gen_ssse3_phsubdv4si3                               (rtx, rtx, rtx);
extern rtx        gen_ssse3_phadddv2si3                               (rtx, rtx, rtx);
extern rtx        gen_ssse3_phsubdv2si3                               (rtx, rtx, rtx);
extern rtx        gen_avx2_pmaddubsw256                               (rtx, rtx, rtx);
extern rtx        gen_avx512bw_pmaddubsw512v8hi                       (rtx, rtx, rtx);
extern rtx        gen_avx512bw_pmaddubsw512v8hi_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_pmaddubsw512v16hi                      (rtx, rtx, rtx);
extern rtx        gen_avx512bw_pmaddubsw512v16hi_mask                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_pmaddubsw512v32hi                      (rtx, rtx, rtx);
extern rtx        gen_avx512bw_pmaddubsw512v32hi_mask                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_umulhrswv32hi3                         (rtx, rtx, rtx);
extern rtx        gen_avx512bw_umulhrswv32hi3_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ssse3_pmaddubsw128                              (rtx, rtx, rtx);
extern rtx        gen_ssse3_pmaddubsw                                 (rtx, rtx, rtx);
extern rtx        gen_avx512bw_pshufbv64qi3                           (rtx, rtx, rtx);
extern rtx        gen_avx512bw_pshufbv64qi3_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pshufbv32qi3                               (rtx, rtx, rtx);
extern rtx        gen_avx2_pshufbv32qi3_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ssse3_pshufbv16qi3                              (rtx, rtx, rtx);
extern rtx        gen_ssse3_pshufbv16qi3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_psignv32qi3                                (rtx, rtx, rtx);
extern rtx        gen_ssse3_psignv16qi3                               (rtx, rtx, rtx);
extern rtx        gen_avx2_psignv16hi3                                (rtx, rtx, rtx);
extern rtx        gen_ssse3_psignv8hi3                                (rtx, rtx, rtx);
extern rtx        gen_avx2_psignv8si3                                 (rtx, rtx, rtx);
extern rtx        gen_ssse3_psignv4si3                                (rtx, rtx, rtx);
extern rtx        gen_ssse3_psignv8qi3                                (rtx, rtx, rtx);
extern rtx        gen_ssse3_psignv4hi3                                (rtx, rtx, rtx);
extern rtx        gen_ssse3_psignv2si3                                (rtx, rtx, rtx);
extern rtx        gen_avx512bw_palignrv64qi_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_palignrv32qi_mask                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ssse3_palignrv16qi_mask                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_palignrv4ti                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_palignrv2ti                                (rtx, rtx, rtx, rtx);
extern rtx        gen_ssse3_palignrv1ti                               (rtx, rtx, rtx, rtx);
extern rtx        gen_ssse3_palignrdi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_absv16si2_mask                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_absv8si2_mask                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_absv4si2_mask                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_absv8di2_mask                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_absv4di2_mask                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_absv2di2_mask                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_absv64qi2_mask                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_absv16qi2_mask                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_absv32qi2_mask                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_absv32hi2_mask                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_absv16hi2_mask                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_absv8hi2_mask                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4a_movntsf                                   (rtx, rtx);
extern rtx        gen_sse4a_movntdf                                   (rtx, rtx);
extern rtx        gen_sse4a_vmmovntv4sf                               (rtx, rtx);
extern rtx        gen_sse4a_vmmovntv2df                               (rtx, rtx);
extern rtx        gen_sse4a_extrqi                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4a_extrq                                     (rtx, rtx, rtx);
extern rtx        gen_sse4a_insertqi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse4a_insertq                                   (rtx, rtx, rtx);
extern rtx        gen_avx_blendps256                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_blendps                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_blendpd256                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_blendpd                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_blendvps256                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_blendvps                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_blendvpd256                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_blendvpd                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_blendvss                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_blendvsd                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_dpps256                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_dpps                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_dppd256                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_dppd                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_movntdqa                                (rtx, rtx);
extern rtx        gen_avx2_movntdqa                                   (rtx, rtx);
extern rtx        gen_sse4_1_movntdqa                                 (rtx, rtx);
extern rtx        gen_avx2_mpsadbw                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_mpsadbw                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_mpsadbw                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_mpsadbw_mask                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_mpsadbw_mask                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_mpsadbw_mask                             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_packusdw                               (rtx, rtx, rtx);
extern rtx        gen_avx512bw_packusdw_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_packusdw                                   (rtx, rtx, rtx);
extern rtx        gen_avx2_packusdw_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_packusdw                                 (rtx, rtx, rtx);
extern rtx        gen_sse4_1_packusdw_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pblendvb                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_pblendvb                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_pblendw                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_pblendph                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_pblendbf                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pblenddv8si                                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pblenddv4si                                (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_phminposuw                               (rtx, rtx);
extern rtx        gen_avx2_sign_extendv16qiv16hi2                     (rtx, rtx);
extern rtx        gen_avx2_sign_extendv16qiv16hi2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_zero_extendv16qiv16hi2                     (rtx, rtx);
extern rtx        gen_avx2_zero_extendv16qiv16hi2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_sign_extendv32qiv32hi2                 (rtx, rtx);
extern rtx        gen_avx512bw_sign_extendv32qiv32hi2_mask            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_zero_extendv32qiv32hi2                 (rtx, rtx);
extern rtx        gen_avx512bw_zero_extendv32qiv32hi2_mask            (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_sign_extendv8qiv8hi2                     (rtx, rtx);
extern rtx        gen_sse4_1_sign_extendv8qiv8hi2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_zero_extendv8qiv8hi2                     (rtx, rtx);
extern rtx        gen_sse4_1_zero_extendv8qiv8hi2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sign_extendv16qiv16si2_mask             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_zero_extendv16qiv16si2_mask             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_sign_extendv8qiv8si2                       (rtx, rtx);
extern rtx        gen_avx2_sign_extendv8qiv8si2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_zero_extendv8qiv8si2                       (rtx, rtx);
extern rtx        gen_avx2_zero_extendv8qiv8si2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_sign_extendv4qiv4si2                     (rtx, rtx);
extern rtx        gen_sse4_1_sign_extendv4qiv4si2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_zero_extendv4qiv4si2                     (rtx, rtx);
extern rtx        gen_sse4_1_zero_extendv4qiv4si2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sign_extendv16hiv16si2                  (rtx, rtx);
extern rtx        gen_avx512f_sign_extendv16hiv16si2_mask             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_zero_extendv16hiv16si2                  (rtx, rtx);
extern rtx        gen_avx512f_zero_extendv16hiv16si2_mask             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_zero_extendv16hiv16si2_1                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_sign_extendv8hiv8si2                       (rtx, rtx);
extern rtx        gen_avx2_sign_extendv8hiv8si2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_zero_extendv8hiv8si2                       (rtx, rtx);
extern rtx        gen_avx2_zero_extendv8hiv8si2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_zero_extendv8hiv8si2_1                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_sign_extendv4hiv4si2                     (rtx, rtx);
extern rtx        gen_sse4_1_sign_extendv4hiv4si2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_zero_extendv4hiv4si2                     (rtx, rtx);
extern rtx        gen_sse4_1_zero_extendv4hiv4si2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sign_extendv8qiv8di2                    (rtx, rtx);
extern rtx        gen_avx512f_sign_extendv8qiv8di2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_zero_extendv8qiv8di2                    (rtx, rtx);
extern rtx        gen_avx512f_zero_extendv8qiv8di2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_sign_extendv4qiv4di2                       (rtx, rtx);
extern rtx        gen_avx2_sign_extendv4qiv4di2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_zero_extendv4qiv4di2                       (rtx, rtx);
extern rtx        gen_avx2_zero_extendv4qiv4di2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_sign_extendv2qiv2di2                     (rtx, rtx);
extern rtx        gen_sse4_1_sign_extendv2qiv2di2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_zero_extendv2qiv2di2                     (rtx, rtx);
extern rtx        gen_sse4_1_zero_extendv2qiv2di2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sign_extendv8hiv8di2                    (rtx, rtx);
extern rtx        gen_avx512f_sign_extendv8hiv8di2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_zero_extendv8hiv8di2                    (rtx, rtx);
extern rtx        gen_avx512f_zero_extendv8hiv8di2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_sign_extendv4hiv4di2                       (rtx, rtx);
extern rtx        gen_avx2_sign_extendv4hiv4di2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_zero_extendv4hiv4di2                       (rtx, rtx);
extern rtx        gen_avx2_zero_extendv4hiv4di2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_sign_extendv2hiv2di2                     (rtx, rtx);
extern rtx        gen_sse4_1_sign_extendv2hiv2di2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_zero_extendv2hiv2di2                     (rtx, rtx);
extern rtx        gen_sse4_1_zero_extendv2hiv2di2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sign_extendv8siv8di2                    (rtx, rtx);
extern rtx        gen_avx512f_sign_extendv8siv8di2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_zero_extendv8siv8di2                    (rtx, rtx);
extern rtx        gen_avx512f_zero_extendv8siv8di2_mask               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_sign_extendv4siv4di2                       (rtx, rtx);
extern rtx        gen_avx2_sign_extendv4siv4di2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_zero_extendv4siv4di2                       (rtx, rtx);
extern rtx        gen_avx2_zero_extendv4siv4di2_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_sign_extendv2siv2di2                     (rtx, rtx);
extern rtx        gen_sse4_1_sign_extendv2siv2di2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_zero_extendv2siv2di2                     (rtx, rtx);
extern rtx        gen_sse4_1_zero_extendv2siv2di2_mask                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vtestps256                                  (rtx, rtx);
extern rtx        gen_avx_vtestps                                     (rtx, rtx);
extern rtx        gen_avx_vtestpd256                                  (rtx, rtx);
extern rtx        gen_avx_vtestpd                                     (rtx, rtx);
extern rtx        gen_ptesttf2                                        (rtx, rtx);
extern rtx        gen_avx_roundps256                                  (rtx, rtx, rtx);
extern rtx        gen_sse4_1_roundps                                  (rtx, rtx, rtx);
extern rtx        gen_avx_roundpd256                                  (rtx, rtx, rtx);
extern rtx        gen_sse4_1_roundpd                                  (rtx, rtx, rtx);
extern rtx        gen_sse4_1_roundsh                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_roundss                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_roundsd                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_2_pcmpestr                                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_2_pcmpestri                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_2_pcmpestrm                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_2_pcmpestr_cconly                          (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_2_pcmpistr                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_2_pcmpistri                                (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_2_pcmpistrm                                (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_2_pcmpistr_cconly                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pmacsww                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pmacssww                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pmacsdd                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pmacssdd                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pmacsdql                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pmacssdql                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pmacsdqh                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pmacssdqh                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pmacswd                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pmacsswd                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pmadcswd                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pmadcsswd                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v32qi256                              (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v16qi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v16hi256                              (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v8hi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v8si256                               (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v4si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v4di256                               (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v2di                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v2ti256                               (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v1ti                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v16hf256                              (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v8hf                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v8sf256                               (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v4sf                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v4df256                               (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcmov_v2df                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_phaddbw                                     (rtx, rtx);
extern rtx        gen_xop_phaddubw                                    (rtx, rtx);
extern rtx        gen_xop_phaddbd                                     (rtx, rtx);
extern rtx        gen_xop_phaddubd                                    (rtx, rtx);
extern rtx        gen_xop_phaddbq                                     (rtx, rtx);
extern rtx        gen_xop_phaddubq                                    (rtx, rtx);
extern rtx        gen_xop_phaddwd                                     (rtx, rtx);
extern rtx        gen_xop_phadduwd                                    (rtx, rtx);
extern rtx        gen_xop_phaddwq                                     (rtx, rtx);
extern rtx        gen_xop_phadduwq                                    (rtx, rtx);
extern rtx        gen_xop_phadddq                                     (rtx, rtx);
extern rtx        gen_xop_phaddudq                                    (rtx, rtx);
extern rtx        gen_xop_phsubbw                                     (rtx, rtx);
extern rtx        gen_xop_phsubwd                                     (rtx, rtx);
extern rtx        gen_xop_phsubdq                                     (rtx, rtx);
extern rtx        gen_xop_pperm                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pperm_pack_v2di_v4si                        (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pperm_pack_v4si_v8hi                        (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pperm_pack_v8hi_v16qi                       (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_rotlv16qi3                                  (rtx, rtx, rtx);
extern rtx        gen_xop_rotlv8hi3                                   (rtx, rtx, rtx);
extern rtx        gen_xop_rotlv4si3                                   (rtx, rtx, rtx);
extern rtx        gen_xop_rotlv2di3                                   (rtx, rtx, rtx);
extern rtx        gen_xop_rotrv16qi3                                  (rtx, rtx, rtx);
extern rtx        gen_xop_rotrv8hi3                                   (rtx, rtx, rtx);
extern rtx        gen_xop_rotrv4si3                                   (rtx, rtx, rtx);
extern rtx        gen_xop_rotrv2di3                                   (rtx, rtx, rtx);
extern rtx        gen_xop_vrotlv16qi3                                 (rtx, rtx, rtx);
extern rtx        gen_xop_vrotlv8hi3                                  (rtx, rtx, rtx);
extern rtx        gen_xop_vrotlv4si3                                  (rtx, rtx, rtx);
extern rtx        gen_xop_vrotlv2di3                                  (rtx, rtx, rtx);
extern rtx        gen_xop_shav16qi3                                   (rtx, rtx, rtx);
extern rtx        gen_xop_shav8hi3                                    (rtx, rtx, rtx);
extern rtx        gen_xop_shav4si3                                    (rtx, rtx, rtx);
extern rtx        gen_xop_shav2di3                                    (rtx, rtx, rtx);
extern rtx        gen_xop_shlv16qi3                                   (rtx, rtx, rtx);
extern rtx        gen_xop_shlv8hi3                                    (rtx, rtx, rtx);
extern rtx        gen_xop_shlv4si3                                    (rtx, rtx, rtx);
extern rtx        gen_xop_shlv2di3                                    (rtx, rtx, rtx);
extern rtx        gen_xop_frczsf2                                     (rtx, rtx);
extern rtx        gen_xop_frczdf2                                     (rtx, rtx);
extern rtx        gen_xop_frczv4sf2                                   (rtx, rtx);
extern rtx        gen_xop_frczv2df2                                   (rtx, rtx);
extern rtx        gen_xop_frczv8sf2                                   (rtx, rtx);
extern rtx        gen_xop_frczv4df2                                   (rtx, rtx);
extern rtx        gen_xop_maskcmpv16qi3                               (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_maskcmpv8hi3                                (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_maskcmpv4si3                                (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_maskcmpv2di3                                (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_maskcmp_unsv16qi3                           (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_maskcmp_unsv8hi3                            (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_maskcmp_unsv4si3                            (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_maskcmp_unsv2di3                            (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_maskcmp_uns2v16qi3                          (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_maskcmp_uns2v8hi3                           (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_maskcmp_uns2v4si3                           (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_maskcmp_uns2v2di3                           (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcom_tfv16qi3                               (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcom_tfv8hi3                                (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcom_tfv4si3                                (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_pcom_tfv2di3                                (rtx, rtx, rtx, rtx);
extern rtx        gen_xop_vpermil2v8sf3                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xop_vpermil2v4sf3                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xop_vpermil2v4df3                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xop_vpermil2v2df3                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_aesenc                                          (rtx, rtx, rtx);
extern rtx        gen_aesenclast                                      (rtx, rtx, rtx);
extern rtx        gen_aesdec                                          (rtx, rtx, rtx);
extern rtx        gen_aesdeclast                                      (rtx, rtx, rtx);
extern rtx        gen_aesimc                                          (rtx, rtx);
extern rtx        gen_aeskeygenassist                                 (rtx, rtx, rtx);
extern rtx        gen_pclmulqdq                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vzeroupper_callee_abi                       (void);
extern rtx        gen_avx2_pbroadcastv16si                            (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv8di                             (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv64qi                            (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv32qi                            (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv16qi                            (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv32hi                            (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv16hi                            (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv8hi                             (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv8si                             (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv4si                             (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv4di                             (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv2di                             (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv32hf                            (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv16hf                            (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv8hf                             (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv32bf                            (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv16bf                            (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv8bf                             (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv32qi_1                          (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv16hi_1                          (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv8si_1                           (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv4di_1                           (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv16hf_1                          (rtx, rtx);
extern rtx        gen_avx2_pbroadcastv16bf_1                          (rtx, rtx);
extern rtx        gen_avx2_permvarv8si                                (rtx, rtx, rtx);
extern rtx        gen_avx2_permvarv8si_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_permvarv8sf                                (rtx, rtx, rtx);
extern rtx        gen_avx2_permvarv8sf_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_permvarv16si                            (rtx, rtx, rtx);
extern rtx        gen_avx512f_permvarv16si_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_permvarv16sf                            (rtx, rtx, rtx);
extern rtx        gen_avx512f_permvarv16sf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_permvarv8di                             (rtx, rtx, rtx);
extern rtx        gen_avx512f_permvarv8di_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_permvarv8df                             (rtx, rtx, rtx);
extern rtx        gen_avx512f_permvarv8df_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_permvarv4di                                (rtx, rtx, rtx);
extern rtx        gen_avx2_permvarv4di_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_permvarv4df                                (rtx, rtx, rtx);
extern rtx        gen_avx2_permvarv4df_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_permvarv64qi                           (rtx, rtx, rtx);
extern rtx        gen_avx512bw_permvarv64qi_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_permvarv16qi                           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_permvarv16qi_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_permvarv32qi                           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_permvarv32qi_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_permvarv8hi                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_permvarv8hi_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_permvarv16hi                           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_permvarv16hi_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_permvarv32hi                           (rtx, rtx, rtx);
extern rtx        gen_avx512bw_permvarv32hi_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_permvarv8hf                          (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_permvarv8hf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_permvarv16hf                           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_permvarv16hf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_permvarv32hf                           (rtx, rtx, rtx);
extern rtx        gen_avx512bw_permvarv32hf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_permvarv8bf                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_permvarv8bf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_permvarv16bf                           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_permvarv16bf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_permvarv32bf                           (rtx, rtx, rtx);
extern rtx        gen_avx512bw_permvarv32bf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_permv4di_1                                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_permv4di_1_mask                            (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_permv4df_1                                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_permv4df_1_mask                            (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_permv8df_1                              (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_permv8df_1_mask                         (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_permv8di_1                              (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_permv8di_1_mask                         (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_permv2ti                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_vec_dupv4df                                (rtx, rtx);
extern rtx        gen_avx512f_vec_dupv16si_1                          (rtx, rtx);
extern rtx        gen_avx512f_vec_dupv8di_1                           (rtx, rtx);
extern rtx        gen_avx512bw_vec_dupv32hi_1                         (rtx, rtx);
extern rtx        gen_avx512bw_vec_dupv64qi_1                         (rtx, rtx);
extern rtx        gen_avx512bw_vec_dupv32hf_1                         (rtx, rtx);
extern rtx        gen_avx512bw_vec_dupv32bf_1                         (rtx, rtx);
extern rtx        gen_avx512f_vec_dupv16si                            (rtx, rtx);
extern rtx        gen_avx512f_vec_dupv16si_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv8si                            (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv8si_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv4si                            (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv4si_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vec_dupv8di                             (rtx, rtx);
extern rtx        gen_avx512f_vec_dupv8di_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv4di                            (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv4di_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv2di                            (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv2di_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vec_dupv16sf                            (rtx, rtx);
extern rtx        gen_avx512f_vec_dupv16sf_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv8sf                            (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv8sf_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv4sf                            (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv4sf_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vec_dupv8df                             (rtx, rtx);
extern rtx        gen_avx512f_vec_dupv8df_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv4df                            (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv4df_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv2df                            (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv2df_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vec_dupv64qi                           (rtx, rtx);
extern rtx        gen_avx512bw_vec_dupv64qi_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv16qi                           (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv16qi_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv32qi                           (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv32qi_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vec_dupv32hi                           (rtx, rtx);
extern rtx        gen_avx512bw_vec_dupv32hi_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv16hi                           (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv16hi_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv8hi                            (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv8hi_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vec_dupv32hf                           (rtx, rtx);
extern rtx        gen_avx512bw_vec_dupv32hf_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv16hf                           (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv16hf_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vec_dupv8hf                          (rtx, rtx);
extern rtx        gen_avx512fp16_vec_dupv8hf_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vec_dupv32bf                           (rtx, rtx);
extern rtx        gen_avx512bw_vec_dupv32bf_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv16bf                           (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv16bf_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv8bf                            (rtx, rtx);
extern rtx        gen_avx512vl_vec_dupv8bf_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_broadcastv16sf_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_broadcastv16si_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_broadcastv8df_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_broadcastv8di_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vec_dup_gprv64qi_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv16qi_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv32qi_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vec_dup_gprv32hi_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv16hi_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv8hi_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vec_dup_gprv32hf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv16hf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vec_dup_gprv8hf_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vec_dup_gprv32bf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv16bf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv8bf_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vec_dup_gprv16si_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv8si_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv4si_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vec_dup_gprv8di_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv4di_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv2di_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vec_dup_gprv16sf_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv8sf_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv4sf_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vec_dup_gprv8df_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv4df_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vec_dup_gprv2df_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_dupv4sf                                     (rtx, rtx);
extern rtx        gen_avx2_vbroadcasti128_v32qi                       (rtx, rtx);
extern rtx        gen_avx2_vbroadcasti128_v16hi                       (rtx, rtx);
extern rtx        gen_avx2_vbroadcasti128_v8si                        (rtx, rtx);
extern rtx        gen_avx2_vbroadcasti128_v4di                        (rtx, rtx);
extern rtx        gen_avx2_lddqu_inserti_to_bcasti                    (rtx, rtx);
extern rtx        gen_vec_dupv8si                                     (rtx, rtx);
extern rtx        gen_vec_dupv8sf                                     (rtx, rtx);
extern rtx        gen_vec_dupv4di                                     (rtx, rtx);
extern rtx        gen_vec_dupv4df                                     (rtx, rtx);
extern rtx        gen_avx_vbroadcastf128_v32qi                        (rtx, rtx);
extern rtx        gen_avx_vbroadcastf128_v16hi                        (rtx, rtx);
extern rtx        gen_avx_vbroadcastf128_v8si                         (rtx, rtx);
extern rtx        gen_avx_vbroadcastf128_v4di                         (rtx, rtx);
extern rtx        gen_avx_vbroadcastf128_v8sf                         (rtx, rtx);
extern rtx        gen_avx_vbroadcastf128_v4df                         (rtx, rtx);
extern rtx        gen_avx_vbroadcastf128_v16hf                        (rtx, rtx);
extern rtx        gen_avx_vbroadcastf128_v16bf                        (rtx, rtx);
extern rtx        gen_avx512dq_broadcastv16si_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_broadcastv8si_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_broadcastv4si_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_broadcastv16sf_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_broadcastv8sf_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_broadcastv8si_mask_1                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_broadcastv8sf_mask_1                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_broadcastv16sf_mask_1                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_broadcastv16si_mask_1                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_broadcastv8di_mask_1                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_broadcastv8df_mask_1                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_broadcastv4di_mask_1                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_broadcastv4df_mask_1                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512cd_maskb_vec_dupv8di                      (rtx, rtx);
extern rtx        gen_avx512cd_maskb_vec_dupv4di                      (rtx, rtx);
extern rtx        gen_avx512cd_maskb_vec_dupv2di                      (rtx, rtx);
extern rtx        gen_avx512cd_maskw_vec_dupv16si                     (rtx, rtx);
extern rtx        gen_avx512cd_maskw_vec_dupv8si                      (rtx, rtx);
extern rtx        gen_avx512cd_maskw_vec_dupv4si                      (rtx, rtx);
extern rtx        gen_avx512f_vpermilvarv16sf3                        (rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermilvarv16sf3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vpermilvarv8sf3                             (rtx, rtx, rtx);
extern rtx        gen_avx_vpermilvarv8sf3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vpermilvarv4sf3                             (rtx, rtx, rtx);
extern rtx        gen_avx_vpermilvarv4sf3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermilvarv8df3                         (rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermilvarv8df3_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vpermilvarv4df3                             (rtx, rtx, rtx);
extern rtx        gen_avx_vpermilvarv4df3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vpermilvarv2df3                             (rtx, rtx, rtx);
extern rtx        gen_avx_vpermilvarv2df3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv16si3                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv16si3_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv16sf3                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv16sf3_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv8di3                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv8di3_maskz_1                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv8df3                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv8df3_maskz_1                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv8si3                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv8si3_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv8sf3                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv8sf3_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4di3                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4di3_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4df3                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4df3_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4si3                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4si3_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4sf3                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4sf3_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv2di3                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv2di3_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv2df3                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv2df3_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vpermt2varv32hi3                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vpermt2varv32hi3_maskz_1               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv16hi3                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv16hi3_maskz_1               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv8hi3                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv8hi3_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vpermt2varv64qi3                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vpermt2varv64qi3_maskz_1               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv32qi3                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv32qi3_maskz_1               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv16qi3                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv16qi3_maskz_1               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vpermt2varv32hf3                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vpermt2varv32hf3_maskz_1               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv16hf3                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv16hf3_maskz_1               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vpermt2varv8hf3                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vpermt2varv8hf3_maskz_1              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vpermt2varv32bf3                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vpermt2varv32bf3_maskz_1               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv16bf3                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv16bf3_maskz_1               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv8bf3                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv8bf3_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv16si3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv16sf3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv8di3_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv8df3_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv8si3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv8sf3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4di3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4df3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4si3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4sf3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv2di3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv2df3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vpermt2varv32hi3_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv16hi3_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv8hi3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vpermt2varv64qi3_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv32qi3_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv16qi3_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v4di                                 (rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v4di_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v4df                                 (rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v4df_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v4di                                 (rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v4di_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v4df                                 (rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v4df_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v8si                                 (rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v8si_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v8sf                                 (rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v8sf_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v8si                                 (rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v8si_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v8sf                                 (rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v8sf_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v16hi                                (rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v16hf                                (rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v16bf                                (rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v16hi                                (rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v16hf                                (rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v16bf                                (rtx, rtx, rtx);
extern rtx        gen_vec_set_lo_v32qi                                (rtx, rtx, rtx);
extern rtx        gen_vec_set_hi_v32qi                                (rtx, rtx, rtx);
extern rtx        gen_avx_maskloadps                                  (rtx, rtx, rtx);
extern rtx        gen_avx_maskloadpd                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_maskloadq256                               (rtx, rtx, rtx);
extern rtx        gen_avx2_maskloadq                                  (rtx, rtx, rtx);
extern rtx        gen_avx_maskloadps256                               (rtx, rtx, rtx);
extern rtx        gen_avx_maskloadpd256                               (rtx, rtx, rtx);
extern rtx        gen_avx2_maskloadd256                               (rtx, rtx, rtx);
extern rtx        gen_avx2_maskloadd                                  (rtx, rtx, rtx);
extern rtx        gen_avx_maskstoreps                                 (rtx, rtx, rtx);
extern rtx        gen_avx_maskstorepd                                 (rtx, rtx, rtx);
extern rtx        gen_avx2_maskstoreq256                              (rtx, rtx, rtx);
extern rtx        gen_avx2_maskstoreq                                 (rtx, rtx, rtx);
extern rtx        gen_avx_maskstoreps256                              (rtx, rtx, rtx);
extern rtx        gen_avx_maskstorepd256                              (rtx, rtx, rtx);
extern rtx        gen_avx2_maskstored256                              (rtx, rtx, rtx);
extern rtx        gen_avx2_maskstored                                 (rtx, rtx, rtx);
extern rtx        gen_avx512f_storev16si_mask                         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev8si_mask                         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev4si_mask                         (rtx, rtx, rtx);
extern rtx        gen_avx512f_storev8di_mask                          (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev4di_mask                         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev2di_mask                         (rtx, rtx, rtx);
extern rtx        gen_avx512f_storev16sf_mask                         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev8sf_mask                         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev4sf_mask                         (rtx, rtx, rtx);
extern rtx        gen_avx512f_storev8df_mask                          (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev4df_mask                         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev2df_mask                         (rtx, rtx, rtx);
extern rtx        gen_avx512bw_storev64qi_mask                        (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev16qi_mask                        (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev32qi_mask                        (rtx, rtx, rtx);
extern rtx        gen_avx512bw_storev32hi_mask                        (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev16hi_mask                        (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev8hi_mask                         (rtx, rtx, rtx);
extern rtx        gen_avx512bw_storev32hf_mask                        (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev16hf_mask                        (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_storev8hf_mask                       (rtx, rtx, rtx);
extern rtx        gen_avx512bw_storev32bf_mask                        (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev16bf_mask                        (rtx, rtx, rtx);
extern rtx        gen_avx512vl_storev8bf_mask                         (rtx, rtx, rtx);
extern rtx        gen_avx_si256_si                                    (rtx, rtx);
extern rtx        gen_avx_ps256_ps                                    (rtx, rtx);
extern rtx        gen_avx_pd256_pd                                    (rtx, rtx);
extern rtx        gen_avx2_ashrvv4si                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_ashrvv4si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_ashrvv8si                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_ashrvv8si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_ashrvv16si                              (rtx, rtx, rtx);
extern rtx        gen_avx512f_ashrvv16si_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_ashrvv2di                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_ashrvv2di_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_ashrvv4di                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_ashrvv4di_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_ashrvv8di                               (rtx, rtx, rtx);
extern rtx        gen_avx512f_ashrvv8di_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ashrvv8hi                              (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ashrvv8hi_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ashrvv16hi                             (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ashrvv16hi_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_ashrvv32hi                             (rtx, rtx, rtx);
extern rtx        gen_avx512bw_ashrvv32hi_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_ashlvv16si                              (rtx, rtx, rtx);
extern rtx        gen_avx512f_ashlvv16si_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_lshrvv16si                              (rtx, rtx, rtx);
extern rtx        gen_avx512f_lshrvv16si_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_ashlvv8si                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_ashlvv8si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_lshrvv8si                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_lshrvv8si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_ashlvv4si                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_ashlvv4si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_lshrvv4si                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_lshrvv4si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_ashlvv8di                               (rtx, rtx, rtx);
extern rtx        gen_avx512f_ashlvv8di_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_lshrvv8di                               (rtx, rtx, rtx);
extern rtx        gen_avx512f_lshrvv8di_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_ashlvv4di                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_ashlvv4di_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_lshrvv4di                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_lshrvv4di_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_ashlvv2di                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_ashlvv2di_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_lshrvv2di                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_lshrvv2di_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ashlvv8hi                              (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ashlvv8hi_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_lshrvv8hi                              (rtx, rtx, rtx);
extern rtx        gen_avx512vl_lshrvv8hi_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_ashlvv16hi                             (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ashlvv16hi_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_lshrvv16hi                             (rtx, rtx, rtx);
extern rtx        gen_avx512vl_lshrvv16hi_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_ashlvv32hi                             (rtx, rtx, rtx);
extern rtx        gen_avx512bw_ashlvv32hi_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_lshrvv32hi                             (rtx, rtx, rtx);
extern rtx        gen_avx512bw_lshrvv32hi_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv32qi                             (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv16hi                             (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv16hf                             (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv16bf                             (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv8si                              (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv4di                              (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv8sf                              (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv4df                              (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv64qi                             (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv32hi                             (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv32hf                             (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv32bf                             (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv16si                             (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv8di                              (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv16sf                             (rtx, rtx, rtx);
extern rtx        gen_avx_vec_concatv8df                              (rtx, rtx, rtx);
extern rtx        gen_vcvtph2ps                                       (rtx, rtx);
extern rtx        gen_vcvtph2ps_mask                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtph2ps256                                    (rtx, rtx);
extern rtx        gen_vcvtph2ps256_mask                               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vcvtph2ps512_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vcvtph2ps512_mask_round                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtps2ph256                                    (rtx, rtx, rtx);
extern rtx        gen_vcvtps2ph256_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vcvtps2ph512_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vcvtps2ph512_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_compressv16si_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_compressv16sf_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_compressv8di_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_compressv8df_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressv8si_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressv8sf_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressv4di_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressv4df_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressv4si_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressv4sf_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressv2di_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressv2df_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_compressv64qi_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_compressv16qi_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_compressv32qi_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_compressv32hi_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_compressv16hi_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_compressv8hi_mask                               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_compressstorev16si_mask                 (rtx, rtx, rtx);
extern rtx        gen_avx512f_compressstorev16sf_mask                 (rtx, rtx, rtx);
extern rtx        gen_avx512f_compressstorev8di_mask                  (rtx, rtx, rtx);
extern rtx        gen_avx512f_compressstorev8df_mask                  (rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressstorev8si_mask                 (rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressstorev8sf_mask                 (rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressstorev4di_mask                 (rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressstorev4df_mask                 (rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressstorev4si_mask                 (rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressstorev4sf_mask                 (rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressstorev2di_mask                 (rtx, rtx, rtx);
extern rtx        gen_avx512vl_compressstorev2df_mask                 (rtx, rtx, rtx);
extern rtx        gen_compressstorev64qi_mask                         (rtx, rtx, rtx);
extern rtx        gen_compressstorev16qi_mask                         (rtx, rtx, rtx);
extern rtx        gen_compressstorev32qi_mask                         (rtx, rtx, rtx);
extern rtx        gen_compressstorev32hi_mask                         (rtx, rtx, rtx);
extern rtx        gen_compressstorev16hi_mask                         (rtx, rtx, rtx);
extern rtx        gen_compressstorev8hi_mask                          (rtx, rtx, rtx);
extern rtx        gen_expandv16si_mask                                (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv16sf_mask                                (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv8di_mask                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv8df_mask                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv8si_mask                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv8sf_mask                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv4di_mask                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv4df_mask                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv4si_mask                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv4sf_mask                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv2di_mask                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv2df_mask                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv64qi_mask                                (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv16qi_mask                                (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv32qi_mask                                (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv32hi_mask                                (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv16hi_mask                                (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv8hi_mask                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangepv16sf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangepv16sf_round                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangepv16sf_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangepv16sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangepv8sf                             (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512dq_rangepv8sf_round                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512dq_rangepv8sf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512dq_rangepv8sf_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512dq_rangepv8sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512dq_rangepv8sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_avx512dq_rangepv4sf                             (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512dq_rangepv4sf_round                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512dq_rangepv4sf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512dq_rangepv4sf_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512dq_rangepv4sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512dq_rangepv4sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_avx512dq_rangepv8df                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangepv8df_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangepv8df_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangepv8df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangepv4df                             (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512dq_rangepv4df_round                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512dq_rangepv4df_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512dq_rangepv4df_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512dq_rangepv4df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512dq_rangepv4df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_avx512dq_rangepv2df                             (rtx, rtx, rtx, rtx);
static inline rtx gen_avx512dq_rangepv2df_round                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512dq_rangepv2df_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx512dq_rangepv2df_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx512dq_rangepv2df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx512dq_rangepv2df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g))
{
  return 0;
}
extern rtx        gen_avx512dq_rangesv4sf                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangesv4sf_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangesv4sf_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangesv4sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangesv2df                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangesv2df_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangesv2df_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_rangesv2df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv32hf                           (rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv32hf_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv16hf                           (rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv16hf_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv8hf                            (rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv8hf_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv16sf                           (rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv16sf_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv8sf                            (rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv8sf_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv4sf                            (rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv4sf_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv8df                            (rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv8df_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv4df                            (rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv4df_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv2df                            (rtx, rtx, rtx);
extern rtx        gen_avx512dq_fpclassv2df_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_vmfpclassv8hf                          (rtx, rtx, rtx);
extern rtx        gen_avx512dq_vmfpclassv8hf_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_vmfpclassv4sf                          (rtx, rtx, rtx);
extern rtx        gen_avx512dq_vmfpclassv4sf_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_vmfpclassv2df                          (rtx, rtx, rtx);
extern rtx        gen_avx512dq_vmfpclassv2df_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_getmantv32hf                           (rtx, rtx, rtx);
extern rtx        gen_avx512bw_getmantv32hf_round                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_getmantv32hf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_getmantv32hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv16hf                           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv16hf_round                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv16hf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv16hf_mask_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_getmantv8hf                          (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_getmantv8hf_round                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_getmantv8hf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_getmantv8hf_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_getmantv16sf                            (rtx, rtx, rtx);
extern rtx        gen_avx512f_getmantv16sf_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_getmantv16sf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_getmantv16sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv8sf                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv8sf_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv8sf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv8sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv4sf                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv4sf_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv4sf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv4sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_getmantv8df                             (rtx, rtx, rtx);
extern rtx        gen_avx512f_getmantv8df_round                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_getmantv8df_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_getmantv8df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv4df                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv4df_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv4df_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv4df_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv2df                            (rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv2df_round                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv2df_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_getmantv2df_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vgetmantv8hf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vgetmantv8hf_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vgetmantv8hf_round                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vgetmantv8hf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vgetmantv4sf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vgetmantv4sf_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vgetmantv4sf_round                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vgetmantv4sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vgetmantv2df                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vgetmantv2df_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vgetmantv2df_round                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vgetmantv2df_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_dbpsadbwv8hi_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_dbpsadbwv16hi_mask                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_dbpsadbwv32hi_mask                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_clzv16si2                                       (rtx, rtx);
extern rtx        gen_clzv16si2_mask                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_clzv8si2                                        (rtx, rtx);
extern rtx        gen_clzv8si2_mask                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_clzv4si2                                        (rtx, rtx);
extern rtx        gen_clzv4si2_mask                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_clzv8di2                                        (rtx, rtx);
extern rtx        gen_clzv8di2_mask                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_clzv4di2                                        (rtx, rtx);
extern rtx        gen_clzv4di2_mask                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_clzv2di2                                        (rtx, rtx);
extern rtx        gen_clzv2di2_mask                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_conflictv16si_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_conflictv8si_mask                               (rtx, rtx, rtx, rtx);
extern rtx        gen_conflictv4si_mask                               (rtx, rtx, rtx, rtx);
extern rtx        gen_conflictv8di_mask                               (rtx, rtx, rtx, rtx);
extern rtx        gen_conflictv4di_mask                               (rtx, rtx, rtx, rtx);
extern rtx        gen_conflictv2di_mask                               (rtx, rtx, rtx, rtx);
extern rtx        gen_sha1msg1                                        (rtx, rtx, rtx);
extern rtx        gen_sha1msg2                                        (rtx, rtx, rtx);
extern rtx        gen_sha1nexte                                       (rtx, rtx, rtx);
extern rtx        gen_sha1rnds4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_sha256msg1                                      (rtx, rtx, rtx);
extern rtx        gen_sha256msg2                                      (rtx, rtx, rtx);
extern rtx        gen_sha256rnds2                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_vsm3msg1                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_vsm3msg2                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_vsm3rnds2                                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vsha512msg1                                     (rtx, rtx, rtx);
extern rtx        gen_vsha512msg2                                     (rtx, rtx, rtx);
extern rtx        gen_vsha512rnds2                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vsm4key4_v16si                                  (rtx, rtx, rtx);
extern rtx        gen_vsm4key4_v8si                                   (rtx, rtx, rtx);
extern rtx        gen_vsm4key4_v4si                                   (rtx, rtx, rtx);
extern rtx        gen_vsm4rnds4_v16si                                 (rtx, rtx, rtx);
extern rtx        gen_vsm4rnds4_v8si                                  (rtx, rtx, rtx);
extern rtx        gen_vsm4rnds4_v4si                                  (rtx, rtx, rtx);
extern rtx        gen_avx512f_si512_si                                (rtx, rtx);
extern rtx        gen_avx512f_ps512_ps                                (rtx, rtx);
extern rtx        gen_avx512f_pd512_pd                                (rtx, rtx);
extern rtx        gen_avx512f_si512_256si                             (rtx, rtx);
extern rtx        gen_avx512f_ps512_256ps                             (rtx, rtx);
extern rtx        gen_avx512f_pd512_256pd                             (rtx, rtx);
extern rtx        gen_vpmadd52luqv8di                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52huqv8di                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52luqv4di                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52huqv4di                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52luqv2di                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52huqv2di                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52luqv8di_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52huqv8di_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52luqv4di_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52huqv4di_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52luqv2di_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52huqv2di_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52luqv8di_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52huqv8di_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52luqv4di_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52huqv4di_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52luqv2di_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52huqv2di_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmultishiftqbv64qi                             (rtx, rtx, rtx);
extern rtx        gen_vpmultishiftqbv64qi_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmultishiftqbv16qi                             (rtx, rtx, rtx);
extern rtx        gen_vpmultishiftqbv16qi_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmultishiftqbv32qi                             (rtx, rtx, rtx);
extern rtx        gen_vpmultishiftqbv32qi_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpopcountv16si                                  (rtx, rtx);
extern rtx        gen_vpopcountv16si_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vpopcountv8si                                   (rtx, rtx);
extern rtx        gen_vpopcountv8si_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vpopcountv4si                                   (rtx, rtx);
extern rtx        gen_vpopcountv4si_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vpopcountv8di                                   (rtx, rtx);
extern rtx        gen_vpopcountv8di_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vpopcountv4di                                   (rtx, rtx);
extern rtx        gen_vpopcountv4di_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vpopcountv2di                                   (rtx, rtx);
extern rtx        gen_vpopcountv2di_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vpopcountv64qi                                  (rtx, rtx);
extern rtx        gen_vpopcountv64qi_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vpopcountv16qi                                  (rtx, rtx);
extern rtx        gen_vpopcountv16qi_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vpopcountv32qi                                  (rtx, rtx);
extern rtx        gen_vpopcountv32qi_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vpopcountv32hi                                  (rtx, rtx);
extern rtx        gen_vpopcountv32hi_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vpopcountv16hi                                  (rtx, rtx);
extern rtx        gen_vpopcountv16hi_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vpopcountv8hi                                   (rtx, rtx);
extern rtx        gen_vpopcountv8hi_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8affineinvqb_v64qi                         (rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8affineinvqb_v64qi_mask                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8affineinvqb_v32qi                         (rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8affineinvqb_v32qi_mask                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8affineinvqb_v16qi                         (rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8affineinvqb_v16qi_mask                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8affineqb_v64qi                            (rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8affineqb_v64qi_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8affineqb_v32qi                            (rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8affineqb_v32qi_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8affineqb_v16qi                            (rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8affineqb_v16qi_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8mulb_v64qi                                (rtx, rtx, rtx);
extern rtx        gen_vgf2p8mulb_v64qi_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8mulb_v32qi                                (rtx, rtx, rtx);
extern rtx        gen_vgf2p8mulb_v32qi_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vgf2p8mulb_v16qi                                (rtx, rtx, rtx);
extern rtx        gen_vgf2p8mulb_v16qi_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v32hi                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v32hi_mask                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v16si                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v16si_mask                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v8di                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v8di_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v16hi                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v16hi_mask                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v8si                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v8si_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v4di                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v4di_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v8hi                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v8hi_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v4si                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v4si_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v2di                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrd_v2di_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v32hi                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v32hi_mask                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v16si                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v16si_mask                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v8di                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v8di_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v16hi                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v16hi_mask                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v8si                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v8si_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v4di                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v4di_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v8hi                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v8hi_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v4si                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v4si_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v2di                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshld_v2di_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v32hi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v16si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v8di                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v16hi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v8si                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v4di                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v8hi                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v4si                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v2di                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v32hi_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v16si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v8di_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v16hi_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v8si_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v4di_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v8hi_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v4si_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v2di_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v32hi_maskz_1                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v16si_maskz_1                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v8di_maskz_1                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v16hi_maskz_1                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v8si_maskz_1                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v4di_maskz_1                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v8hi_maskz_1                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v4si_maskz_1                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v2di_maskz_1                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v32hi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v16si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v8di                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v16hi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v8si                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v4di                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v8hi                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v4si                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v2di                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v32hi_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v16si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v8di_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v16hi_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v8si_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v4di_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v8hi_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v4si_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v2di_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v32hi_maskz_1                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v16si_maskz_1                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v8di_maskz_1                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v16hi_maskz_1                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v8si_maskz_1                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v4di_maskz_1                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v8hi_maskz_1                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v4si_maskz_1                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v2di_maskz_1                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusd_v16si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusd_v8si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusd_v4si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusd_v16si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusd_v8si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusd_v4si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusd_v16si_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusd_v8si_maskz_1                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusd_v4si_maskz_1                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusds_v16si                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusds_v8si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusds_v4si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusds_v16si_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusds_v8si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusds_v4si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusds_v16si_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusds_v8si_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusds_v4si_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssd_v16si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssd_v8si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssd_v4si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssd_v16si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssd_v8si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssd_v4si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssd_v16si_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssd_v8si_maskz_1                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssd_v4si_maskz_1                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssds_v16si                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssds_v8si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssds_v4si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssds_v16si_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssds_v8si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssds_v4si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssds_v16si_maskz_1                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssds_v8si_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssds_v4si_maskz_1                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vaesdec_v32qi                                   (rtx, rtx, rtx);
extern rtx        gen_vaesdec_v16qi                                   (rtx, rtx, rtx);
extern rtx        gen_vaesdec_v64qi                                   (rtx, rtx, rtx);
extern rtx        gen_vaesdeclast_v32qi                               (rtx, rtx, rtx);
extern rtx        gen_vaesdeclast_v16qi                               (rtx, rtx, rtx);
extern rtx        gen_vaesdeclast_v64qi                               (rtx, rtx, rtx);
extern rtx        gen_vaesenc_v32qi                                   (rtx, rtx, rtx);
extern rtx        gen_vaesenc_v16qi                                   (rtx, rtx, rtx);
extern rtx        gen_vaesenc_v64qi                                   (rtx, rtx, rtx);
extern rtx        gen_vaesenclast_v32qi                               (rtx, rtx, rtx);
extern rtx        gen_vaesenclast_v16qi                               (rtx, rtx, rtx);
extern rtx        gen_vaesenclast_v64qi                               (rtx, rtx, rtx);
extern rtx        gen_vpclmulqdq_v8di                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpclmulqdq_v4di                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpclmulqdq_v2di                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpshufbitqmbv64qi                      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpshufbitqmbv64qi_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpshufbitqmbv16qi                      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpshufbitqmbv16qi_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpshufbitqmbv32qi                      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpshufbitqmbv32qi_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vp2intersect_2intersectv8di               (rtx, rtx, rtx);
extern rtx        gen_avx512vp2intersect_2intersectv4di               (rtx, rtx, rtx);
extern rtx        gen_avx512vp2intersect_2intersectv2di               (rtx, rtx, rtx);
extern rtx        gen_avx512vp2intersect_2intersectv8si               (rtx, rtx, rtx);
extern rtx        gen_avx512vp2intersect_2intersectv4si               (rtx, rtx, rtx);
extern rtx        gen_avx512vp2intersect_2intersectv16si              (rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtne2ps2bf16_v32bf                     (rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtne2ps2bf16_v32bf_mask                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtne2ps2bf16_v16bf                     (rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtne2ps2bf16_v16bf_mask                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtne2ps2bf16_v8bf                      (rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtne2ps2bf16_v8bf_mask                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtneps2bf16_v4sf_mask_1                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtneps2bf16_v8sf                              (rtx, rtx);
extern rtx        gen_avx512f_cvtneps2bf16_v16sf                      (rtx, rtx);
extern rtx        gen_avx512f_cvtneps2bf16_v16sf_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtneps2bf16_v8sf                       (rtx, rtx);
extern rtx        gen_avx512f_cvtneps2bf16_v8sf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_dpbf16ps_v16sf                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_dpbf16ps_v16sf_maskz_1                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_dpbf16ps_v8sf                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_dpbf16ps_v8sf_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_dpbf16ps_v4sf                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_dpbf16ps_v4sf_maskz_1                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_dpbf16ps_v16sf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_dpbf16ps_v8sf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_dpbf16ps_v4sf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_loadiwkey                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_aesdec128klu8                                   (rtx, rtx, rtx);
extern rtx        gen_aesdec256klu8                                   (rtx, rtx, rtx);
extern rtx        gen_aesenc128klu8                                   (rtx, rtx, rtx);
extern rtx        gen_aesenc256klu8                                   (rtx, rtx, rtx);
extern rtx        gen_vpdpbssd_v8si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssds_v8si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsud_v8si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsuds_v8si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuud_v8si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuuds_v8si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssd_v4si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssds_v4si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsud_v4si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsuds_v4si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuud_v4si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuuds_v4si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssd_v16si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssds_v16si                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsud_v16si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsuds_v16si                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuud_v16si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuuds_v16si                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssd_v16si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssds_v16si_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsud_v16si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsuds_v16si_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuud_v16si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuuds_v16si_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssd_v8si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssds_v8si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsud_v8si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsuds_v8si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuud_v8si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuuds_v8si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssd_v4si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssds_v4si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsud_v4si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsuds_v4si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuud_v4si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuuds_v4si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vbcstnebf162ps_v8sf                             (rtx, rtx);
extern rtx        gen_vbcstnebf162ps_v4sf                             (rtx, rtx);
extern rtx        gen_vbcstnesh2ps_v8sf                               (rtx, rtx);
extern rtx        gen_vbcstnesh2ps_v4sf                               (rtx, rtx);
extern rtx        gen_vcvtneeph2ps_v8hf                               (rtx, rtx);
extern rtx        gen_vcvtneebf162ps_v8bf                             (rtx, rtx);
extern rtx        gen_vcvtneeph2ps_v16hf                              (rtx, rtx);
extern rtx        gen_vcvtneebf162ps_v16bf                            (rtx, rtx);
extern rtx        gen_vcvtneoph2ps_v8hf                               (rtx, rtx);
extern rtx        gen_vcvtneobf162ps_v8bf                             (rtx, rtx);
extern rtx        gen_vcvtneoph2ps_v16hf                              (rtx, rtx);
extern rtx        gen_vcvtneobf162ps_v16bf                            (rtx, rtx);
extern rtx        gen_avx10_2_cvt2ps2phx_v32hf                        (rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvt2ps2phx_v32hf_round                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvt2ps2phx_v32hf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvt2ps2phx_v32hf_mask_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvt2ps2phx_v16hf                        (rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvt2ps2phx_v16hf_round                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvt2ps2phx_v16hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_avx10_2_cvt2ps2phx_v16hf_mask                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvt2ps2phx_v16hf_mask_round             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvt2ps2phx_v16hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx10_2_cvt2ps2phx_v8hf                         (rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvt2ps2phx_v8hf_round                   (rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvt2ps2phx_v8hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_avx10_2_cvt2ps2phx_v8hf_mask                    (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvt2ps2phx_v8hf_mask_round              (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvt2ps2phx_v8hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_vcvt2ph2bf8v32hf                                (rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2bf8v32hf_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2bf8sv32hf                               (rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2bf8sv32hf_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2hf8v32hf                                (rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2hf8v32hf_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2hf8sv32hf                               (rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2hf8sv32hf_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2bf8v16hf                                (rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2bf8v16hf_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2bf8sv16hf                               (rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2bf8sv16hf_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2hf8v16hf                                (rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2hf8v16hf_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2hf8sv16hf                               (rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2hf8sv16hf_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2bf8v8hf                                 (rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2bf8v8hf_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2bf8sv8hf                                (rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2bf8sv8hf_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2hf8v8hf                                 (rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2hf8v8hf_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2hf8sv8hf                                (rtx, rtx, rtx);
extern rtx        gen_vcvt2ph2hf8sv8hf_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2bf8v32hf                             (rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2bf8v32hf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2bf8sv32hf                            (rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2bf8sv32hf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2hf8v32hf                             (rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2hf8v32hf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2hf8sv32hf                            (rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2hf8sv32hf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2bf8v16hf                             (rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2bf8v16hf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2bf8sv16hf                            (rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2bf8sv16hf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2hf8v16hf                             (rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2hf8v16hf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2hf8sv16hf                            (rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2hf8sv16hf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtph2bf8v16hf                                 (rtx, rtx);
extern rtx        gen_vcvtph2bf8v16hf_mask                            (rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtph2bf8sv16hf                                (rtx, rtx);
extern rtx        gen_vcvtph2bf8sv16hf_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtph2hf8v16hf                                 (rtx, rtx);
extern rtx        gen_vcvtph2hf8v16hf_mask                            (rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtph2hf8sv16hf                                (rtx, rtx);
extern rtx        gen_vcvtph2hf8sv16hf_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtph2bf8v32hf                                 (rtx, rtx);
extern rtx        gen_vcvtph2bf8v32hf_mask                            (rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtph2bf8sv32hf                                (rtx, rtx);
extern rtx        gen_vcvtph2bf8sv32hf_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtph2hf8v32hf                                 (rtx, rtx);
extern rtx        gen_vcvtph2hf8v32hf_mask                            (rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtph2hf8sv32hf                                (rtx, rtx);
extern rtx        gen_vcvtph2hf8sv32hf_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_vcvthf82phv32hf                                 (rtx, rtx);
extern rtx        gen_vcvthf82phv32hf_mask                            (rtx, rtx, rtx, rtx);
extern rtx        gen_vcvthf82phv16hf                                 (rtx, rtx);
extern rtx        gen_vcvthf82phv16hf_mask                            (rtx, rtx, rtx, rtx);
extern rtx        gen_vcvthf82phv8hf                                  (rtx, rtx);
extern rtx        gen_vcvthf82phv8hf_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusd_v8si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusds_v8si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsud_v8si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsuds_v8si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuud_v8si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuuds_v8si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusd_v4si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusds_v4si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsud_v4si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsuds_v4si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuud_v4si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuuds_v4si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusd_v16si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusds_v16si                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsud_v16si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsuds_v16si                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuud_v16si                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuuds_v16si                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusd_v16si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusds_v16si_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsud_v16si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsuds_v16si_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuud_v16si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuuds_v16si_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusd_v8si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusds_v8si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsud_v8si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsuds_v8si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuud_v8si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuuds_v8si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusd_v4si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusds_v4si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsud_v4si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsuds_v4si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuud_v4si_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuuds_v4si_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vdpphps_v16sf                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vdpphps_v8sf                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vdpphps_v4sf                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vdpphps_v16sf_mask                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vdpphps_v8sf_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vdpphps_v4sf_mask                               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vdpphps_v16sf_maskz_1                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vdpphps_v8sf_maskz_1                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vdpphps_v4sf_maskz_1                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_scalefbf16_v32bf                        (rtx, rtx, rtx);
extern rtx        gen_avx10_2_scalefbf16_v32bf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_scalefbf16_v16bf                        (rtx, rtx, rtx);
extern rtx        gen_avx10_2_scalefbf16_v16bf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_scalefbf16_v8bf                         (rtx, rtx, rtx);
extern rtx        gen_avx10_2_scalefbf16_v8bf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_smaxbf16_v32bf                          (rtx, rtx, rtx);
extern rtx        gen_avx10_2_smaxbf16_v32bf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_sminbf16_v32bf                          (rtx, rtx, rtx);
extern rtx        gen_avx10_2_sminbf16_v32bf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_smaxbf16_v16bf                          (rtx, rtx, rtx);
extern rtx        gen_avx10_2_smaxbf16_v16bf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_sminbf16_v16bf                          (rtx, rtx, rtx);
extern rtx        gen_avx10_2_sminbf16_v16bf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_smaxbf16_v8bf                           (rtx, rtx, rtx);
extern rtx        gen_avx10_2_smaxbf16_v8bf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_sminbf16_v8bf                           (rtx, rtx, rtx);
extern rtx        gen_avx10_2_sminbf16_v8bf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_addbf16_v32bf                           (rtx, rtx, rtx);
extern rtx        gen_avx10_2_addbf16_v32bf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_subbf16_v32bf                           (rtx, rtx, rtx);
extern rtx        gen_avx10_2_subbf16_v32bf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_mulbf16_v32bf                           (rtx, rtx, rtx);
extern rtx        gen_avx10_2_mulbf16_v32bf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_divbf16_v32bf                           (rtx, rtx, rtx);
extern rtx        gen_avx10_2_divbf16_v32bf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_addbf16_v16bf                           (rtx, rtx, rtx);
extern rtx        gen_avx10_2_addbf16_v16bf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_subbf16_v16bf                           (rtx, rtx, rtx);
extern rtx        gen_avx10_2_subbf16_v16bf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_mulbf16_v16bf                           (rtx, rtx, rtx);
extern rtx        gen_avx10_2_mulbf16_v16bf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_divbf16_v16bf                           (rtx, rtx, rtx);
extern rtx        gen_avx10_2_divbf16_v16bf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_addbf16_v8bf                            (rtx, rtx, rtx);
extern rtx        gen_avx10_2_addbf16_v8bf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_subbf16_v8bf                            (rtx, rtx, rtx);
extern rtx        gen_avx10_2_subbf16_v8bf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_mulbf16_v8bf                            (rtx, rtx, rtx);
extern rtx        gen_avx10_2_mulbf16_v8bf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_divbf16_v8bf                            (rtx, rtx, rtx);
extern rtx        gen_avx10_2_divbf16_v8bf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v32bf                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v32bf_maskz_1                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v16bf                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v16bf_maskz_1                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v8bf                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v8bf_maskz_1                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v32bf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v16bf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v8bf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v32bf_mask3                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v16bf_mask3                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v8bf_mask3                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v32bf                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v32bf_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v16bf                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v16bf_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v8bf                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v8bf_maskz_1                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v32bf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v16bf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v8bf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v32bf_mask3                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v16bf_mask3                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v8bf_mask3                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v32bf                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v32bf_maskz_1                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v16bf                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v16bf_maskz_1                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v8bf                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v8bf_maskz_1                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v32bf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v16bf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v8bf_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v32bf_mask3                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v16bf_mask3                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v8bf_mask3                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v32bf                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v32bf_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v16bf                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v16bf_maskz_1                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v8bf                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v8bf_maskz_1                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v32bf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v16bf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v8bf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v32bf_mask3                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v16bf_mask3                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v8bf_mask3                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_rsqrtbf16_v32bf                         (rtx, rtx);
extern rtx        gen_avx10_2_rsqrtbf16_v32bf_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_rsqrtbf16_v16bf                         (rtx, rtx);
extern rtx        gen_avx10_2_rsqrtbf16_v16bf_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_rsqrtbf16_v8bf                          (rtx, rtx);
extern rtx        gen_avx10_2_rsqrtbf16_v8bf_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_sqrtbf16_v32bf                          (rtx, rtx);
extern rtx        gen_avx10_2_sqrtbf16_v32bf_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_sqrtbf16_v16bf                          (rtx, rtx);
extern rtx        gen_avx10_2_sqrtbf16_v16bf_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_sqrtbf16_v8bf                           (rtx, rtx);
extern rtx        gen_avx10_2_sqrtbf16_v8bf_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_rcpbf16_v32bf                           (rtx, rtx);
extern rtx        gen_avx10_2_rcpbf16_v32bf_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_rcpbf16_v16bf                           (rtx, rtx);
extern rtx        gen_avx10_2_rcpbf16_v16bf_mask                      (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_rcpbf16_v8bf                            (rtx, rtx);
extern rtx        gen_avx10_2_rcpbf16_v8bf_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_getexpbf16_v32bf                        (rtx, rtx);
extern rtx        gen_avx10_2_getexpbf16_v32bf_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_getexpbf16_v16bf                        (rtx, rtx);
extern rtx        gen_avx10_2_getexpbf16_v16bf_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_getexpbf16_v8bf                         (rtx, rtx);
extern rtx        gen_avx10_2_getexpbf16_v8bf_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_rndscalebf16_v32bf                      (rtx, rtx, rtx);
extern rtx        gen_avx10_2_rndscalebf16_v32bf_mask                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_reducebf16_v32bf                        (rtx, rtx, rtx);
extern rtx        gen_avx10_2_reducebf16_v32bf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_getmantbf16_v32bf                       (rtx, rtx, rtx);
extern rtx        gen_avx10_2_getmantbf16_v32bf_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_rndscalebf16_v16bf                      (rtx, rtx, rtx);
extern rtx        gen_avx10_2_rndscalebf16_v16bf_mask                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_reducebf16_v16bf                        (rtx, rtx, rtx);
extern rtx        gen_avx10_2_reducebf16_v16bf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_getmantbf16_v16bf                       (rtx, rtx, rtx);
extern rtx        gen_avx10_2_getmantbf16_v16bf_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_rndscalebf16_v8bf                       (rtx, rtx, rtx);
extern rtx        gen_avx10_2_rndscalebf16_v8bf_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_reducebf16_v8bf                         (rtx, rtx, rtx);
extern rtx        gen_avx10_2_reducebf16_v8bf_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_getmantbf16_v8bf                        (rtx, rtx, rtx);
extern rtx        gen_avx10_2_getmantbf16_v8bf_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fpclassbf16_v32bf                       (rtx, rtx, rtx);
extern rtx        gen_avx10_2_fpclassbf16_v32bf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fpclassbf16_v16bf                       (rtx, rtx, rtx);
extern rtx        gen_avx10_2_fpclassbf16_v16bf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fpclassbf16_v8bf                        (rtx, rtx, rtx);
extern rtx        gen_avx10_2_fpclassbf16_v8bf_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cmpbf16_v32bf                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cmpbf16_v32bf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cmpbf16_v16bf                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cmpbf16_v16bf_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cmpbf16_v8bf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cmpbf16_v8bf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtbf162ibsv32bf                        (rtx, rtx);
extern rtx        gen_avx10_2_cvtbf162ibsv32bf_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtbf162iubsv32bf                       (rtx, rtx);
extern rtx        gen_avx10_2_cvtbf162iubsv32bf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttbf162ibsv32bf                       (rtx, rtx);
extern rtx        gen_avx10_2_cvttbf162ibsv32bf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttbf162iubsv32bf                      (rtx, rtx);
extern rtx        gen_avx10_2_cvttbf162iubsv32bf_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtbf162ibsv16bf                        (rtx, rtx);
extern rtx        gen_avx10_2_cvtbf162ibsv16bf_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtbf162iubsv16bf                       (rtx, rtx);
extern rtx        gen_avx10_2_cvtbf162iubsv16bf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttbf162ibsv16bf                       (rtx, rtx);
extern rtx        gen_avx10_2_cvttbf162ibsv16bf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttbf162iubsv16bf                      (rtx, rtx);
extern rtx        gen_avx10_2_cvttbf162iubsv16bf_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtbf162ibsv8bf                         (rtx, rtx);
extern rtx        gen_avx10_2_cvtbf162ibsv8bf_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtbf162iubsv8bf                        (rtx, rtx);
extern rtx        gen_avx10_2_cvtbf162iubsv8bf_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttbf162ibsv8bf                        (rtx, rtx);
extern rtx        gen_avx10_2_cvttbf162ibsv8bf_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttbf162iubsv8bf                       (rtx, rtx);
extern rtx        gen_avx10_2_cvttbf162iubsv8bf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtph2ibsv32hf                          (rtx, rtx);
extern rtx        gen_avx10_2_cvtph2ibsv32hf_round                    (rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtph2ibsv32hf_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtph2ibsv32hf_mask_round               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtph2iubsv32hf                         (rtx, rtx);
extern rtx        gen_avx10_2_cvtph2iubsv32hf_round                   (rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtph2iubsv32hf_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtph2iubsv32hf_mask_round              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtph2ibsv16hf                          (rtx, rtx);
static inline rtx gen_avx10_2_cvtph2ibsv16hf_round                    (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtph2ibsv16hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtph2ibsv16hf_mask                     (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvtph2ibsv16hf_mask_round               (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtph2ibsv16hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtph2iubsv16hf                         (rtx, rtx);
static inline rtx gen_avx10_2_cvtph2iubsv16hf_round                   (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtph2iubsv16hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtph2iubsv16hf_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvtph2iubsv16hf_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtph2iubsv16hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtph2ibsv8hf                           (rtx, rtx);
static inline rtx gen_avx10_2_cvtph2ibsv8hf_round                     (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtph2ibsv8hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtph2ibsv8hf_mask                      (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvtph2ibsv8hf_mask_round                (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtph2ibsv8hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtph2iubsv8hf                          (rtx, rtx);
static inline rtx gen_avx10_2_cvtph2iubsv8hf_round                    (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtph2iubsv8hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtph2iubsv8hf_mask                     (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvtph2iubsv8hf_mask_round               (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtph2iubsv8hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttph2ibsv32hf                         (rtx, rtx);
extern rtx        gen_avx10_2_cvttph2ibsv32hf_round                   (rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttph2ibsv32hf_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttph2ibsv32hf_mask_round              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttph2iubsv32hf                        (rtx, rtx);
extern rtx        gen_avx10_2_cvttph2iubsv32hf_round                  (rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttph2iubsv32hf_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttph2iubsv32hf_mask_round             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttph2ibsv16hf                         (rtx, rtx);
static inline rtx gen_avx10_2_cvttph2ibsv16hf_round                   (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttph2ibsv16hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttph2ibsv16hf_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvttph2ibsv16hf_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttph2ibsv16hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttph2iubsv16hf                        (rtx, rtx);
static inline rtx gen_avx10_2_cvttph2iubsv16hf_round                  (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttph2iubsv16hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttph2iubsv16hf_mask                   (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvttph2iubsv16hf_mask_round             (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttph2iubsv16hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttph2ibsv8hf                          (rtx, rtx);
static inline rtx gen_avx10_2_cvttph2ibsv8hf_round                    (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttph2ibsv8hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttph2ibsv8hf_mask                     (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvttph2ibsv8hf_mask_round               (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttph2ibsv8hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttph2iubsv8hf                         (rtx, rtx);
static inline rtx gen_avx10_2_cvttph2iubsv8hf_round                   (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttph2iubsv8hf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttph2iubsv8hf_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvttph2iubsv8hf_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttph2iubsv8hf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtps2ibsv16sf                          (rtx, rtx);
extern rtx        gen_avx10_2_cvtps2ibsv16sf_round                    (rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtps2ibsv16sf_mask                     (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtps2ibsv16sf_mask_round               (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtps2iubsv16sf                         (rtx, rtx);
extern rtx        gen_avx10_2_cvtps2iubsv16sf_round                   (rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtps2iubsv16sf_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtps2iubsv16sf_mask_round              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvtps2ibsv8sf                           (rtx, rtx);
static inline rtx gen_avx10_2_cvtps2ibsv8sf_round                     (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtps2ibsv8sf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtps2ibsv8sf_mask                      (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvtps2ibsv8sf_mask_round                (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtps2ibsv8sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtps2iubsv8sf                          (rtx, rtx);
static inline rtx gen_avx10_2_cvtps2iubsv8sf_round                    (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtps2iubsv8sf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtps2iubsv8sf_mask                     (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvtps2iubsv8sf_mask_round               (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtps2iubsv8sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtps2ibsv4sf                           (rtx, rtx);
static inline rtx gen_avx10_2_cvtps2ibsv4sf_round                     (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtps2ibsv4sf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtps2ibsv4sf_mask                      (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvtps2ibsv4sf_mask_round                (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtps2ibsv4sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtps2iubsv4sf                          (rtx, rtx);
static inline rtx gen_avx10_2_cvtps2iubsv4sf_round                    (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtps2iubsv4sf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvtps2iubsv4sf_mask                     (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvtps2iubsv4sf_mask_round               (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvtps2iubsv4sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttps2ibsv16sf                         (rtx, rtx);
extern rtx        gen_avx10_2_cvttps2ibsv16sf_round                   (rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttps2ibsv16sf_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttps2ibsv16sf_mask_round              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttps2iubsv16sf                        (rtx, rtx);
extern rtx        gen_avx10_2_cvttps2iubsv16sf_round                  (rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttps2iubsv16sf_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttps2iubsv16sf_mask_round             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_cvttps2ibsv8sf                          (rtx, rtx);
static inline rtx gen_avx10_2_cvttps2ibsv8sf_round                    (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttps2ibsv8sf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttps2ibsv8sf_mask                     (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvttps2ibsv8sf_mask_round               (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttps2ibsv8sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttps2iubsv8sf                         (rtx, rtx);
static inline rtx gen_avx10_2_cvttps2iubsv8sf_round                   (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttps2iubsv8sf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttps2iubsv8sf_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvttps2iubsv8sf_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttps2iubsv8sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttps2ibsv4sf                          (rtx, rtx);
static inline rtx gen_avx10_2_cvttps2ibsv4sf_round                    (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttps2ibsv4sf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttps2ibsv4sf_mask                     (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvttps2ibsv4sf_mask_round               (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttps2ibsv4sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttps2iubsv4sf                         (rtx, rtx);
static inline rtx gen_avx10_2_cvttps2iubsv4sf_round                   (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttps2iubsv4sf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_cvttps2iubsv4sf_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_cvttps2iubsv4sf_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_cvttps2iubsv4sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2dqsv16sf                        (rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2dqsv16sf_round                  (rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2dqsv16sf_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2dqsv16sf_mask_round             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2udqsv16sf                       (rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2udqsv16sf_round                 (rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2udqsv16sf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2udqsv16sf_mask_round            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2dqsv8sf                         (rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2dqsv8sf_round                   (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2dqsv8sf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2dqsv8sf_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2dqsv8sf_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2dqsv8sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2udqsv8sf                        (rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2udqsv8sf_round                  (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2udqsv8sf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2udqsv8sf_mask                   (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2udqsv8sf_mask_round             (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2udqsv8sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2dqsv4sf                         (rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2dqsv4sf_round                   (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2dqsv4sf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2dqsv4sf_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2dqsv4sf_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2dqsv4sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2udqsv4sf                        (rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2udqsv4sf_round                  (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2udqsv4sf_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2udqsv4sf_mask                   (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2udqsv4sf_mask_round             (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2udqsv4sf_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2dqsv8df                         (rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2dqsv8df_round                   (rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2dqsv8df_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2dqsv8df_mask_round              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2udqsv8df                        (rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2udqsv8df_round                  (rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2udqsv8df_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2udqsv8df_mask_round             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2dqsv4df                         (rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2dqsv4df_round                   (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2dqsv4df_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2dqsv4df_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2dqsv4df_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2dqsv4df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2udqsv4df                        (rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2udqsv4df_round                  (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2udqsv4df_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2udqsv4df_mask                   (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2udqsv4df_mask_round             (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2udqsv4df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2dqsv2df                         (rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2dqsv2df_round                   (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2dqsv2df_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2dqsv2df_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2dqsv2df_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2dqsv2df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2udqsv2df                        (rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2udqsv2df_round                  (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2udqsv2df_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2udqsv2df_mask                   (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2udqsv2df_mask_round             (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2udqsv2df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2qqsv8df                         (rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2qqsv8df_round                   (rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2qqsv8df_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2qqsv8df_mask_round              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2uqqsv8df                        (rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2uqqsv8df_round                  (rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2uqqsv8df_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2uqqsv8df_mask_round             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttpd2qqsv4df                         (rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2qqsv4df_round                   (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2qqsv4df_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2qqsv4df_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2qqsv4df_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2qqsv4df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2uqqsv4df                        (rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2uqqsv4df_round                  (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2uqqsv4df_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2uqqsv4df_mask                   (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2uqqsv4df_mask_round             (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2uqqsv4df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2qqsv2df                         (rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2qqsv2df_round                   (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2qqsv2df_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2qqsv2df_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2qqsv2df_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2qqsv2df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2uqqsv2df                        (rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2uqqsv2df_round                  (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2uqqsv2df_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttpd2uqqsv2df_mask                   (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttpd2uqqsv2df_mask_round             (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttpd2uqqsv2df_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2qqsv8di                         (rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2qqsv8di_round                   (rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2qqsv8di_mask                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2qqsv8di_mask_round              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2uqqsv8di                        (rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2uqqsv8di_round                  (rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2uqqsv8di_mask                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2uqqsv8di_mask_round             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttps2qqsv4di                         (rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2qqsv4di_round                   (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2qqsv4di_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2qqsv4di_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2qqsv4di_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2qqsv4di_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2uqqsv4di                        (rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2uqqsv4di_round                  (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2uqqsv4di_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2uqqsv4di_mask                   (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2uqqsv4di_mask_round             (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2uqqsv4di_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2qqsv2di                         (rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2qqsv2di_round                   (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2qqsv2di_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2qqsv2di_mask                    (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2qqsv2di_mask_round              (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2qqsv2di_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2uqqsv2di                        (rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2uqqsv2di_round                  (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2uqqsv2di_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttps2uqqsv2di_mask                   (rtx, rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttps2uqqsv2di_mask_round             (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttps2uqqsv2di_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttsd2sissi                           (rtx, rtx);
extern rtx        gen_avx10_2_vcvttsd2sissi_round                     (rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttsd2usissi                          (rtx, rtx);
extern rtx        gen_avx10_2_vcvttsd2usissi_round                    (rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttsd2sisdi                           (rtx, rtx);
static inline rtx
gen_avx10_2_vcvttsd2sisdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx10_2_vcvttsd2sisdi_round                     (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttsd2sisdi_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_avx10_2_vcvttsd2usisdi                          (rtx, rtx);
static inline rtx
gen_avx10_2_vcvttsd2usisdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx10_2_vcvttsd2usisdi_round                    (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttsd2usisdi_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_vcvttss2sissi                           (rtx, rtx);
extern rtx        gen_avx10_2_vcvttss2sissi_round                     (rtx, rtx, rtx);
extern rtx        gen_avx10_2_vcvttss2usissi                          (rtx, rtx);
extern rtx        gen_avx10_2_vcvttss2usissi_round                    (rtx, rtx, rtx);
static inline rtx gen_avx10_2_vcvttss2sisdi                           (rtx, rtx);
static inline rtx
gen_avx10_2_vcvttss2sisdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx10_2_vcvttss2sisdi_round                     (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttss2sisdi_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_avx10_2_vcvttss2usisdi                          (rtx, rtx);
static inline rtx
gen_avx10_2_vcvttss2usisdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_avx10_2_vcvttss2usisdi_round                    (rtx, rtx, rtx);
static inline rtx
gen_avx10_2_vcvttss2usisdi_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_avx10_2_minmaxbf16_v32bf                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxbf16_v32bf_mask                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxbf16_v16bf                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxbf16_v16bf_mask                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxbf16_v8bf                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxbf16_v8bf_mask                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv32hf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv32hf_round                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv32hf_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv32hf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv16hf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv16hf_round                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv16hf_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv16hf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv8hf                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv8hf_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv8hf_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv8hf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv16sf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv16sf_round                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv16sf_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv16sf_mask_round                 (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv8sf                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv8sf_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv8sf_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv8sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv4sf                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv4sf_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv4sf_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv4sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv8df                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv8df_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv8df_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv8df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv4df                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv4df_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv4df_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv4df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv2df                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv2df_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv2df_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxpv2df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxsv8hf                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxsv8hf_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxsv8hf_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxsv8hf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxsv4sf                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxsv4sf_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxsv4sf_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxsv4sf_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxsv2df                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxsv2df_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxsv2df_round                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_minmaxsv2df_mask_round                  (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vmovrsbv64qi                            (rtx, rtx);
extern rtx        gen_avx10_2_vmovrsbv64qi_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vmovrsbv32qi                            (rtx, rtx);
extern rtx        gen_avx10_2_vmovrsbv32qi_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vmovrsbv16qi                            (rtx, rtx);
extern rtx        gen_avx10_2_vmovrsbv16qi_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vmovrswv32hi                            (rtx, rtx);
extern rtx        gen_avx10_2_vmovrswv32hi_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vmovrswv16hi                            (rtx, rtx);
extern rtx        gen_avx10_2_vmovrswv16hi_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vmovrswv8hi                             (rtx, rtx);
extern rtx        gen_avx10_2_vmovrswv8hi_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vmovrsdv16si                            (rtx, rtx);
extern rtx        gen_avx10_2_vmovrsdv16si_mask                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vmovrsdv8si                             (rtx, rtx);
extern rtx        gen_avx10_2_vmovrsdv8si_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vmovrsdv4si                             (rtx, rtx);
extern rtx        gen_avx10_2_vmovrsdv4si_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vmovrsqv8di                             (rtx, rtx);
extern rtx        gen_avx10_2_vmovrsqv8di_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vmovrsqv4di                             (rtx, rtx);
extern rtx        gen_avx10_2_vmovrsqv4di_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_vmovrsqv2di                             (rtx, rtx);
extern rtx        gen_avx10_2_vmovrsqv2di_mask                        (rtx, rtx, rtx, rtx);
extern rtx        gen_mfence_sse2                                     (rtx);
extern rtx        gen_mfence_nosse                                    (rtx);
extern rtx        gen_atomic_loaddi_fpu                               (rtx, rtx, rtx);
extern rtx        gen_atomic_storeqi_1                                (rtx, rtx, rtx);
extern rtx        gen_atomic_storehi_1                                (rtx, rtx, rtx);
extern rtx        gen_atomic_storesi_1                                (rtx, rtx, rtx);
static inline rtx gen_atomic_storedi_1                                (rtx, rtx, rtx);
static inline rtx
gen_atomic_storedi_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_atomic_storedi_fpu                              (rtx, rtx, rtx);
extern rtx        gen_loaddi_via_fpu                                  (rtx, rtx);
extern rtx        gen_storedi_via_fpu                                 (rtx, rtx);
extern rtx        gen_loaddi_via_sse                                  (rtx, rtx);
extern rtx        gen_storedi_via_sse                                 (rtx, rtx);
extern rtx        gen_atomic_compare_and_swapdi_doubleword            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_atomic_compare_and_swapti_doubleword            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_compare_and_swapti_doubleword(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_atomic_compare_and_swapqi_1                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_compare_and_swaphi_1                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_compare_and_swapsi_1                     (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_atomic_compare_and_swapdi_1                     (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_compare_and_swapdi_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_atomic_fetch_addqi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_addhi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_addsi                              (rtx, rtx, rtx, rtx);
static inline rtx gen_atomic_fetch_adddi                              (rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_fetch_adddi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_atomic_exchangeqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_exchangehi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_exchangesi                               (rtx, rtx, rtx, rtx);
static inline rtx gen_atomic_exchangedi                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_exchangedi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_rao_aandsi                                      (rtx, rtx);
extern rtx        gen_rao_aorsi                                       (rtx, rtx);
extern rtx        gen_rao_axorsi                                      (rtx, rtx);
extern rtx        gen_rao_aaddsi                                      (rtx, rtx);
static inline rtx gen_rao_aanddi                                      (rtx, rtx);
static inline rtx
gen_rao_aanddi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_rao_aordi                                       (rtx, rtx);
static inline rtx
gen_rao_aordi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_rao_axordi                                      (rtx, rtx);
static inline rtx
gen_rao_axordi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_rao_aadddi                                      (rtx, rtx);
static inline rtx
gen_rao_aadddi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_atomic_addqi                                    (rtx, rtx, rtx);
extern rtx        gen_atomic_addhi                                    (rtx, rtx, rtx);
extern rtx        gen_atomic_addsi                                    (rtx, rtx, rtx);
static inline rtx gen_atomic_adddi                                    (rtx, rtx, rtx);
static inline rtx
gen_atomic_adddi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_atomic_subqi                                    (rtx, rtx, rtx);
extern rtx        gen_atomic_subhi                                    (rtx, rtx, rtx);
extern rtx        gen_atomic_subsi                                    (rtx, rtx, rtx);
static inline rtx gen_atomic_subdi                                    (rtx, rtx, rtx);
static inline rtx
gen_atomic_subdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_atomic_andqi                                    (rtx, rtx, rtx);
extern rtx        gen_atomic_orqi                                     (rtx, rtx, rtx);
extern rtx        gen_atomic_xorqi                                    (rtx, rtx, rtx);
extern rtx        gen_atomic_andhi                                    (rtx, rtx, rtx);
extern rtx        gen_atomic_orhi                                     (rtx, rtx, rtx);
extern rtx        gen_atomic_xorhi                                    (rtx, rtx, rtx);
extern rtx        gen_atomic_andsi                                    (rtx, rtx, rtx);
extern rtx        gen_atomic_orsi                                     (rtx, rtx, rtx);
extern rtx        gen_atomic_xorsi                                    (rtx, rtx, rtx);
static inline rtx gen_atomic_anddi                                    (rtx, rtx, rtx);
static inline rtx
gen_atomic_anddi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_atomic_ordi                                     (rtx, rtx, rtx);
static inline rtx
gen_atomic_ordi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_atomic_xordi                                    (rtx, rtx, rtx);
static inline rtx
gen_atomic_xordi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_atomic_bit_test_and_sethi_1                     (rtx, rtx, rtx);
extern rtx        gen_atomic_bit_test_and_setsi_1                     (rtx, rtx, rtx);
static inline rtx gen_atomic_bit_test_and_setdi_1                     (rtx, rtx, rtx);
static inline rtx
gen_atomic_bit_test_and_setdi_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_atomic_bit_test_and_complementhi_1              (rtx, rtx, rtx);
extern rtx        gen_atomic_bit_test_and_complementsi_1              (rtx, rtx, rtx);
static inline rtx gen_atomic_bit_test_and_complementdi_1              (rtx, rtx, rtx);
static inline rtx
gen_atomic_bit_test_and_complementdi_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_atomic_bit_test_and_resethi_1                   (rtx, rtx, rtx);
extern rtx        gen_atomic_bit_test_and_resetsi_1                   (rtx, rtx, rtx);
static inline rtx gen_atomic_bit_test_and_resetdi_1                   (rtx, rtx, rtx);
static inline rtx
gen_atomic_bit_test_and_resetdi_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_atomic_add_fetch_cmp_0qi_1                      (rtx, rtx, rtx);
extern rtx        gen_atomic_add_fetch_cmp_0hi_1                      (rtx, rtx, rtx);
extern rtx        gen_atomic_add_fetch_cmp_0si_1                      (rtx, rtx, rtx);
static inline rtx gen_atomic_add_fetch_cmp_0di_1                      (rtx, rtx, rtx);
static inline rtx
gen_atomic_add_fetch_cmp_0di_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_atomic_sub_fetch_cmp_0qi_1                      (rtx, rtx, rtx);
extern rtx        gen_atomic_sub_fetch_cmp_0hi_1                      (rtx, rtx, rtx);
extern rtx        gen_atomic_sub_fetch_cmp_0si_1                      (rtx, rtx, rtx);
static inline rtx gen_atomic_sub_fetch_cmp_0di_1                      (rtx, rtx, rtx);
static inline rtx
gen_atomic_sub_fetch_cmp_0di_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_atomic_and_fetch_cmp_0qi_1                      (rtx, rtx, rtx);
extern rtx        gen_atomic_or_fetch_cmp_0qi_1                       (rtx, rtx, rtx);
extern rtx        gen_atomic_xor_fetch_cmp_0qi_1                      (rtx, rtx, rtx);
extern rtx        gen_atomic_and_fetch_cmp_0hi_1                      (rtx, rtx, rtx);
extern rtx        gen_atomic_or_fetch_cmp_0hi_1                       (rtx, rtx, rtx);
extern rtx        gen_atomic_xor_fetch_cmp_0hi_1                      (rtx, rtx, rtx);
extern rtx        gen_atomic_and_fetch_cmp_0si_1                      (rtx, rtx, rtx);
extern rtx        gen_atomic_or_fetch_cmp_0si_1                       (rtx, rtx, rtx);
extern rtx        gen_atomic_xor_fetch_cmp_0si_1                      (rtx, rtx, rtx);
static inline rtx gen_atomic_and_fetch_cmp_0di_1                      (rtx, rtx, rtx);
static inline rtx
gen_atomic_and_fetch_cmp_0di_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_atomic_or_fetch_cmp_0di_1                       (rtx, rtx, rtx);
static inline rtx
gen_atomic_or_fetch_cmp_0di_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_atomic_xor_fetch_cmp_0di_1                      (rtx, rtx, rtx);
static inline rtx
gen_atomic_xor_fetch_cmp_0di_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_cmpccxadd_si                                    (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_cmpccxadd_si(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
static inline rtx gen_cmpccxadd_di                                    (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_cmpccxadd_di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_cbranchqi4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchhi4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchsi4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchdi4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchti4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchoi4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchxi4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cstoreqi4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_cstorehi4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_cstoresi4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_cstoredi4                                       (rtx, rtx, rtx, rtx);
static inline rtx gen_cstoreti4                                       (rtx, rtx, rtx, rtx);
static inline rtx
gen_cstoreti4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_cmpqi_1                                         (rtx, rtx);
extern rtx        gen_cmphi_1                                         (rtx, rtx);
extern rtx        gen_cmpsi_1                                         (rtx, rtx);
static inline rtx gen_cmpdi_1                                         (rtx, rtx);
static inline rtx
gen_cmpdi_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_cmpqi_ext_3                                     (rtx, rtx);
extern rtx        gen_cbranchxf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cstorexf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchhf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchsf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchdf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchbf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cstorehf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_cstorebf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_cstoresf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_cstoredf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchcc4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cstoreccgc4                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_cstoreccgoc4                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_cstoreccno4                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_cstoreccgz4                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_cstorecca4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cstoreccc4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cstorecco4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cstoreccp4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cstoreccs4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cstoreccz4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cstorecc4                                       (rtx, rtx, rtx, rtx);
static inline rtx gen_reload_noff_store                               (rtx, rtx, rtx);
static inline rtx
gen_reload_noff_store(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_reload_noff_load                                (rtx, rtx, rtx);
static inline rtx
gen_reload_noff_load(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_movxi                                           (rtx, rtx);
extern rtx        gen_movoi                                           (rtx, rtx);
extern rtx        gen_movti                                           (rtx, rtx);
extern rtx        gen_movcdi                                          (rtx, rtx);
extern rtx        gen_movqi                                           (rtx, rtx);
extern rtx        gen_movhi                                           (rtx, rtx);
extern rtx        gen_movsi                                           (rtx, rtx);
extern rtx        gen_movdi                                           (rtx, rtx);
extern rtx        gen_movstrictqi                                     (rtx, rtx);
extern rtx        gen_movstricthi                                     (rtx, rtx);
extern rtx        gen_extvhi                                          (rtx, rtx, rtx, rtx);
extern rtx        gen_extvsi                                          (rtx, rtx, rtx, rtx);
extern rtx        gen_extzvhi                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_extzvsi                                         (rtx, rtx, rtx, rtx);
static inline rtx gen_extzvdi                                         (rtx, rtx, rtx, rtx);
static inline rtx
gen_extzvdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_insvhi                                          (rtx, rtx, rtx, rtx);
extern rtx        gen_insvsi                                          (rtx, rtx, rtx, rtx);
static inline rtx gen_insvdi                                          (rtx, rtx, rtx, rtx);
static inline rtx
gen_insvdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_movtf                                           (rtx, rtx);
extern rtx        gen_movhf                                           (rtx, rtx);
extern rtx        gen_movsf                                           (rtx, rtx);
extern rtx        gen_movdf                                           (rtx, rtx);
extern rtx        gen_movxf                                           (rtx, rtx);
extern rtx        gen_movbf                                           (rtx, rtx);
extern rtx        gen_zero_extendsidi2                                (rtx, rtx);
extern rtx        gen_zero_extendqisi2                                (rtx, rtx);
extern rtx        gen_zero_extendhisi2                                (rtx, rtx);
extern rtx        gen_zero_extendqihi2                                (rtx, rtx);
extern rtx        gen_extendsidi2                                     (rtx, rtx);
extern rtx        gen_extendsfdf2                                     (rtx, rtx);
extern rtx        gen_extendhfsf2                                     (rtx, rtx);
extern rtx        gen_extendhfdf2                                     (rtx, rtx);
extern rtx        gen_extendbfsf2                                     (rtx, rtx);
extern rtx        gen_extendsfxf2                                     (rtx, rtx);
extern rtx        gen_extenddfxf2                                     (rtx, rtx);
extern rtx        gen_truncsfhf2                                      (rtx, rtx);
extern rtx        gen_truncdfhf2                                      (rtx, rtx);
extern rtx        gen_fix_truncxfdi2                                  (rtx, rtx);
extern rtx        gen_fix_truncsfdi2                                  (rtx, rtx);
extern rtx        gen_fix_truncdfdi2                                  (rtx, rtx);
extern rtx        gen_fix_truncxfsi2                                  (rtx, rtx);
extern rtx        gen_fix_truncsfsi2                                  (rtx, rtx);
extern rtx        gen_fix_truncdfsi2                                  (rtx, rtx);
extern rtx        gen_fix_truncsfhi2                                  (rtx, rtx);
extern rtx        gen_fix_truncdfhi2                                  (rtx, rtx);
extern rtx        gen_fix_truncxfhi2                                  (rtx, rtx);
extern rtx        gen_fixuns_truncsfsi2                               (rtx, rtx);
extern rtx        gen_fixuns_truncdfsi2                               (rtx, rtx);
extern rtx        gen_fixuns_trunchfhi2                               (rtx, rtx);
extern rtx        gen_fixuns_truncsfhi2                               (rtx, rtx);
extern rtx        gen_fixuns_truncdfhi2                               (rtx, rtx);
extern rtx        gen_floatsisf2                                      (rtx, rtx);
extern rtx        gen_floatdisf2                                      (rtx, rtx);
extern rtx        gen_floatsidf2                                      (rtx, rtx);
extern rtx        gen_floatdidf2                                      (rtx, rtx);
extern rtx        gen_floatunsqisf2                                   (rtx, rtx);
extern rtx        gen_floatunshisf2                                   (rtx, rtx);
extern rtx        gen_floatunsqidf2                                   (rtx, rtx);
extern rtx        gen_floatunshidf2                                   (rtx, rtx);
extern rtx        gen_floatunssisf2                                   (rtx, rtx);
extern rtx        gen_floatunssidf2                                   (rtx, rtx);
extern rtx        gen_floatunssixf2                                   (rtx, rtx);
static inline rtx gen_floatunsdisf2                                   (rtx, rtx);
static inline rtx
gen_floatunsdisf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_floatunsdidf2                                   (rtx, rtx);
static inline rtx
gen_floatunsdidf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_addqi3                                          (rtx, rtx, rtx);
extern rtx        gen_addhi3                                          (rtx, rtx, rtx);
extern rtx        gen_addsi3                                          (rtx, rtx, rtx);
extern rtx        gen_adddi3                                          (rtx, rtx, rtx);
static inline rtx gen_addti3                                          (rtx, rtx, rtx);
static inline rtx
gen_addti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_addqi_ext_1                                     (rtx, rtx, rtx);
extern rtx        gen_addvqi4                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_addvhi4                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_addvsi4                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_addvdi4                                         (rtx, rtx, rtx, rtx);
static inline rtx gen_addvti4                                         (rtx, rtx, rtx, rtx);
static inline rtx
gen_addvti4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_uaddvqi4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_uaddvhi4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_uaddvsi4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_uaddvdi4                                        (rtx, rtx, rtx, rtx);
static inline rtx gen_uaddvti4                                        (rtx, rtx, rtx, rtx);
static inline rtx
gen_uaddvti4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_subqi3                                          (rtx, rtx, rtx);
extern rtx        gen_subhi3                                          (rtx, rtx, rtx);
extern rtx        gen_subsi3                                          (rtx, rtx, rtx);
extern rtx        gen_subdi3                                          (rtx, rtx, rtx);
static inline rtx gen_subti3                                          (rtx, rtx, rtx);
static inline rtx
gen_subti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_subvqi4                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_subvhi4                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_subvsi4                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_subvdi4                                         (rtx, rtx, rtx, rtx);
static inline rtx gen_subvti4                                         (rtx, rtx, rtx, rtx);
static inline rtx
gen_subvti4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_usubvqi4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_usubvhi4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_usubvsi4                                        (rtx, rtx, rtx, rtx);
static inline rtx gen_usubvdi4                                        (rtx, rtx, rtx, rtx);
static inline rtx
gen_usubvdi4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_subqi_3                                         (rtx, rtx, rtx);
extern rtx        gen_subhi_3                                         (rtx, rtx, rtx);
extern rtx        gen_subsi_3                                         (rtx, rtx, rtx);
static inline rtx gen_subdi_3                                         (rtx, rtx, rtx);
static inline rtx
gen_subdi_3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_addcarrysi_0                                    (rtx, rtx, rtx);
extern rtx        gen_addcarrydi_0                                    (rtx, rtx, rtx);
extern rtx        gen_subborrowsi_0                                   (rtx, rtx, rtx);
extern rtx        gen_subborrowdi_0                                   (rtx, rtx, rtx);
extern rtx        gen_uaddcsi5                                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_uaddcdi5                                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_uaddcdi5(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_usubcsi5                                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_usubcdi5                                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_usubcdi5(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_addqi3_cconly_overflow                          (rtx, rtx);
extern rtx        gen_usaddqi3                                        (rtx, rtx, rtx);
extern rtx        gen_usaddhi3                                        (rtx, rtx, rtx);
extern rtx        gen_usaddsi3                                        (rtx, rtx, rtx);
static inline rtx gen_usadddi3                                        (rtx, rtx, rtx);
static inline rtx
gen_usadddi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_ussubqi3                                        (rtx, rtx, rtx);
extern rtx        gen_ussubhi3                                        (rtx, rtx, rtx);
extern rtx        gen_ussubsi3                                        (rtx, rtx, rtx);
static inline rtx gen_ussubdi3                                        (rtx, rtx, rtx);
static inline rtx
gen_ussubdi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_ustruncdiqi2                                    (rtx, rtx);
static inline rtx
gen_ustruncdiqi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_ustruncdihi2                                    (rtx, rtx);
static inline rtx
gen_ustruncdihi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_ustruncdisi2                                    (rtx, rtx);
static inline rtx
gen_ustruncdisi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_ustruncsiqi2                                    (rtx, rtx);
extern rtx        gen_ustruncsihi2                                    (rtx, rtx);
extern rtx        gen_ustrunchiqi2                                    (rtx, rtx);
extern rtx        gen_addxf3                                          (rtx, rtx, rtx);
extern rtx        gen_subxf3                                          (rtx, rtx, rtx);
extern rtx        gen_addhf3                                          (rtx, rtx, rtx);
extern rtx        gen_subhf3                                          (rtx, rtx, rtx);
extern rtx        gen_addsf3                                          (rtx, rtx, rtx);
extern rtx        gen_subsf3                                          (rtx, rtx, rtx);
extern rtx        gen_adddf3                                          (rtx, rtx, rtx);
extern rtx        gen_subdf3                                          (rtx, rtx, rtx);
extern rtx        gen_mulhi3                                          (rtx, rtx, rtx);
extern rtx        gen_mulsi3                                          (rtx, rtx, rtx);
static inline rtx gen_muldi3                                          (rtx, rtx, rtx);
static inline rtx
gen_muldi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mulqi3                                          (rtx, rtx, rtx);
extern rtx        gen_mulvhi4                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_mulvsi4                                         (rtx, rtx, rtx, rtx);
static inline rtx gen_mulvdi4                                         (rtx, rtx, rtx, rtx);
static inline rtx
gen_mulvdi4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_umulvhi4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_umulvsi4                                        (rtx, rtx, rtx, rtx);
static inline rtx gen_umulvdi4                                        (rtx, rtx, rtx, rtx);
static inline rtx
gen_umulvdi4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_mulvqi4                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_umulvqi4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_mulsidi3                                        (rtx, rtx, rtx);
extern rtx        gen_umulsidi3                                       (rtx, rtx, rtx);
static inline rtx gen_mulditi3                                        (rtx, rtx, rtx);
static inline rtx
gen_mulditi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_umulditi3                                       (rtx, rtx, rtx);
static inline rtx
gen_umulditi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mulqihi3                                        (rtx, rtx, rtx);
extern rtx        gen_umulqihi3                                       (rtx, rtx, rtx);
extern rtx        gen_mulxf3                                          (rtx, rtx, rtx);
extern rtx        gen_mulhf3                                          (rtx, rtx, rtx);
extern rtx        gen_mulsf3                                          (rtx, rtx, rtx);
extern rtx        gen_muldf3                                          (rtx, rtx, rtx);
extern rtx        gen_divxf3                                          (rtx, rtx, rtx);
extern rtx        gen_divhf3                                          (rtx, rtx, rtx);
extern rtx        gen_divsf3                                          (rtx, rtx, rtx);
extern rtx        gen_divdf3                                          (rtx, rtx, rtx);
extern rtx        gen_divmodhi4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_udivmodhi4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_divmodsi4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_udivmodsi4                                      (rtx, rtx, rtx, rtx);
static inline rtx gen_divmoddi4                                       (rtx, rtx, rtx, rtx);
static inline rtx
gen_divmoddi4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_udivmoddi4                                      (rtx, rtx, rtx, rtx);
static inline rtx
gen_udivmoddi4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_divmodqi4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_udivmodqi4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_testsi_ccno_1                                   (rtx, rtx);
static inline rtx gen_testdi_ccno_1                                   (rtx, rtx);
static inline rtx
gen_testdi_ccno_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_testqi_ccz_1                                    (rtx, rtx);
extern rtx        gen_testqi_ext_1_ccno                               (rtx, rtx);
extern rtx        gen_andqi3                                          (rtx, rtx, rtx);
extern rtx        gen_andhi3                                          (rtx, rtx, rtx);
extern rtx        gen_andsi3                                          (rtx, rtx, rtx);
extern rtx        gen_anddi3                                          (rtx, rtx, rtx);
static inline rtx gen_andti3                                          (rtx, rtx, rtx);
static inline rtx
gen_andti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_andqi_ext_1                                     (rtx, rtx, rtx);
extern rtx        gen_iorqi3                                          (rtx, rtx, rtx);
extern rtx        gen_xorqi3                                          (rtx, rtx, rtx);
extern rtx        gen_iorhi3                                          (rtx, rtx, rtx);
extern rtx        gen_xorhi3                                          (rtx, rtx, rtx);
extern rtx        gen_iorsi3                                          (rtx, rtx, rtx);
extern rtx        gen_xorsi3                                          (rtx, rtx, rtx);
extern rtx        gen_iordi3                                          (rtx, rtx, rtx);
extern rtx        gen_xordi3                                          (rtx, rtx, rtx);
static inline rtx gen_iorti3                                          (rtx, rtx, rtx);
static inline rtx
gen_iorti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xorti3                                          (rtx, rtx, rtx);
static inline rtx
gen_xorti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_xorqi_ext_1_cc                                  (rtx, rtx, rtx);
extern rtx        gen_negqi2                                          (rtx, rtx);
extern rtx        gen_neghi2                                          (rtx, rtx);
extern rtx        gen_negsi2                                          (rtx, rtx);
extern rtx        gen_negdi2                                          (rtx, rtx);
static inline rtx gen_negti2                                          (rtx, rtx);
static inline rtx
gen_negti2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_x86_negsi_ccc                                   (rtx, rtx);
static inline rtx gen_x86_negdi_ccc                                   (rtx, rtx);
static inline rtx
gen_x86_negdi_ccc(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_negvqi3                                         (rtx, rtx, rtx);
extern rtx        gen_negvhi3                                         (rtx, rtx, rtx);
extern rtx        gen_negvsi3                                         (rtx, rtx, rtx);
static inline rtx gen_negvdi3                                         (rtx, rtx, rtx);
static inline rtx
gen_negvdi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_absqi2                                          (rtx, rtx);
extern rtx        gen_abshi2                                          (rtx, rtx);
extern rtx        gen_abssi2                                          (rtx, rtx);
extern rtx        gen_absdi2                                          (rtx, rtx);
static inline rtx gen_absti2                                          (rtx, rtx);
static inline rtx
gen_absti2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_abstf2                                          (rtx, rtx);
extern rtx        gen_negtf2                                          (rtx, rtx);
extern rtx        gen_abshf2                                          (rtx, rtx);
extern rtx        gen_neghf2                                          (rtx, rtx);
extern rtx        gen_abssf2                                          (rtx, rtx);
extern rtx        gen_negsf2                                          (rtx, rtx);
extern rtx        gen_absdf2                                          (rtx, rtx);
extern rtx        gen_negdf2                                          (rtx, rtx);
extern rtx        gen_absxf2                                          (rtx, rtx);
extern rtx        gen_negxf2                                          (rtx, rtx);
extern rtx        gen_copysignhf3                                     (rtx, rtx, rtx);
extern rtx        gen_copysignsf3                                     (rtx, rtx, rtx);
extern rtx        gen_copysigndf3                                     (rtx, rtx, rtx);
extern rtx        gen_copysigntf3                                     (rtx, rtx, rtx);
extern rtx        gen_xorsignhf3                                      (rtx, rtx, rtx);
extern rtx        gen_xorsignsf3                                      (rtx, rtx, rtx);
extern rtx        gen_xorsigndf3                                      (rtx, rtx, rtx);
extern rtx        gen_one_cmplqi2                                     (rtx, rtx);
extern rtx        gen_one_cmplhi2                                     (rtx, rtx);
extern rtx        gen_one_cmplsi2                                     (rtx, rtx);
extern rtx        gen_one_cmpldi2                                     (rtx, rtx);
static inline rtx gen_one_cmplti2                                     (rtx, rtx);
static inline rtx
gen_one_cmplti2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_ashlqi3                                         (rtx, rtx, rtx);
extern rtx        gen_ashlhi3                                         (rtx, rtx, rtx);
extern rtx        gen_ashlsi3                                         (rtx, rtx, rtx);
extern rtx        gen_ashldi3                                         (rtx, rtx, rtx);
static inline rtx gen_ashlti3                                         (rtx, rtx, rtx);
static inline rtx
gen_ashlti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_x86_shiftsi_adj_1                               (rtx, rtx, rtx, rtx);
static inline rtx gen_x86_shiftdi_adj_1                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_x86_shiftdi_adj_1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_x86_shiftsi_adj_2                               (rtx, rtx, rtx);
static inline rtx gen_x86_shiftdi_adj_2                               (rtx, rtx, rtx);
static inline rtx
gen_x86_shiftdi_adj_2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_lshrqi3                                         (rtx, rtx, rtx);
extern rtx        gen_ashrqi3                                         (rtx, rtx, rtx);
extern rtx        gen_lshrhi3                                         (rtx, rtx, rtx);
extern rtx        gen_ashrhi3                                         (rtx, rtx, rtx);
extern rtx        gen_lshrsi3                                         (rtx, rtx, rtx);
extern rtx        gen_ashrsi3                                         (rtx, rtx, rtx);
extern rtx        gen_lshrdi3                                         (rtx, rtx, rtx);
extern rtx        gen_ashrdi3                                         (rtx, rtx, rtx);
static inline rtx gen_lshrti3                                         (rtx, rtx, rtx);
static inline rtx
gen_lshrti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_ashrti3                                         (rtx, rtx, rtx);
static inline rtx
gen_ashrti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_x86_shiftsi_adj_3                               (rtx, rtx, rtx);
static inline rtx gen_x86_shiftdi_adj_3                               (rtx, rtx, rtx);
static inline rtx
gen_x86_shiftdi_adj_3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_rotlti3                                         (rtx, rtx, rtx);
static inline rtx
gen_rotlti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_rotrti3                                         (rtx, rtx, rtx);
static inline rtx
gen_rotrti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_rotldi3                                         (rtx, rtx, rtx);
extern rtx        gen_rotrdi3                                         (rtx, rtx, rtx);
extern rtx        gen_rotlqi3                                         (rtx, rtx, rtx);
extern rtx        gen_rotrqi3                                         (rtx, rtx, rtx);
extern rtx        gen_rotlhi3                                         (rtx, rtx, rtx);
extern rtx        gen_rotrhi3                                         (rtx, rtx, rtx);
extern rtx        gen_rotlsi3                                         (rtx, rtx, rtx);
extern rtx        gen_rotrsi3                                         (rtx, rtx, rtx);
extern rtx        gen_setcc_si_slp                                    (rtx, rtx, rtx);
extern rtx        gen_indirect_jump                                   (rtx);
extern rtx        gen_tablejump                                       (rtx, rtx);
extern rtx        gen_call                                            (rtx, rtx, rtx);
extern rtx        gen_sibcall                                         (rtx, rtx, rtx);
extern rtx        gen_call_pop                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_call_value                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_sibcall_value                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_call_value_pop                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_untyped_call                                    (rtx, rtx, rtx);
extern rtx        gen_memory_blockage                                 (void);
extern rtx        gen_return                                          (void);
extern rtx        gen_simple_return                                   (void);
extern rtx        gen_simple_return_indirect_internal                 (rtx);
extern rtx        gen_prologue                                        (void);
extern rtx        gen_set_got                                         (rtx);
extern rtx        gen_set_got_labelled                                (rtx, rtx);
extern rtx        gen_epilogue                                        (void);
extern rtx        gen_sibcall_epilogue                                (void);
extern rtx        gen_eh_return                                       (rtx);
extern rtx        gen_leave_si                                        (void);
extern rtx        gen_leave_di                                        (void);
extern rtx        gen_split_stack_prologue                            (void);
extern rtx        gen_split_stack_space_check                         (rtx, rtx);
extern rtx        gen_ffssi2                                          (rtx, rtx);
static inline rtx gen_ffsdi2                                          (rtx, rtx);
static inline rtx
gen_ffsdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_clzsi2                                          (rtx, rtx);
static inline rtx gen_clzdi2                                          (rtx, rtx);
static inline rtx
gen_clzdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_bmi2_bzhi_si3                                   (rtx, rtx, rtx);
static inline rtx gen_bmi2_bzhi_di3                                   (rtx, rtx, rtx);
static inline rtx
gen_bmi2_bzhi_di3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_bswapdi2                                        (rtx, rtx);
static inline rtx
gen_bswapdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_bswapsi2                                        (rtx, rtx);
extern rtx        gen_bswaphi2                                        (rtx, rtx);
extern rtx        gen_paritydi2                                       (rtx, rtx);
extern rtx        gen_paritysi2                                       (rtx, rtx);
extern rtx        gen_parityhi2                                       (rtx, rtx);
extern rtx        gen_parityqi2                                       (rtx, rtx);
extern rtx        gen_tls_global_dynamic_32                           (rtx, rtx, rtx, rtx);
static inline rtx gen_tls_global_dynamic_64_si                        (rtx, rtx, rtx, rtx);
static inline rtx
gen_tls_global_dynamic_64_si(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_tls_global_dynamic_64_di                        (rtx, rtx, rtx, rtx);
static inline rtx
gen_tls_global_dynamic_64_di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_tls_local_dynamic_base_32                       (rtx, rtx, rtx);
static inline rtx gen_tls_local_dynamic_base_64_si                    (rtx, rtx, rtx);
static inline rtx
gen_tls_local_dynamic_base_64_si(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_tls_local_dynamic_base_64_di                    (rtx, rtx, rtx);
static inline rtx
gen_tls_local_dynamic_base_64_di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_get_thread_pointersi                            (rtx);
extern rtx        gen_get_thread_pointerdi                            (rtx);
extern rtx        gen_tls_dynamic_gnu2_32                             (rtx, rtx, rtx);
static inline rtx gen_tls_dynamic_gnu2_64_si                          (rtx, rtx);
static inline rtx
gen_tls_dynamic_gnu2_64_si(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_tls_dynamic_gnu2_64_di                          (rtx, rtx);
static inline rtx
gen_tls_dynamic_gnu2_64_di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_rsqrtsf2                                        (rtx, rtx);
extern rtx        gen_sqrtsf2                                         (rtx, rtx);
extern rtx        gen_sqrtdf2                                         (rtx, rtx);
extern rtx        gen_hypotsf3                                        (rtx, rtx, rtx);
extern rtx        gen_hypotdf3                                        (rtx, rtx, rtx);
extern rtx        gen_fmodxf3                                         (rtx, rtx, rtx);
extern rtx        gen_fmodsf3                                         (rtx, rtx, rtx);
extern rtx        gen_fmoddf3                                         (rtx, rtx, rtx);
extern rtx        gen_remainderxf3                                    (rtx, rtx, rtx);
extern rtx        gen_remaindersf3                                    (rtx, rtx, rtx);
extern rtx        gen_remainderdf3                                    (rtx, rtx, rtx);
extern rtx        gen_sinsf2                                          (rtx, rtx);
extern rtx        gen_cossf2                                          (rtx, rtx);
extern rtx        gen_sindf2                                          (rtx, rtx);
extern rtx        gen_cosdf2                                          (rtx, rtx);
extern rtx        gen_sincossf3                                       (rtx, rtx, rtx);
extern rtx        gen_sincosdf3                                       (rtx, rtx, rtx);
extern rtx        gen_tanxf2                                          (rtx, rtx);
extern rtx        gen_tansf2                                          (rtx, rtx);
extern rtx        gen_tandf2                                          (rtx, rtx);
extern rtx        gen_atan2sf3                                        (rtx, rtx, rtx);
extern rtx        gen_atan2df3                                        (rtx, rtx, rtx);
extern rtx        gen_atanxf2                                         (rtx, rtx);
extern rtx        gen_atansf2                                         (rtx, rtx);
extern rtx        gen_atandf2                                         (rtx, rtx);
extern rtx        gen_asinxf2                                         (rtx, rtx);
extern rtx        gen_asinsf2                                         (rtx, rtx);
extern rtx        gen_asindf2                                         (rtx, rtx);
extern rtx        gen_acosxf2                                         (rtx, rtx);
extern rtx        gen_acossf2                                         (rtx, rtx);
extern rtx        gen_acosdf2                                         (rtx, rtx);
extern rtx        gen_sinhxf2                                         (rtx, rtx);
extern rtx        gen_sinhsf2                                         (rtx, rtx);
extern rtx        gen_sinhdf2                                         (rtx, rtx);
extern rtx        gen_coshxf2                                         (rtx, rtx);
extern rtx        gen_coshsf2                                         (rtx, rtx);
extern rtx        gen_coshdf2                                         (rtx, rtx);
extern rtx        gen_tanhxf2                                         (rtx, rtx);
extern rtx        gen_tanhsf2                                         (rtx, rtx);
extern rtx        gen_tanhdf2                                         (rtx, rtx);
extern rtx        gen_asinhxf2                                        (rtx, rtx);
extern rtx        gen_asinhsf2                                        (rtx, rtx);
extern rtx        gen_asinhdf2                                        (rtx, rtx);
extern rtx        gen_acoshxf2                                        (rtx, rtx);
extern rtx        gen_acoshsf2                                        (rtx, rtx);
extern rtx        gen_acoshdf2                                        (rtx, rtx);
extern rtx        gen_atanhxf2                                        (rtx, rtx);
extern rtx        gen_atanhsf2                                        (rtx, rtx);
extern rtx        gen_atanhdf2                                        (rtx, rtx);
extern rtx        gen_logxf2                                          (rtx, rtx);
extern rtx        gen_logsf2                                          (rtx, rtx);
extern rtx        gen_logdf2                                          (rtx, rtx);
extern rtx        gen_log10xf2                                        (rtx, rtx);
extern rtx        gen_log10sf2                                        (rtx, rtx);
extern rtx        gen_log10df2                                        (rtx, rtx);
extern rtx        gen_log2xf2                                         (rtx, rtx);
extern rtx        gen_log2sf2                                         (rtx, rtx);
extern rtx        gen_log2df2                                         (rtx, rtx);
extern rtx        gen_log1pxf2                                        (rtx, rtx);
extern rtx        gen_log1psf2                                        (rtx, rtx);
extern rtx        gen_log1pdf2                                        (rtx, rtx);
extern rtx        gen_logbxf2                                         (rtx, rtx);
extern rtx        gen_logbsf2                                         (rtx, rtx);
extern rtx        gen_logbdf2                                         (rtx, rtx);
extern rtx        gen_ilogbxf2                                        (rtx, rtx);
extern rtx        gen_ilogbsf2                                        (rtx, rtx);
extern rtx        gen_ilogbdf2                                        (rtx, rtx);
extern rtx        gen_expNcorexf3                                     (rtx, rtx, rtx);
extern rtx        gen_expxf2                                          (rtx, rtx);
extern rtx        gen_expsf2                                          (rtx, rtx);
extern rtx        gen_expdf2                                          (rtx, rtx);
extern rtx        gen_exp10xf2                                        (rtx, rtx);
extern rtx        gen_exp10sf2                                        (rtx, rtx);
extern rtx        gen_exp10df2                                        (rtx, rtx);
extern rtx        gen_exp2xf2                                         (rtx, rtx);
extern rtx        gen_exp2sf2                                         (rtx, rtx);
extern rtx        gen_exp2df2                                         (rtx, rtx);
extern rtx        gen_expm1xf2                                        (rtx, rtx);
extern rtx        gen_expm1sf2                                        (rtx, rtx);
extern rtx        gen_expm1df2                                        (rtx, rtx);
extern rtx        gen_ldexpxf3                                        (rtx, rtx, rtx);
extern rtx        gen_ldexpsf3                                        (rtx, rtx, rtx);
extern rtx        gen_ldexpdf3                                        (rtx, rtx, rtx);
extern rtx        gen_scalbxf3                                        (rtx, rtx, rtx);
extern rtx        gen_scalbsf3                                        (rtx, rtx, rtx);
extern rtx        gen_scalbdf3                                        (rtx, rtx, rtx);
extern rtx        gen_significandxf2                                  (rtx, rtx);
extern rtx        gen_significandsf2                                  (rtx, rtx);
extern rtx        gen_significanddf2                                  (rtx, rtx);
extern rtx        gen_rinthf2                                         (rtx, rtx);
extern rtx        gen_rintsf2                                         (rtx, rtx);
extern rtx        gen_rintdf2                                         (rtx, rtx);
extern rtx        gen_nearbyintxf2                                    (rtx, rtx);
extern rtx        gen_nearbyinthf2                                    (rtx, rtx);
extern rtx        gen_nearbyintsf2                                    (rtx, rtx);
extern rtx        gen_nearbyintdf2                                    (rtx, rtx);
extern rtx        gen_roundhf2                                        (rtx, rtx);
extern rtx        gen_roundsf2                                        (rtx, rtx);
extern rtx        gen_rounddf2                                        (rtx, rtx);
extern rtx        gen_roundxf2                                        (rtx, rtx);
extern rtx        gen_lroundhfhi2                                     (rtx, rtx);
extern rtx        gen_lroundhfsi2                                     (rtx, rtx);
static inline rtx gen_lroundhfdi2                                     (rtx, rtx);
static inline rtx
gen_lroundhfdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_lrinthfsi2                                      (rtx, rtx);
static inline rtx gen_lrinthfdi2                                      (rtx, rtx);
static inline rtx
gen_lrinthfdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_lrintsfsi2                                      (rtx, rtx);
static inline rtx gen_lrintsfdi2                                      (rtx, rtx);
static inline rtx
gen_lrintsfdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_lrintdfsi2                                      (rtx, rtx);
static inline rtx gen_lrintdfdi2                                      (rtx, rtx);
static inline rtx
gen_lrintdfdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_lroundsfhi2                                     (rtx, rtx);
extern rtx        gen_lrounddfhi2                                     (rtx, rtx);
extern rtx        gen_lroundxfhi2                                     (rtx, rtx);
extern rtx        gen_lroundsfsi2                                     (rtx, rtx);
extern rtx        gen_lrounddfsi2                                     (rtx, rtx);
extern rtx        gen_lroundxfsi2                                     (rtx, rtx);
extern rtx        gen_lroundsfdi2                                     (rtx, rtx);
extern rtx        gen_lrounddfdi2                                     (rtx, rtx);
extern rtx        gen_lroundxfdi2                                     (rtx, rtx);
extern rtx        gen_roundevenxf2                                    (rtx, rtx);
extern rtx        gen_floorxf2                                        (rtx, rtx);
extern rtx        gen_ceilxf2                                         (rtx, rtx);
extern rtx        gen_btruncxf2                                       (rtx, rtx);
extern rtx        gen_roundevenhf2                                    (rtx, rtx);
extern rtx        gen_floorhf2                                        (rtx, rtx);
extern rtx        gen_ceilhf2                                         (rtx, rtx);
extern rtx        gen_btrunchf2                                       (rtx, rtx);
extern rtx        gen_roundevensf2                                    (rtx, rtx);
extern rtx        gen_floorsf2                                        (rtx, rtx);
extern rtx        gen_ceilsf2                                         (rtx, rtx);
extern rtx        gen_btruncsf2                                       (rtx, rtx);
extern rtx        gen_roundevendf2                                    (rtx, rtx);
extern rtx        gen_floordf2                                        (rtx, rtx);
extern rtx        gen_ceildf2                                         (rtx, rtx);
extern rtx        gen_btruncdf2                                       (rtx, rtx);
extern rtx        gen_lfloorxfhi2                                     (rtx, rtx);
extern rtx        gen_lceilxfhi2                                      (rtx, rtx);
extern rtx        gen_lfloorxfsi2                                     (rtx, rtx);
extern rtx        gen_lceilxfsi2                                      (rtx, rtx);
extern rtx        gen_lfloorxfdi2                                     (rtx, rtx);
extern rtx        gen_lceilxfdi2                                      (rtx, rtx);
extern rtx        gen_lfloorhfsi2                                     (rtx, rtx);
extern rtx        gen_lceilhfsi2                                      (rtx, rtx);
static inline rtx gen_lfloorhfdi2                                     (rtx, rtx);
static inline rtx
gen_lfloorhfdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_lceilhfdi2                                      (rtx, rtx);
static inline rtx
gen_lceilhfdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_lfloorsfsi2                                     (rtx, rtx);
extern rtx        gen_lceilsfsi2                                      (rtx, rtx);
static inline rtx gen_lfloorsfdi2                                     (rtx, rtx);
static inline rtx
gen_lfloorsfdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_lceilsfdi2                                      (rtx, rtx);
static inline rtx
gen_lceilsfdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_lfloordfsi2                                     (rtx, rtx);
extern rtx        gen_lceildfsi2                                      (rtx, rtx);
static inline rtx gen_lfloordfdi2                                     (rtx, rtx);
static inline rtx
gen_lfloordfdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_lceildfdi2                                      (rtx, rtx);
static inline rtx
gen_lceildfdi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_signbittf2                                      (rtx, rtx);
extern rtx        gen_signbitxf2                                      (rtx, rtx);
extern rtx        gen_signbitdf2                                      (rtx, rtx);
extern rtx        gen_signbitsf2                                      (rtx, rtx);
extern rtx        gen_cpymemsi                                        (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_cpymemdi                                        (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_cpymemdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g), rtx ARG_UNUSED (h), rtx ARG_UNUSED (i))
{
  return 0;
}
extern rtx        gen_strmov                                          (rtx, rtx, rtx, rtx);
extern rtx        gen_strmov_singleop                                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_rep_mov                                         (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_setmemsi                                        (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_setmemdi                                        (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_setmemdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g), rtx ARG_UNUSED (h), rtx ARG_UNUSED (i))
{
  return 0;
}
extern rtx        gen_strset                                          (rtx, rtx, rtx);
extern rtx        gen_strset_singleop                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_rep_stos                                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cmpmemsi                                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cmpstrnsi                                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cmpintqi                                        (rtx);
extern rtx        gen_cmpstrnqi_nz_1                                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cmpstrnqi_1                                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_strlensi                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_strlendi                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_strlenqi_1                                      (rtx, rtx, rtx);
extern rtx        gen_movqicc                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_movhicc                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_movsicc                                         (rtx, rtx, rtx, rtx);
static inline rtx gen_movdicc                                         (rtx, rtx, rtx, rtx);
static inline rtx
gen_movdicc(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_x86_movsicc_0_m1                                (rtx, rtx, rtx);
static inline rtx gen_x86_movdicc_0_m1                                (rtx, rtx, rtx);
static inline rtx
gen_x86_movdicc_0_m1(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_x86_movqicc_0_m1_neg                            (rtx);
extern rtx        gen_x86_movhicc_0_m1_neg                            (rtx);
extern rtx        gen_x86_movsicc_0_m1_neg                            (rtx);
static inline rtx gen_x86_movdicc_0_m1_neg                            (rtx);
static inline rtx
gen_x86_movdicc_0_m1_neg(rtx ARG_UNUSED (a))
{
  return 0;
}
extern rtx        gen_movhfcc                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_movsfcc                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_movdfcc                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_movxfcc                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_addqicc                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_addhicc                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_addsicc                                         (rtx, rtx, rtx, rtx);
static inline rtx gen_adddicc                                         (rtx, rtx, rtx, rtx);
static inline rtx
gen_adddicc(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_smaxqi3                                         (rtx, rtx, rtx);
extern rtx        gen_sminqi3                                         (rtx, rtx, rtx);
extern rtx        gen_umaxqi3                                         (rtx, rtx, rtx);
extern rtx        gen_uminqi3                                         (rtx, rtx, rtx);
extern rtx        gen_smaxhi3                                         (rtx, rtx, rtx);
extern rtx        gen_sminhi3                                         (rtx, rtx, rtx);
extern rtx        gen_umaxhi3                                         (rtx, rtx, rtx);
extern rtx        gen_uminhi3                                         (rtx, rtx, rtx);
extern rtx        gen_smaxsi3                                         (rtx, rtx, rtx);
extern rtx        gen_sminsi3                                         (rtx, rtx, rtx);
extern rtx        gen_umaxsi3                                         (rtx, rtx, rtx);
extern rtx        gen_uminsi3                                         (rtx, rtx, rtx);
extern rtx        gen_smaxdi3                                         (rtx, rtx, rtx);
extern rtx        gen_smindi3                                         (rtx, rtx, rtx);
extern rtx        gen_umaxdi3                                         (rtx, rtx, rtx);
extern rtx        gen_umindi3                                         (rtx, rtx, rtx);
static inline rtx gen_smaxti3                                         (rtx, rtx, rtx);
static inline rtx
gen_smaxti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_sminti3                                         (rtx, rtx, rtx);
static inline rtx
gen_sminti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_umaxti3                                         (rtx, rtx, rtx);
static inline rtx
gen_umaxti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_uminti3                                         (rtx, rtx, rtx);
static inline rtx
gen_uminti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_allocate_stack                                  (rtx, rtx);
extern rtx        gen_probe_stack                                     (rtx);
extern rtx        gen_builtin_setjmp_receiver                         (rtx);
extern rtx        gen_save_stack_nonlocal                             (rtx, rtx);
extern rtx        gen_restore_stack_nonlocal                          (rtx, rtx);
extern rtx        gen_stack_protect_set                               (rtx, rtx);
extern rtx        gen_stack_protect_test                              (rtx, rtx, rtx);
extern rtx        gen_prefetch                                        (rtx, rtx, rtx);
extern rtx        gen_pause                                           (void);
extern rtx        gen_xbegin                                          (rtx);
extern rtx        gen_xtest                                           (rtx);
extern rtx        gen_rdpkru                                          (rtx);
extern rtx        gen_wrpkru                                          (rtx);
extern rtx        gen_spaceshipsf4                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_spaceshipdf4                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_spaceshipxf4                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_spaceshipqi4                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_spaceshiphi4                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_spaceshipsi4                                    (rtx, rtx, rtx, rtx);
static inline rtx gen_spaceshipdi4                                    (rtx, rtx, rtx, rtx);
static inline rtx
gen_spaceshipdi4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_issignalingxf2                                  (rtx, rtx);
extern rtx        gen_movv8qi                                         (rtx, rtx);
extern rtx        gen_movv4hi                                         (rtx, rtx);
extern rtx        gen_movv2si                                         (rtx, rtx);
extern rtx        gen_movv1di                                         (rtx, rtx);
extern rtx        gen_movv2sf                                         (rtx, rtx);
extern rtx        gen_movv4hf                                         (rtx, rtx);
extern rtx        gen_movv4bf                                         (rtx, rtx);
extern rtx        gen_movmisalignv8qi                                 (rtx, rtx);
extern rtx        gen_movmisalignv4hi                                 (rtx, rtx);
extern rtx        gen_movmisalignv2si                                 (rtx, rtx);
extern rtx        gen_movmisalignv1di                                 (rtx, rtx);
extern rtx        gen_movmisalignv2sf                                 (rtx, rtx);
extern rtx        gen_movmisalignv4hf                                 (rtx, rtx);
extern rtx        gen_movmisalignv4bf                                 (rtx, rtx);
extern rtx        gen_movv4qi                                         (rtx, rtx);
extern rtx        gen_movv2hi                                         (rtx, rtx);
extern rtx        gen_movv1si                                         (rtx, rtx);
extern rtx        gen_movv2hf                                         (rtx, rtx);
extern rtx        gen_movv2bf                                         (rtx, rtx);
extern rtx        gen_movmisalignv4qi                                 (rtx, rtx);
extern rtx        gen_movmisalignv2hi                                 (rtx, rtx);
extern rtx        gen_movmisalignv1si                                 (rtx, rtx);
extern rtx        gen_movmisalignv2hf                                 (rtx, rtx);
extern rtx        gen_movmisalignv2bf                                 (rtx, rtx);
extern rtx        gen_movv2qi                                         (rtx, rtx);
extern rtx        gen_movmisalignv2qi                                 (rtx, rtx);
extern rtx        gen_movq_v2sf_to_sse                                (rtx, rtx);
extern rtx        gen_movq_v2si_to_sse                                (rtx, rtx);
extern rtx        gen_movq_v4hf_to_sse                                (rtx, rtx);
extern rtx        gen_movq_v4hi_to_sse                                (rtx, rtx);
static inline rtx gen_absv2sf2                                        (rtx, rtx);
static inline rtx
gen_absv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_negv2sf2                                        (rtx, rtx);
static inline rtx
gen_negv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_addv2sf3                                        (rtx, rtx, rtx);
static inline rtx
gen_addv2sf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_subv2sf3                                        (rtx, rtx, rtx);
static inline rtx
gen_subv2sf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_mulv2sf3                                        (rtx, rtx, rtx);
static inline rtx
gen_mulv2sf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mmx_addv2sf3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_subv2sf3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_subrv2sf3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_mulv2sf3                                    (rtx, rtx, rtx);
static inline rtx gen_divv2sf3                                        (rtx, rtx, rtx);
static inline rtx
gen_divv2sf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_smaxv2sf3                                       (rtx, rtx, rtx);
static inline rtx
gen_smaxv2sf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_sminv2sf3                                       (rtx, rtx, rtx);
static inline rtx
gen_sminv2sf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mmx_smaxv2sf3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_sminv2sf3                                   (rtx, rtx, rtx);
static inline rtx gen_sqrtv2sf2                                       (rtx, rtx);
static inline rtx
gen_sqrtv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_mmx_haddv2sf3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_haddsubv2sf3                                (rtx, rtx, rtx);
static inline rtx gen_vec_addsubv2sf3                                 (rtx, rtx, rtx);
static inline rtx
gen_vec_addsubv2sf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vec_fmaddsubv2sf4                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_vec_fmaddsubv2sf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vec_fmsubaddv2sf4                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_vec_fmsubaddv2sf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_mmx_eqv2sf3                                     (rtx, rtx, rtx);
static inline rtx gen_vec_cmpv2sfv2si                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_vec_cmpv2sfv2si(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_copysignv2sf3                                   (rtx, rtx, rtx);
static inline rtx
gen_copysignv2sf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xorsignv2sf3                                    (rtx, rtx, rtx);
static inline rtx
gen_xorsignv2sf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_signbitv2sf2                                    (rtx, rtx);
static inline rtx
gen_signbitv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_fmav2sf4                                        (rtx, rtx, rtx, rtx);
static inline rtx
gen_fmav2sf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_fmsv2sf4                                        (rtx, rtx, rtx, rtx);
static inline rtx
gen_fmsv2sf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_fnmav2sf4                                       (rtx, rtx, rtx, rtx);
static inline rtx
gen_fnmav2sf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_fnmsv2sf4                                       (rtx, rtx, rtx, rtx);
static inline rtx
gen_fnmsv2sf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_fix_truncv2sfv2si2                              (rtx, rtx);
static inline rtx
gen_fix_truncv2sfv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_fixuns_truncv2sfv2si2                           (rtx, rtx);
static inline rtx
gen_fixuns_truncv2sfv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_floatv2siv2sf2                                  (rtx, rtx);
static inline rtx
gen_floatv2siv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_floatunsv2siv2sf2                               (rtx, rtx);
static inline rtx
gen_floatunsv2siv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_vec_setv2sf                                     (rtx, rtx, rtx);
extern rtx        gen_vec_extractv2sfsf                               (rtx, rtx, rtx);
extern rtx        gen_vec_initv2sfsf                                  (rtx, rtx);
static inline rtx gen_nearbyintv2sf2                                  (rtx, rtx);
static inline rtx
gen_nearbyintv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_rintv2sf2                                       (rtx, rtx);
static inline rtx
gen_rintv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_lrintv2sfv2si2                                  (rtx, rtx);
static inline rtx
gen_lrintv2sfv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_ceilv2sf2                                       (rtx, rtx);
static inline rtx
gen_ceilv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_lceilv2sfv2si2                                  (rtx, rtx);
static inline rtx
gen_lceilv2sfv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_floorv2sf2                                      (rtx, rtx);
static inline rtx
gen_floorv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_lfloorv2sfv2si2                                 (rtx, rtx);
static inline rtx
gen_lfloorv2sfv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_btruncv2sf2                                     (rtx, rtx);
static inline rtx
gen_btruncv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_roundv2sf2                                      (rtx, rtx);
static inline rtx
gen_roundv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_lroundv2sfv2si2                                 (rtx, rtx);
static inline rtx
gen_lroundv2sfv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_divv4hf3                                        (rtx, rtx, rtx);
extern rtx        gen_movd_v2hf_to_sse                                (rtx, rtx);
extern rtx        gen_movd_v2bf_to_sse                                (rtx, rtx);
extern rtx        gen_movd_v2hi_to_sse                                (rtx, rtx);
extern rtx        gen_movd_v2hf_to_sse_reg                            (rtx, rtx, rtx);
extern rtx        gen_movd_v2bf_to_sse_reg                            (rtx, rtx, rtx);
extern rtx        gen_movd_v2hi_to_sse_reg                            (rtx, rtx, rtx);
extern rtx        gen_addv2hf3                                        (rtx, rtx, rtx);
extern rtx        gen_subv2hf3                                        (rtx, rtx, rtx);
extern rtx        gen_mulv2hf3                                        (rtx, rtx, rtx);
static inline rtx gen_addv4hf3                                        (rtx, rtx, rtx);
static inline rtx
gen_addv4hf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_subv4hf3                                        (rtx, rtx, rtx);
static inline rtx
gen_subv4hf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_mulv4hf3                                        (rtx, rtx, rtx);
static inline rtx
gen_mulv4hf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_addv2bf3                                        (rtx, rtx, rtx);
extern rtx        gen_subv2bf3                                        (rtx, rtx, rtx);
extern rtx        gen_mulv2bf3                                        (rtx, rtx, rtx);
extern rtx        gen_divv2bf3                                        (rtx, rtx, rtx);
static inline rtx gen_addv4bf3                                        (rtx, rtx, rtx);
static inline rtx
gen_addv4bf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_subv4bf3                                        (rtx, rtx, rtx);
static inline rtx
gen_subv4bf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_mulv4bf3                                        (rtx, rtx, rtx);
static inline rtx
gen_mulv4bf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_divv4bf3                                        (rtx, rtx, rtx);
static inline rtx
gen_divv4bf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_divv2hf3                                        (rtx, rtx, rtx);
extern rtx        gen_smaxv2hf3                                       (rtx, rtx, rtx);
extern rtx        gen_sminv2hf3                                       (rtx, rtx, rtx);
static inline rtx gen_smaxv4hf3                                       (rtx, rtx, rtx);
static inline rtx
gen_smaxv4hf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_sminv4hf3                                       (rtx, rtx, rtx);
static inline rtx
gen_sminv4hf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_smaxv2bf3                                       (rtx, rtx, rtx);
extern rtx        gen_sminv2bf3                                       (rtx, rtx, rtx);
static inline rtx gen_smaxv4bf3                                       (rtx, rtx, rtx);
static inline rtx
gen_smaxv4bf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_sminv4bf3                                       (rtx, rtx, rtx);
static inline rtx
gen_sminv4bf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_sqrtv2hf2                                       (rtx, rtx);
static inline rtx gen_sqrtv4hf2                                       (rtx, rtx);
static inline rtx
gen_sqrtv4hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_sqrtv2bf2                                       (rtx, rtx);
static inline rtx gen_sqrtv4bf2                                       (rtx, rtx);
static inline rtx
gen_sqrtv4bf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_absv2bf2                                        (rtx, rtx);
extern rtx        gen_negv2bf2                                        (rtx, rtx);
static inline rtx gen_absv4bf2                                        (rtx, rtx);
static inline rtx
gen_absv4bf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_negv4bf2                                        (rtx, rtx);
static inline rtx
gen_negv4bf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_absv2hf2                                        (rtx, rtx);
extern rtx        gen_negv2hf2                                        (rtx, rtx);
static inline rtx gen_absv4hf2                                        (rtx, rtx);
static inline rtx
gen_absv4hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_negv4hf2                                        (rtx, rtx);
static inline rtx
gen_negv4hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_vec_cmpv4hfqi                                   (rtx, rtx, rtx, rtx);
static inline rtx
gen_vec_cmpv4hfqi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vcond_mask_v4hfv4hi                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_vcond_mask_v4hfv4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vcond_mask_v4bfv4hi                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_vcond_mask_v4bfv4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vcond_mask_v4hfqi                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_vcond_mask_v4hfqi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vcond_mask_v4bfqi                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_vcond_mask_v4bfqi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vcond_mask_v4hiqi                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_vcond_mask_v4hiqi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_vec_cmpv2hfqi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v2hfv2hi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v2bfv2hi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v2hfqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v2bfqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v2hiqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv2bfqi                                   (rtx, rtx, rtx, rtx);
static inline rtx gen_vec_cmpv4bfqi                                   (rtx, rtx, rtx, rtx);
static inline rtx
gen_vec_cmpv4bfqi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_btruncv2hf2                                     (rtx, rtx);
static inline rtx gen_btruncv4hf2                                     (rtx, rtx);
static inline rtx
gen_btruncv4hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_nearbyintv2hf2                                  (rtx, rtx);
static inline rtx gen_nearbyintv4hf2                                  (rtx, rtx);
static inline rtx
gen_nearbyintv4hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_rintv2hf2                                       (rtx, rtx);
static inline rtx gen_rintv4hf2                                       (rtx, rtx);
static inline rtx
gen_rintv4hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_lrintv2hfv2hi2                                  (rtx, rtx);
static inline rtx gen_lrintv4hfv4hi2                                  (rtx, rtx);
static inline rtx
gen_lrintv4hfv4hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_floorv2hf2                                      (rtx, rtx);
static inline rtx gen_floorv4hf2                                      (rtx, rtx);
static inline rtx
gen_floorv4hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_lfloorv2hfv2hi2                                 (rtx, rtx);
static inline rtx gen_lfloorv4hfv4hi2                                 (rtx, rtx);
static inline rtx
gen_lfloorv4hfv4hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_ceilv2hf2                                       (rtx, rtx);
static inline rtx gen_ceilv4hf2                                       (rtx, rtx);
static inline rtx
gen_ceilv4hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_lceilv2hfv2hi2                                  (rtx, rtx);
static inline rtx gen_lceilv4hfv4hi2                                  (rtx, rtx);
static inline rtx
gen_lceilv4hfv4hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_roundv2hf2                                      (rtx, rtx);
static inline rtx gen_roundv4hf2                                      (rtx, rtx);
static inline rtx
gen_roundv4hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_lroundv2hfv2hi2                                 (rtx, rtx);
static inline rtx gen_lroundv4hfv4hi2                                 (rtx, rtx);
static inline rtx
gen_lroundv4hfv4hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_copysignv2bf3                                   (rtx, rtx, rtx);
static inline rtx gen_copysignv4bf3                                   (rtx, rtx, rtx);
static inline rtx
gen_copysignv4bf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_copysignv2hf3                                   (rtx, rtx, rtx);
static inline rtx gen_copysignv4hf3                                   (rtx, rtx, rtx);
static inline rtx
gen_copysignv4hf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_xorsignv2bf3                                    (rtx, rtx, rtx);
static inline rtx gen_xorsignv4bf3                                    (rtx, rtx, rtx);
static inline rtx
gen_xorsignv4bf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_xorsignv2hf3                                    (rtx, rtx, rtx);
static inline rtx gen_xorsignv4hf3                                    (rtx, rtx, rtx);
static inline rtx
gen_xorsignv4hf3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_signbitv2bf2                                    (rtx, rtx);
static inline rtx gen_signbitv4bf2                                    (rtx, rtx);
static inline rtx
gen_signbitv4bf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_signbitv2hf2                                    (rtx, rtx);
static inline rtx gen_signbitv4hf2                                    (rtx, rtx);
static inline rtx
gen_signbitv4hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_fmav2hf4                                        (rtx, rtx, rtx, rtx);
static inline rtx gen_fmav4hf4                                        (rtx, rtx, rtx, rtx);
static inline rtx
gen_fmav4hf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_fmsv2hf4                                        (rtx, rtx, rtx, rtx);
static inline rtx gen_fmsv4hf4                                        (rtx, rtx, rtx, rtx);
static inline rtx
gen_fmsv4hf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_fnmav2hf4                                       (rtx, rtx, rtx, rtx);
static inline rtx gen_fnmav4hf4                                       (rtx, rtx, rtx, rtx);
static inline rtx
gen_fnmav4hf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_fnmsv2hf4                                       (rtx, rtx, rtx, rtx);
static inline rtx gen_fnmsv4hf4                                       (rtx, rtx, rtx, rtx);
static inline rtx
gen_fnmsv4hf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vec_fmaddsubv4hf4                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_vec_fmaddsubv4hf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vec_fmsubaddv4hf4                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_vec_fmsubaddv4hf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_fmav2bf4                                        (rtx, rtx, rtx, rtx);
static inline rtx gen_fmav4bf4                                        (rtx, rtx, rtx, rtx);
static inline rtx
gen_fmav4bf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_fmsv2bf4                                        (rtx, rtx, rtx, rtx);
static inline rtx gen_fmsv4bf4                                        (rtx, rtx, rtx, rtx);
static inline rtx
gen_fmsv4bf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_fnmav2bf4                                       (rtx, rtx, rtx, rtx);
static inline rtx gen_fnmav4bf4                                       (rtx, rtx, rtx, rtx);
static inline rtx
gen_fnmav4bf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_fnmsv2bf4                                       (rtx, rtx, rtx, rtx);
static inline rtx gen_fnmsv4bf4                                       (rtx, rtx, rtx, rtx);
static inline rtx
gen_fnmsv4bf4(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_cmlav4hf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_cmla_conjv4hf4                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_cmulv4hf3                                       (rtx, rtx, rtx);
extern rtx        gen_cmul_conjv4hf3                                  (rtx, rtx, rtx);
extern rtx        gen_fix_truncv2hfv2hi2                              (rtx, rtx);
extern rtx        gen_fixuns_truncv2hfv2hi2                           (rtx, rtx);
static inline rtx gen_fix_truncv4hfv4hi2                              (rtx, rtx);
static inline rtx
gen_fix_truncv4hfv4hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_fixuns_truncv4hfv4hi2                           (rtx, rtx);
static inline rtx
gen_fixuns_truncv4hfv4hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_fix_truncv2hfv2si2                              (rtx, rtx);
static inline rtx
gen_fix_truncv2hfv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_fixuns_truncv2hfv2si2                           (rtx, rtx);
static inline rtx
gen_fixuns_truncv2hfv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_floatv2hiv2hf2                                  (rtx, rtx);
extern rtx        gen_floatunsv2hiv2hf2                               (rtx, rtx);
static inline rtx gen_floatv4hiv4hf2                                  (rtx, rtx);
static inline rtx
gen_floatv4hiv4hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_floatunsv4hiv4hf2                               (rtx, rtx);
static inline rtx
gen_floatunsv4hiv4hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_floatv2siv2hf2                                  (rtx, rtx);
static inline rtx
gen_floatv2siv2hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_floatunsv2siv2hf2                               (rtx, rtx);
static inline rtx
gen_floatunsv2siv2hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_extendv2hfv2sf2                                 (rtx, rtx);
static inline rtx
gen_extendv2hfv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_truncv2sfv2hf2                                  (rtx, rtx);
static inline rtx
gen_truncv2sfv2hf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_truncv2sfv2bf2                                  (rtx, rtx);
static inline rtx
gen_truncv2sfv2bf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_extendv2bfv2sf2                                 (rtx, rtx);
static inline rtx
gen_extendv2bfv2sf2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_negv8qi2                                        (rtx, rtx);
static inline rtx
gen_negv8qi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_negv4hi2                                        (rtx, rtx);
static inline rtx
gen_negv4hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_negv2si2                                        (rtx, rtx);
static inline rtx
gen_negv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_negv4qi2                                        (rtx, rtx);
extern rtx        gen_negv2hi2                                        (rtx, rtx);
extern rtx        gen_mmx_addv8qi3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_subv8qi3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_addv4hi3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_subv4hi3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_addv2si3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_subv2si3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_addv1di3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_subv1di3                                    (rtx, rtx, rtx);
static inline rtx gen_addv8qi3                                        (rtx, rtx, rtx);
static inline rtx
gen_addv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_subv8qi3                                        (rtx, rtx, rtx);
static inline rtx
gen_subv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_addv4hi3                                        (rtx, rtx, rtx);
static inline rtx
gen_addv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_subv4hi3                                        (rtx, rtx, rtx);
static inline rtx
gen_subv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_addv2si3                                        (rtx, rtx, rtx);
static inline rtx
gen_addv2si3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_subv2si3                                        (rtx, rtx, rtx);
static inline rtx
gen_subv2si3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mmx_ssaddv8qi3                                  (rtx, rtx, rtx);
extern rtx        gen_mmx_usaddv8qi3                                  (rtx, rtx, rtx);
extern rtx        gen_mmx_sssubv8qi3                                  (rtx, rtx, rtx);
extern rtx        gen_mmx_ussubv8qi3                                  (rtx, rtx, rtx);
extern rtx        gen_mmx_ssaddv4hi3                                  (rtx, rtx, rtx);
extern rtx        gen_mmx_usaddv4hi3                                  (rtx, rtx, rtx);
extern rtx        gen_mmx_sssubv4hi3                                  (rtx, rtx, rtx);
extern rtx        gen_mmx_ussubv4hi3                                  (rtx, rtx, rtx);
static inline rtx gen_ssaddv8qi3                                      (rtx, rtx, rtx);
static inline rtx
gen_ssaddv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_usaddv8qi3                                      (rtx, rtx, rtx);
static inline rtx
gen_usaddv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_sssubv8qi3                                      (rtx, rtx, rtx);
static inline rtx
gen_sssubv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_ussubv8qi3                                      (rtx, rtx, rtx);
static inline rtx
gen_ussubv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_ssaddv4hi3                                      (rtx, rtx, rtx);
static inline rtx
gen_ssaddv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_usaddv4hi3                                      (rtx, rtx, rtx);
static inline rtx
gen_usaddv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_sssubv4hi3                                      (rtx, rtx, rtx);
static inline rtx
gen_sssubv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_ussubv4hi3                                      (rtx, rtx, rtx);
static inline rtx
gen_ussubv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mmx_mulv4hi3                                    (rtx, rtx, rtx);
static inline rtx gen_mulv4hi3                                        (rtx, rtx, rtx);
static inline rtx
gen_mulv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_mulv8qi3                                        (rtx, rtx, rtx);
static inline rtx
gen_mulv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mulv4qi3                                        (rtx, rtx, rtx);
extern rtx        gen_mmx_smulv4hi3_highpart                          (rtx, rtx, rtx);
extern rtx        gen_mmx_umulv4hi3_highpart                          (rtx, rtx, rtx);
static inline rtx gen_smulv4hi3_highpart                              (rtx, rtx, rtx);
static inline rtx
gen_smulv4hi3_highpart(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_umulv4hi3_highpart                              (rtx, rtx, rtx);
static inline rtx
gen_umulv4hi3_highpart(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mmx_pmaddwd                                     (rtx, rtx, rtx);
extern rtx        gen_mmx_pmulhrwv4hi3                                (rtx, rtx, rtx);
extern rtx        gen_sse2_umulv1siv1di3                              (rtx, rtx, rtx);
extern rtx        gen_mmx_smaxv4hi3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_sminv4hi3                                   (rtx, rtx, rtx);
static inline rtx gen_smaxv4hi3                                       (rtx, rtx, rtx);
static inline rtx
gen_smaxv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_sminv4hi3                                       (rtx, rtx, rtx);
static inline rtx
gen_sminv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mmx_umaxv8qi3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_uminv8qi3                                   (rtx, rtx, rtx);
static inline rtx gen_umaxv8qi3                                       (rtx, rtx, rtx);
static inline rtx
gen_umaxv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_uminv8qi3                                       (rtx, rtx, rtx);
static inline rtx
gen_uminv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_absv8qi2                                        (rtx, rtx);
static inline rtx
gen_absv8qi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_absv4hi2                                        (rtx, rtx);
static inline rtx
gen_absv4hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_absv2si2                                        (rtx, rtx);
static inline rtx
gen_absv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_ashrv4hi3                                       (rtx, rtx, rtx);
static inline rtx
gen_ashrv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_ashrv2si3                                       (rtx, rtx, rtx);
static inline rtx
gen_ashrv2si3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_ashlv4hi3                                       (rtx, rtx, rtx);
static inline rtx
gen_ashlv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_lshrv4hi3                                       (rtx, rtx, rtx);
static inline rtx
gen_lshrv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_ashlv2si3                                       (rtx, rtx, rtx);
static inline rtx
gen_ashlv2si3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_lshrv2si3                                       (rtx, rtx, rtx);
static inline rtx
gen_lshrv2si3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_ashlv8qi3                                       (rtx, rtx, rtx);
static inline rtx
gen_ashlv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_lshrv8qi3                                       (rtx, rtx, rtx);
static inline rtx
gen_lshrv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_ashrv8qi3                                       (rtx, rtx, rtx);
static inline rtx
gen_ashrv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_ashlv4qi3                                       (rtx, rtx, rtx);
extern rtx        gen_lshrv4qi3                                       (rtx, rtx, rtx);
extern rtx        gen_ashrv4qi3                                       (rtx, rtx, rtx);
static inline rtx gen_vashlv8qi3                                      (rtx, rtx, rtx);
static inline rtx
gen_vashlv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vlshrv8qi3                                      (rtx, rtx, rtx);
static inline rtx
gen_vlshrv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vashrv8qi3                                      (rtx, rtx, rtx);
static inline rtx
gen_vashrv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_vashlv4qi3                                      (rtx, rtx, rtx);
extern rtx        gen_vlshrv4qi3                                      (rtx, rtx, rtx);
extern rtx        gen_vashrv4qi3                                      (rtx, rtx, rtx);
static inline rtx gen_vec_shl_v2sf                                    (rtx, rtx, rtx);
static inline rtx
gen_vec_shl_v2sf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vec_shl_v2si                                    (rtx, rtx, rtx);
static inline rtx
gen_vec_shl_v2si(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vec_shl_v4hf                                    (rtx, rtx, rtx);
static inline rtx
gen_vec_shl_v4hf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vec_shl_v4bf                                    (rtx, rtx, rtx);
static inline rtx
gen_vec_shl_v4bf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vec_shl_v4hi                                    (rtx, rtx, rtx);
static inline rtx
gen_vec_shl_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vec_shl_v8qi                                    (rtx, rtx, rtx);
static inline rtx
gen_vec_shl_v8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_vec_shl_v2hf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shl_v2bf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shl_v2hi                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shl_v4qi                                    (rtx, rtx, rtx);
static inline rtx gen_vec_shr_v2sf                                    (rtx, rtx, rtx);
static inline rtx
gen_vec_shr_v2sf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vec_shr_v2si                                    (rtx, rtx, rtx);
static inline rtx
gen_vec_shr_v2si(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vec_shr_v4hf                                    (rtx, rtx, rtx);
static inline rtx
gen_vec_shr_v4hf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vec_shr_v4bf                                    (rtx, rtx, rtx);
static inline rtx
gen_vec_shr_v4bf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vec_shr_v4hi                                    (rtx, rtx, rtx);
static inline rtx
gen_vec_shr_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vec_shr_v8qi                                    (rtx, rtx, rtx);
static inline rtx
gen_vec_shr_v8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_vec_shr_v2hf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shr_v2bf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shr_v2hi                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shr_v4qi                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_eqv8qi3                                     (rtx, rtx, rtx);
extern rtx        gen_mmx_eqv4hi3                                     (rtx, rtx, rtx);
extern rtx        gen_mmx_eqv2si3                                     (rtx, rtx, rtx);
static inline rtx gen_vec_cmpv8qiv8qi                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_vec_cmpv8qiv8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vec_cmpv4hiv4hi                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_vec_cmpv4hiv4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vec_cmpv2siv2si                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_vec_cmpv2siv2si(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_vec_cmpv4qiv4qi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv2qiv2qi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv2hiv2hi                                 (rtx, rtx, rtx, rtx);
static inline rtx gen_vec_cmpuv8qiv8qi                                (rtx, rtx, rtx, rtx);
static inline rtx
gen_vec_cmpuv8qiv8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vec_cmpuv4hiv4hi                                (rtx, rtx, rtx, rtx);
static inline rtx
gen_vec_cmpuv4hiv4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vec_cmpuv2siv2si                                (rtx, rtx, rtx, rtx);
static inline rtx
gen_vec_cmpuv2siv2si(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_vec_cmpuv4qiv4qi                                (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv2qiv2qi                                (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv2hiv2hi                                (rtx, rtx, rtx, rtx);
static inline rtx gen_vcond_mask_v8qiv8qi                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_vcond_mask_v8qiv8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vcond_mask_v4hiv4hi                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_vcond_mask_v4hiv4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vcond_mask_v2siv2si                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_vcond_mask_v2siv2si(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_vcond_mask_v2sfv2si                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_vcond_mask_v2sfv2si(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_vcond_mask_v4qiv4qi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v2qiv2qi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v2hiv2hi                             (rtx, rtx, rtx, rtx);
static inline rtx gen_one_cmplv8qi2                                   (rtx, rtx);
static inline rtx
gen_one_cmplv8qi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_one_cmplv4hi2                                   (rtx, rtx);
static inline rtx
gen_one_cmplv4hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_one_cmplv2si2                                   (rtx, rtx);
static inline rtx
gen_one_cmplv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_andnv8qi3                                       (rtx, rtx, rtx);
static inline rtx
gen_andnv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_andnv4hi3                                       (rtx, rtx, rtx);
static inline rtx
gen_andnv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_andnv2si3                                       (rtx, rtx, rtx);
static inline rtx
gen_andnv2si3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mmx_andv8qi3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_iorv8qi3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_xorv8qi3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_andv4hi3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_iorv4hi3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_xorv4hi3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_andv2si3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_iorv2si3                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_xorv2si3                                    (rtx, rtx, rtx);
static inline rtx gen_andv8qi3                                        (rtx, rtx, rtx);
static inline rtx
gen_andv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_iorv8qi3                                        (rtx, rtx, rtx);
static inline rtx
gen_iorv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xorv8qi3                                        (rtx, rtx, rtx);
static inline rtx
gen_xorv8qi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_andv4hi3                                        (rtx, rtx, rtx);
static inline rtx
gen_andv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_iorv4hi3                                        (rtx, rtx, rtx);
static inline rtx
gen_iorv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xorv4hi3                                        (rtx, rtx, rtx);
static inline rtx
gen_xorv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_andv2si3                                        (rtx, rtx, rtx);
static inline rtx
gen_andv2si3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_iorv2si3                                        (rtx, rtx, rtx);
static inline rtx
gen_iorv2si3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_xorv2si3                                        (rtx, rtx, rtx);
static inline rtx
gen_xorv2si3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_andv4qi3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv4qi3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv4qi3                                        (rtx, rtx, rtx);
extern rtx        gen_andv2qi3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv2qi3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv2qi3                                        (rtx, rtx, rtx);
extern rtx        gen_andv2hi3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv2hi3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv2hi3                                        (rtx, rtx, rtx);
static inline rtx gen_extendv4qiv4hi2                                 (rtx, rtx);
static inline rtx
gen_extendv4qiv4hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_zero_extendv4qiv4hi2                            (rtx, rtx);
static inline rtx
gen_zero_extendv4qiv4hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_extendv2hiv2si2                                 (rtx, rtx);
static inline rtx
gen_extendv2hiv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_zero_extendv2hiv2si2                            (rtx, rtx);
static inline rtx
gen_zero_extendv2hiv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_extendv2qiv2si2                                 (rtx, rtx);
static inline rtx
gen_extendv2qiv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_zero_extendv2qiv2si2                            (rtx, rtx);
static inline rtx
gen_zero_extendv2qiv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_extendv2qiv2hi2                                 (rtx, rtx);
extern rtx        gen_zero_extendv2qiv2hi2                            (rtx, rtx);
static inline rtx gen_truncv4hiv4qi2                                  (rtx, rtx);
static inline rtx
gen_truncv4hiv4qi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_truncv2hiv2qi2                                  (rtx, rtx);
static inline rtx gen_truncv2siv2qi2                                  (rtx, rtx);
static inline rtx
gen_truncv2siv2qi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_truncv2siv2hi2                                  (rtx, rtx);
static inline rtx
gen_truncv2siv2hi2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_vec_pack_trunc_v4hi                             (rtx, rtx, rtx);
static inline rtx
gen_vec_pack_trunc_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vec_pack_trunc_v2si                             (rtx, rtx, rtx);
static inline rtx
gen_vec_pack_trunc_v2si(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_vec_pack_trunc_v2hi                             (rtx, rtx, rtx);
static inline rtx gen_vec_unpacks_lo_v8qi                             (rtx, rtx);
static inline rtx
gen_vec_unpacks_lo_v8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_vec_unpacks_lo_v4hi                             (rtx, rtx);
static inline rtx
gen_vec_unpacks_lo_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_vec_unpacks_hi_v8qi                             (rtx, rtx);
static inline rtx
gen_vec_unpacks_hi_v8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_vec_unpacks_hi_v4hi                             (rtx, rtx);
static inline rtx
gen_vec_unpacks_hi_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_vec_unpacku_lo_v8qi                             (rtx, rtx);
static inline rtx
gen_vec_unpacku_lo_v8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_vec_unpacku_lo_v4hi                             (rtx, rtx);
static inline rtx
gen_vec_unpacku_lo_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_vec_unpacku_hi_v8qi                             (rtx, rtx);
static inline rtx
gen_vec_unpacku_hi_v8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_vec_unpacku_hi_v4hi                             (rtx, rtx);
static inline rtx
gen_vec_unpacku_hi_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_vec_unpacks_lo_v4qi                             (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v4qi                             (rtx, rtx);
extern rtx        gen_vec_unpacku_lo_v4qi                             (rtx, rtx);
extern rtx        gen_vec_unpacku_hi_v4qi                             (rtx, rtx);
extern rtx        gen_mmx_pshufw                                      (rtx, rtx, rtx);
extern rtx        gen_vec_setv2si                                     (rtx, rtx, rtx);
extern rtx        gen_vec_extractv2sisi                               (rtx, rtx, rtx);
extern rtx        gen_vec_initv2sisi                                  (rtx, rtx);
extern rtx        gen_vec_setv4hf                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv4bf                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv4hi                                     (rtx, rtx, rtx);
extern rtx        gen_vec_extractv4hfhf                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv4bfbf                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv4hihi                               (rtx, rtx, rtx);
extern rtx        gen_vec_initv4hihi                                  (rtx, rtx);
static inline rtx gen_vec_initv4hfhf                                  (rtx, rtx);
static inline rtx
gen_vec_initv4hfhf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_vec_initv4bfbf                                  (rtx, rtx);
static inline rtx
gen_vec_initv4bfbf(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_vec_setv8qi                                     (rtx, rtx, rtx);
static inline rtx
gen_vec_setv8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_vec_extractv8qiqi                               (rtx, rtx, rtx);
static inline rtx
gen_vec_extractv8qiqi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_vec_initv8qiqi                                  (rtx, rtx);
extern rtx        gen_vec_setv2hf                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv2bf                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv2hi                                     (rtx, rtx, rtx);
extern rtx        gen_vec_extractv2hfhf                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv2bfbf                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv2hihi                               (rtx, rtx, rtx);
extern rtx        gen_vec_setv4qi                                     (rtx, rtx, rtx);
extern rtx        gen_vec_extractv4qiqi                               (rtx, rtx, rtx);
extern rtx        gen_vec_initv2hfhf                                  (rtx, rtx);
extern rtx        gen_vec_initv2bfbf                                  (rtx, rtx);
extern rtx        gen_vec_initv2hihi                                  (rtx, rtx);
extern rtx        gen_vec_initv4qiqi                                  (rtx, rtx);
extern rtx        gen_mmx_uavgv8qi3                                   (rtx, rtx, rtx);
extern rtx        gen_mmx_uavgv4hi3                                   (rtx, rtx, rtx);
static inline rtx gen_uavgv8qi3_ceil                                  (rtx, rtx, rtx);
static inline rtx
gen_uavgv8qi3_ceil(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_uavgv4hi3_ceil                                  (rtx, rtx, rtx);
static inline rtx
gen_uavgv4hi3_ceil(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_mmx_psadbw                                      (rtx, rtx, rtx);
static inline rtx gen_reduc_and_scal_v8qi                             (rtx, rtx);
static inline rtx
gen_reduc_and_scal_v8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_reduc_ior_scal_v8qi                             (rtx, rtx);
static inline rtx
gen_reduc_ior_scal_v8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_reduc_xor_scal_v8qi                             (rtx, rtx);
static inline rtx
gen_reduc_xor_scal_v8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_reduc_and_scal_v4hi                             (rtx, rtx);
static inline rtx
gen_reduc_and_scal_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_reduc_ior_scal_v4hi                             (rtx, rtx);
static inline rtx
gen_reduc_ior_scal_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_reduc_xor_scal_v4hi                             (rtx, rtx);
static inline rtx
gen_reduc_xor_scal_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_reduc_and_scal_v4qi                             (rtx, rtx);
extern rtx        gen_reduc_ior_scal_v4qi                             (rtx, rtx);
extern rtx        gen_reduc_xor_scal_v4qi                             (rtx, rtx);
static inline rtx gen_reduc_plus_scal_v8qi                            (rtx, rtx);
static inline rtx
gen_reduc_plus_scal_v8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_reduc_plus_scal_v4hi                            (rtx, rtx);
static inline rtx
gen_reduc_plus_scal_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_reduc_smax_scal_v4hi                            (rtx, rtx);
static inline rtx
gen_reduc_smax_scal_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_reduc_smin_scal_v4hi                            (rtx, rtx);
static inline rtx
gen_reduc_smin_scal_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_reduc_smax_scal_v4qi                            (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v4qi                            (rtx, rtx);
static inline rtx gen_reduc_umax_scal_v4hi                            (rtx, rtx);
static inline rtx
gen_reduc_umax_scal_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_reduc_umin_scal_v4hi                            (rtx, rtx);
static inline rtx
gen_reduc_umin_scal_v4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_reduc_umax_scal_v4qi                            (rtx, rtx);
extern rtx        gen_reduc_umin_scal_v4qi                            (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v4qi                            (rtx, rtx);
static inline rtx gen_usadv8qi                                        (rtx, rtx, rtx, rtx);
static inline rtx
gen_usadv8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_usdot_prodv2siv8qi                              (rtx, rtx, rtx, rtx);
static inline rtx
gen_usdot_prodv2siv8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_sdot_prodv2siv8qi                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_sdot_prodv2siv8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_udot_prodv2siv8qi                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_udot_prodv2siv8qi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_usdot_prodv2siv4hi                              (rtx, rtx, rtx, rtx);
static inline rtx
gen_usdot_prodv2siv4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_udot_prodv2siv4hi                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_udot_prodv2siv4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_sdot_prodv2siv4hi                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_sdot_prodv2siv4hi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_mmx_maskmovq                                    (rtx, rtx, rtx);
extern rtx        gen_mmx_emms                                        (void);
extern rtx        gen_mmx_femms                                       (void);
extern rtx        gen_movv64qi                                        (rtx, rtx);
extern rtx        gen_movv32qi                                        (rtx, rtx);
extern rtx        gen_movv16qi                                        (rtx, rtx);
extern rtx        gen_movv32hi                                        (rtx, rtx);
extern rtx        gen_movv16hi                                        (rtx, rtx);
extern rtx        gen_movv8hi                                         (rtx, rtx);
extern rtx        gen_movv16si                                        (rtx, rtx);
extern rtx        gen_movv8si                                         (rtx, rtx);
extern rtx        gen_movv4si                                         (rtx, rtx);
extern rtx        gen_movv8di                                         (rtx, rtx);
extern rtx        gen_movv4di                                         (rtx, rtx);
extern rtx        gen_movv2di                                         (rtx, rtx);
extern rtx        gen_movv4ti                                         (rtx, rtx);
extern rtx        gen_movv2ti                                         (rtx, rtx);
extern rtx        gen_movv1ti                                         (rtx, rtx);
extern rtx        gen_movv32hf                                        (rtx, rtx);
extern rtx        gen_movv16hf                                        (rtx, rtx);
extern rtx        gen_movv8hf                                         (rtx, rtx);
extern rtx        gen_movv32bf                                        (rtx, rtx);
extern rtx        gen_movv16bf                                        (rtx, rtx);
extern rtx        gen_movv8bf                                         (rtx, rtx);
extern rtx        gen_movv16sf                                        (rtx, rtx);
extern rtx        gen_movv8sf                                         (rtx, rtx);
extern rtx        gen_movv4sf                                         (rtx, rtx);
extern rtx        gen_movv8df                                         (rtx, rtx);
extern rtx        gen_movv4df                                         (rtx, rtx);
extern rtx        gen_movv2df                                         (rtx, rtx);
extern rtx        gen_avx512f_loadv16si_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_loadv8si_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_loadv4si_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_loadv8di_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_loadv4di_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_loadv2di_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_loadv16sf_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_loadv8sf_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_loadv4sf_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_loadv8df_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_loadv4df_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_loadv2df_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_loadv64qi_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_loadv16qi_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_loadv32qi_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_loadv32hi_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_loadv16hi_mask                         (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_loadv8hi_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_loadhf_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_loadsf_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_loaddf_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_movq128                                    (rtx, rtx);
extern rtx        gen_movmisalignv64qi                                (rtx, rtx);
extern rtx        gen_movmisalignv32qi                                (rtx, rtx);
extern rtx        gen_movmisalignv16qi                                (rtx, rtx);
extern rtx        gen_movmisalignv32hi                                (rtx, rtx);
extern rtx        gen_movmisalignv16hi                                (rtx, rtx);
extern rtx        gen_movmisalignv8hi                                 (rtx, rtx);
extern rtx        gen_movmisalignv16si                                (rtx, rtx);
extern rtx        gen_movmisalignv8si                                 (rtx, rtx);
extern rtx        gen_movmisalignv4si                                 (rtx, rtx);
extern rtx        gen_movmisalignv8di                                 (rtx, rtx);
extern rtx        gen_movmisalignv4di                                 (rtx, rtx);
extern rtx        gen_movmisalignv2di                                 (rtx, rtx);
extern rtx        gen_movmisalignv4ti                                 (rtx, rtx);
extern rtx        gen_movmisalignv2ti                                 (rtx, rtx);
extern rtx        gen_movmisalignv1ti                                 (rtx, rtx);
extern rtx        gen_movmisalignv32hf                                (rtx, rtx);
extern rtx        gen_movmisalignv16hf                                (rtx, rtx);
extern rtx        gen_movmisalignv8hf                                 (rtx, rtx);
extern rtx        gen_movmisalignv32bf                                (rtx, rtx);
extern rtx        gen_movmisalignv16bf                                (rtx, rtx);
extern rtx        gen_movmisalignv8bf                                 (rtx, rtx);
extern rtx        gen_movmisalignv16sf                                (rtx, rtx);
extern rtx        gen_movmisalignv8sf                                 (rtx, rtx);
extern rtx        gen_movmisalignv4sf                                 (rtx, rtx);
extern rtx        gen_movmisalignv8df                                 (rtx, rtx);
extern rtx        gen_movmisalignv4df                                 (rtx, rtx);
extern rtx        gen_movmisalignv2df                                 (rtx, rtx);
static inline rtx gen_storentdi                                       (rtx, rtx);
static inline rtx
gen_storentdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_storentsi                                       (rtx, rtx);
extern rtx        gen_storentsf                                       (rtx, rtx);
extern rtx        gen_storentdf                                       (rtx, rtx);
extern rtx        gen_storentv8di                                     (rtx, rtx);
extern rtx        gen_storentv4di                                     (rtx, rtx);
extern rtx        gen_storentv2di                                     (rtx, rtx);
extern rtx        gen_storentv16sf                                    (rtx, rtx);
extern rtx        gen_storentv8sf                                     (rtx, rtx);
extern rtx        gen_storentv4sf                                     (rtx, rtx);
extern rtx        gen_storentv8df                                     (rtx, rtx);
extern rtx        gen_storentv4df                                     (rtx, rtx);
extern rtx        gen_storentv2df                                     (rtx, rtx);
extern rtx        gen_kmovb                                           (rtx, rtx);
extern rtx        gen_kmovw                                           (rtx, rtx);
extern rtx        gen_kmovd                                           (rtx, rtx);
extern rtx        gen_kmovq                                           (rtx, rtx);
extern rtx        gen_kortestqi                                       (rtx, rtx);
extern rtx        gen_kortesthi                                       (rtx, rtx);
extern rtx        gen_kortestsi                                       (rtx, rtx);
extern rtx        gen_kortestdi                                       (rtx, rtx);
extern rtx        gen_absv32bf2                                       (rtx, rtx);
extern rtx        gen_negv32bf2                                       (rtx, rtx);
extern rtx        gen_absv16bf2                                       (rtx, rtx);
extern rtx        gen_negv16bf2                                       (rtx, rtx);
extern rtx        gen_absv8bf2                                        (rtx, rtx);
extern rtx        gen_negv8bf2                                        (rtx, rtx);
extern rtx        gen_absv32hf2                                       (rtx, rtx);
extern rtx        gen_negv32hf2                                       (rtx, rtx);
extern rtx        gen_absv16hf2                                       (rtx, rtx);
extern rtx        gen_negv16hf2                                       (rtx, rtx);
extern rtx        gen_absv8hf2                                        (rtx, rtx);
extern rtx        gen_negv8hf2                                        (rtx, rtx);
extern rtx        gen_absv16sf2                                       (rtx, rtx);
extern rtx        gen_negv16sf2                                       (rtx, rtx);
extern rtx        gen_absv8sf2                                        (rtx, rtx);
extern rtx        gen_negv8sf2                                        (rtx, rtx);
extern rtx        gen_absv4sf2                                        (rtx, rtx);
extern rtx        gen_negv4sf2                                        (rtx, rtx);
extern rtx        gen_absv8df2                                        (rtx, rtx);
extern rtx        gen_negv8df2                                        (rtx, rtx);
extern rtx        gen_absv4df2                                        (rtx, rtx);
extern rtx        gen_negv4df2                                        (rtx, rtx);
extern rtx        gen_absv2df2                                        (rtx, rtx);
extern rtx        gen_negv2df2                                        (rtx, rtx);
extern rtx        gen_cond_addv32hf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv32hf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv16hf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv16hf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv8hf                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv8hf                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv16sf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv16sf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv8sf                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv8sf                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv4sf                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv4sf                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv8df                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv8df                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv4df                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv4df                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv2df                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv2df                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv32hf3                                       (rtx, rtx, rtx);
extern rtx        gen_addv32hf3_round                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_addv32hf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv32hf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv32hf3                                       (rtx, rtx, rtx);
extern rtx        gen_subv32hf3_round                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_subv32hf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv32hf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv16hf3                                       (rtx, rtx, rtx);
static inline rtx gen_addv16hf3_round                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_addv16hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_addv16hf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_addv16hf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_addv16hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_subv16hf3                                       (rtx, rtx, rtx);
static inline rtx gen_subv16hf3_round                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_subv16hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_subv16hf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_subv16hf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_subv16hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_addv8hf3                                        (rtx, rtx, rtx);
static inline rtx gen_addv8hf3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_addv8hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_addv8hf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_addv8hf3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_addv8hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_subv8hf3                                        (rtx, rtx, rtx);
static inline rtx gen_subv8hf3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_subv8hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_subv8hf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_subv8hf3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_subv8hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_addv16sf3                                       (rtx, rtx, rtx);
extern rtx        gen_addv16sf3_round                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_addv16sf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv16sf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv16sf3                                       (rtx, rtx, rtx);
extern rtx        gen_subv16sf3_round                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_subv16sf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv16sf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv8sf3                                        (rtx, rtx, rtx);
static inline rtx gen_addv8sf3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_addv8sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_addv8sf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_addv8sf3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_addv8sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_subv8sf3                                        (rtx, rtx, rtx);
static inline rtx gen_subv8sf3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_subv8sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_subv8sf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_subv8sf3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_subv8sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_addv4sf3                                        (rtx, rtx, rtx);
static inline rtx gen_addv4sf3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_addv4sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_addv4sf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_addv4sf3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_addv4sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_subv4sf3                                        (rtx, rtx, rtx);
static inline rtx gen_subv4sf3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_subv4sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_subv4sf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_subv4sf3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_subv4sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_addv8df3                                        (rtx, rtx, rtx);
extern rtx        gen_addv8df3_round                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_addv8df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv8df3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv8df3                                        (rtx, rtx, rtx);
extern rtx        gen_subv8df3_round                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_subv8df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv8df3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv4df3                                        (rtx, rtx, rtx);
static inline rtx gen_addv4df3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_addv4df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_addv4df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_addv4df3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_addv4df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_subv4df3                                        (rtx, rtx, rtx);
static inline rtx gen_subv4df3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_subv4df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_subv4df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_subv4df3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_subv4df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_addv2df3                                        (rtx, rtx, rtx);
static inline rtx gen_addv2df3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_addv2df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_addv2df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_addv2df3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_addv2df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_subv2df3                                        (rtx, rtx, rtx);
static inline rtx gen_subv2df3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_subv2df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_subv2df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_subv2df3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_subv2df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_addv32bf3                                       (rtx, rtx, rtx);
extern rtx        gen_addv32bf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv32bf3                                       (rtx, rtx, rtx);
extern rtx        gen_subv32bf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv16bf3                                       (rtx, rtx, rtx);
extern rtx        gen_addv16bf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv16bf3                                       (rtx, rtx, rtx);
extern rtx        gen_subv16bf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv8bf3                                        (rtx, rtx, rtx);
extern rtx        gen_addv8bf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv8bf3                                        (rtx, rtx, rtx);
extern rtx        gen_subv8bf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv32hf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv16hf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv8hf                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv16sf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv8sf                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv4sf                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv8df                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv4df                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv2df                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv32hf3                                       (rtx, rtx, rtx);
extern rtx        gen_mulv32hf3_round                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_mulv32hf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv32hf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv16hf3                                       (rtx, rtx, rtx);
static inline rtx gen_mulv16hf3_round                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_mulv16hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_mulv16hf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_mulv16hf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_mulv16hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_mulv8hf3                                        (rtx, rtx, rtx);
static inline rtx gen_mulv8hf3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_mulv8hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_mulv8hf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_mulv8hf3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_mulv8hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_mulv16sf3                                       (rtx, rtx, rtx);
extern rtx        gen_mulv16sf3_round                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_mulv16sf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv16sf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv8sf3                                        (rtx, rtx, rtx);
static inline rtx gen_mulv8sf3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_mulv8sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_mulv8sf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_mulv8sf3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_mulv8sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_mulv4sf3                                        (rtx, rtx, rtx);
static inline rtx gen_mulv4sf3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_mulv4sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_mulv4sf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_mulv4sf3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_mulv4sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_mulv8df3                                        (rtx, rtx, rtx);
extern rtx        gen_mulv8df3_round                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_mulv8df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv8df3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv4df3                                        (rtx, rtx, rtx);
static inline rtx gen_mulv4df3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_mulv4df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_mulv4df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_mulv4df3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_mulv4df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_mulv2df3                                        (rtx, rtx, rtx);
static inline rtx gen_mulv2df3_round                                  (rtx, rtx, rtx, rtx);
static inline rtx
gen_mulv2df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_mulv2df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_mulv2df3_mask_round                             (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_mulv2df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_mulv32bf3                                       (rtx, rtx, rtx);
extern rtx        gen_mulv32bf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv16bf3                                       (rtx, rtx, rtx);
extern rtx        gen_mulv16bf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv8bf3                                        (rtx, rtx, rtx);
extern rtx        gen_mulv8bf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_divv8df3                                        (rtx, rtx, rtx);
extern rtx        gen_divv4df3                                        (rtx, rtx, rtx);
extern rtx        gen_divv2df3                                        (rtx, rtx, rtx);
extern rtx        gen_divv32hf3                                       (rtx, rtx, rtx);
extern rtx        gen_divv16hf3                                       (rtx, rtx, rtx);
extern rtx        gen_divv8hf3                                        (rtx, rtx, rtx);
extern rtx        gen_divv16sf3                                       (rtx, rtx, rtx);
extern rtx        gen_divv8sf3                                        (rtx, rtx, rtx);
extern rtx        gen_divv4sf3                                        (rtx, rtx, rtx);
extern rtx        gen_divv32bf3                                       (rtx, rtx, rtx);
extern rtx        gen_divv16bf3                                       (rtx, rtx, rtx);
extern rtx        gen_divv8bf3                                        (rtx, rtx, rtx);
extern rtx        gen_cond_divv32hf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_divv16hf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_divv8hf                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_divv16sf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_divv8sf                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_divv4sf                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_divv8df                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_divv4df                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_divv2df                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sqrtv32bf2                                      (rtx, rtx);
extern rtx        gen_sqrtv16bf2                                      (rtx, rtx);
extern rtx        gen_sqrtv8bf2                                       (rtx, rtx);
extern rtx        gen_sqrtv32hf2                                      (rtx, rtx);
extern rtx        gen_sqrtv16hf2                                      (rtx, rtx);
extern rtx        gen_sqrtv8hf2                                       (rtx, rtx);
extern rtx        gen_sqrtv8df2                                       (rtx, rtx);
extern rtx        gen_sqrtv4df2                                       (rtx, rtx);
extern rtx        gen_sqrtv2df2                                       (rtx, rtx);
extern rtx        gen_sqrtv16sf2                                      (rtx, rtx);
extern rtx        gen_sqrtv8sf2                                       (rtx, rtx);
extern rtx        gen_sqrtv4sf2                                       (rtx, rtx);
extern rtx        gen_rsqrtv8sf2                                      (rtx, rtx);
extern rtx        gen_rsqrtv4sf2                                      (rtx, rtx);
extern rtx        gen_rsqrtv32hf2                                     (rtx, rtx);
extern rtx        gen_rsqrtv16hf2                                     (rtx, rtx);
extern rtx        gen_rsqrtv8hf2                                      (rtx, rtx);
extern rtx        gen_cond_smaxv32hf                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv32hf                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv16hf                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv16hf                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv8hf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv8hf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv16sf                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv16sf                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv8sf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv8sf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv4sf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv4sf                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv8df                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv8df                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv4df                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv4df                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv2df                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv2df                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv32hf3                                      (rtx, rtx, rtx);
extern rtx        gen_smaxv32hf3_round                                (rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv32hf3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv32hf3_mask_round                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv32hf3                                      (rtx, rtx, rtx);
extern rtx        gen_sminv32hf3_round                                (rtx, rtx, rtx, rtx);
extern rtx        gen_sminv32hf3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv32hf3_mask_round                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv16hf3                                      (rtx, rtx, rtx);
static inline rtx gen_smaxv16hf3_round                                (rtx, rtx, rtx, rtx);
static inline rtx
gen_smaxv16hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_smaxv16hf3_mask                                 (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_smaxv16hf3_mask_round                           (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_smaxv16hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_sminv16hf3                                      (rtx, rtx, rtx);
static inline rtx gen_sminv16hf3_round                                (rtx, rtx, rtx, rtx);
static inline rtx
gen_sminv16hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_sminv16hf3_mask                                 (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_sminv16hf3_mask_round                           (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_sminv16hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_smaxv8hf3                                       (rtx, rtx, rtx);
static inline rtx gen_smaxv8hf3_round                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_smaxv8hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_smaxv8hf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_smaxv8hf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_smaxv8hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_sminv8hf3                                       (rtx, rtx, rtx);
static inline rtx gen_sminv8hf3_round                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_sminv8hf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_sminv8hf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_sminv8hf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_sminv8hf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_smaxv16sf3                                      (rtx, rtx, rtx);
extern rtx        gen_smaxv16sf3_round                                (rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv16sf3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv16sf3_mask_round                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv16sf3                                      (rtx, rtx, rtx);
extern rtx        gen_sminv16sf3_round                                (rtx, rtx, rtx, rtx);
extern rtx        gen_sminv16sf3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv16sf3_mask_round                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv8sf3                                       (rtx, rtx, rtx);
static inline rtx gen_smaxv8sf3_round                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_smaxv8sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_smaxv8sf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_smaxv8sf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_smaxv8sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_sminv8sf3                                       (rtx, rtx, rtx);
static inline rtx gen_sminv8sf3_round                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_sminv8sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_sminv8sf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_sminv8sf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_sminv8sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_smaxv4sf3                                       (rtx, rtx, rtx);
static inline rtx gen_smaxv4sf3_round                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_smaxv4sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_smaxv4sf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_smaxv4sf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_smaxv4sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_sminv4sf3                                       (rtx, rtx, rtx);
static inline rtx gen_sminv4sf3_round                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_sminv4sf3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_sminv4sf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_sminv4sf3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_sminv4sf3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_smaxv8df3                                       (rtx, rtx, rtx);
extern rtx        gen_smaxv8df3_round                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv8df3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv8df3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv8df3                                       (rtx, rtx, rtx);
extern rtx        gen_sminv8df3_round                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_sminv8df3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv8df3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv4df3                                       (rtx, rtx, rtx);
static inline rtx gen_smaxv4df3_round                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_smaxv4df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_smaxv4df3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_smaxv4df3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_smaxv4df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_sminv4df3                                       (rtx, rtx, rtx);
static inline rtx gen_sminv4df3_round                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_sminv4df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_sminv4df3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_sminv4df3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_sminv4df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_smaxv2df3                                       (rtx, rtx, rtx);
static inline rtx gen_smaxv2df3_round                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_smaxv2df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_smaxv2df3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_smaxv2df3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_smaxv2df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_sminv2df3                                       (rtx, rtx, rtx);
static inline rtx gen_sminv2df3_round                                 (rtx, rtx, rtx, rtx);
static inline rtx
gen_sminv2df3_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_sminv2df3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_sminv2df3_mask_round                            (rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_sminv2df3_mask_round(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f))
{
  return 0;
}
extern rtx        gen_avx512fp16_vmsmaxv8hf3                          (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmsmaxv8hf3_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmsmaxv8hf3_round                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmsmaxv8hf3_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmsminv8hf3                          (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmsminv8hf3_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmsminv8hf3_round                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vmsminv8hf3_mask_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmsmaxv4sf3                                 (rtx, rtx, rtx);
extern rtx        gen_sse_vmsmaxv4sf3_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmsmaxv4sf3_round                           (rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmsmaxv4sf3_mask_round                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmsminv4sf3                                 (rtx, rtx, rtx);
extern rtx        gen_sse_vmsminv4sf3_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmsminv4sf3_round                           (rtx, rtx, rtx, rtx);
extern rtx        gen_sse_vmsminv4sf3_mask_round                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmsmaxv2df3                                (rtx, rtx, rtx);
extern rtx        gen_sse2_vmsmaxv2df3_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmsmaxv2df3_round                          (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmsmaxv2df3_mask_round                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmsminv2df3                                (rtx, rtx, rtx);
extern rtx        gen_sse2_vmsminv2df3_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmsminv2df3_round                          (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_vmsminv2df3_mask_round                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse3_haddv2df3                                  (rtx, rtx, rtx);
extern rtx        gen_reduc_plus_scal_v2df                            (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v4sf                            (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v8hf                            (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v8hi                            (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v4si                            (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v2di                            (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v16qi                           (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v4df                            (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v8sf                            (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v16hf                           (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v8df                            (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v16sf                           (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v32hf                           (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v32qi                           (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v16hi                           (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v8si                            (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v4di                            (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v64qi                           (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v32hi                           (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v16si                           (rtx, rtx);
extern rtx        gen_reduc_plus_scal_v8di                            (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v8hf                            (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v8hf                            (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v4sf                            (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v4sf                            (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v2df                            (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v2df                            (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v4si                            (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v4si                            (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v8hi                            (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v8hi                            (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v16qi                           (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v16qi                           (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v2di                            (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v2di                            (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v32qi                           (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v32qi                           (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v16hi                           (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v16hi                           (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v16hf                           (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v16hf                           (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v8si                            (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v8si                            (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v4di                            (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v4di                            (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v8sf                            (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v8sf                            (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v4df                            (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v4df                            (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v64qi                           (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v64qi                           (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v32hf                           (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v32hf                           (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v32hi                           (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v32hi                           (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v16si                           (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v16si                           (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v8di                            (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v8di                            (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v16sf                           (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v16sf                           (rtx, rtx);
extern rtx        gen_reduc_smax_scal_v8df                            (rtx, rtx);
extern rtx        gen_reduc_smin_scal_v8df                            (rtx, rtx);
extern rtx        gen_reduc_umax_scal_v16si                           (rtx, rtx);
extern rtx        gen_reduc_umin_scal_v16si                           (rtx, rtx);
extern rtx        gen_reduc_umax_scal_v8di                            (rtx, rtx);
extern rtx        gen_reduc_umin_scal_v8di                            (rtx, rtx);
extern rtx        gen_reduc_umax_scal_v32hi                           (rtx, rtx);
extern rtx        gen_reduc_umin_scal_v32hi                           (rtx, rtx);
extern rtx        gen_reduc_umax_scal_v64qi                           (rtx, rtx);
extern rtx        gen_reduc_umin_scal_v64qi                           (rtx, rtx);
extern rtx        gen_reduc_umax_scal_v32qi                           (rtx, rtx);
extern rtx        gen_reduc_umin_scal_v32qi                           (rtx, rtx);
extern rtx        gen_reduc_umax_scal_v16hi                           (rtx, rtx);
extern rtx        gen_reduc_umin_scal_v16hi                           (rtx, rtx);
extern rtx        gen_reduc_umax_scal_v8si                            (rtx, rtx);
extern rtx        gen_reduc_umin_scal_v8si                            (rtx, rtx);
extern rtx        gen_reduc_umax_scal_v4di                            (rtx, rtx);
extern rtx        gen_reduc_umin_scal_v4di                            (rtx, rtx);
extern rtx        gen_reduc_umin_scal_v8hi                            (rtx, rtx);
extern rtx        gen_reduc_and_scal_v16qi                            (rtx, rtx);
extern rtx        gen_reduc_ior_scal_v16qi                            (rtx, rtx);
extern rtx        gen_reduc_xor_scal_v16qi                            (rtx, rtx);
extern rtx        gen_reduc_and_scal_v8hi                             (rtx, rtx);
extern rtx        gen_reduc_ior_scal_v8hi                             (rtx, rtx);
extern rtx        gen_reduc_xor_scal_v8hi                             (rtx, rtx);
extern rtx        gen_reduc_and_scal_v4si                             (rtx, rtx);
extern rtx        gen_reduc_ior_scal_v4si                             (rtx, rtx);
extern rtx        gen_reduc_xor_scal_v4si                             (rtx, rtx);
extern rtx        gen_reduc_and_scal_v2di                             (rtx, rtx);
extern rtx        gen_reduc_ior_scal_v2di                             (rtx, rtx);
extern rtx        gen_reduc_xor_scal_v2di                             (rtx, rtx);
extern rtx        gen_reduc_and_scal_v32qi                            (rtx, rtx);
extern rtx        gen_reduc_ior_scal_v32qi                            (rtx, rtx);
extern rtx        gen_reduc_xor_scal_v32qi                            (rtx, rtx);
extern rtx        gen_reduc_and_scal_v16hi                            (rtx, rtx);
extern rtx        gen_reduc_ior_scal_v16hi                            (rtx, rtx);
extern rtx        gen_reduc_xor_scal_v16hi                            (rtx, rtx);
extern rtx        gen_reduc_and_scal_v8si                             (rtx, rtx);
extern rtx        gen_reduc_ior_scal_v8si                             (rtx, rtx);
extern rtx        gen_reduc_xor_scal_v8si                             (rtx, rtx);
extern rtx        gen_reduc_and_scal_v4di                             (rtx, rtx);
extern rtx        gen_reduc_ior_scal_v4di                             (rtx, rtx);
extern rtx        gen_reduc_xor_scal_v4di                             (rtx, rtx);
extern rtx        gen_reduc_and_scal_v64qi                            (rtx, rtx);
extern rtx        gen_reduc_ior_scal_v64qi                            (rtx, rtx);
extern rtx        gen_reduc_xor_scal_v64qi                            (rtx, rtx);
extern rtx        gen_reduc_and_scal_v32hi                            (rtx, rtx);
extern rtx        gen_reduc_ior_scal_v32hi                            (rtx, rtx);
extern rtx        gen_reduc_xor_scal_v32hi                            (rtx, rtx);
extern rtx        gen_reduc_and_scal_v16si                            (rtx, rtx);
extern rtx        gen_reduc_ior_scal_v16si                            (rtx, rtx);
extern rtx        gen_reduc_xor_scal_v16si                            (rtx, rtx);
extern rtx        gen_reduc_and_scal_v8di                             (rtx, rtx);
extern rtx        gen_reduc_ior_scal_v8di                             (rtx, rtx);
extern rtx        gen_reduc_xor_scal_v8di                             (rtx, rtx);
extern rtx        gen_vec_cmpv16sihi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv8siqi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv4siqi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv8diqi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv4diqi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv2diqi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv32hfsi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv16hfhi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv8hfqi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv16sfhi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv8sfqi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv4sfqi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv8dfqi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv4dfqi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv2dfqi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv64qidi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv16qihi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv32qisi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv32hisi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv16hihi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv8hiqi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv32bfsi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv16bfhi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv8bfqi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv32qiv32qi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv16hiv16hi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv8siv8si                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv4div4di                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv16qiv16qi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv8hiv8hi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv4siv4si                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv2div2di                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv8sfv8si                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv4dfv4di                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv4sfv4si                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpv2dfv2di                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv16sihi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv8siqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv4siqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv8diqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv4diqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv2diqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv64qidi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv16qihi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv32qisi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv32hisi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv16hihi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv8hiqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv32qiv32qi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv16hiv16hi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv8siv8si                                (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv4div4di                                (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv16qiv16qi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv8hiv8hi                                (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv4siv4si                                (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpuv2div2di                                (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpeqv2div2di                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_cmpeqv1tiv1ti                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v16sihi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v8siqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v4siqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v8diqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v4diqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v2diqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v16sfhi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v8sfqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v4sfqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v8dfqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v4dfqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v2dfqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v64qidi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v16qihi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v32qisi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v32hisi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v16hihi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v8hiqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v32hfsi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v16hfhi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v8hfqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v32bfsi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v16bfhi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v8bfqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v32qiv32qi                           (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v16hiv16hi                           (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v8siv8si                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v4div4di                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v16qiv16qi                           (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v8hiv8hi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v4siv4si                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v2div2di                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v1tiv1ti                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v8sfv8si                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v4dfv4di                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v4sfv4si                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_v2dfv2di                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_qiqi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_hihi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_sisi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_vcond_mask_didi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_andv16bf3                                       (rtx, rtx, rtx);
static inline rtx gen_andv16bf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_andv16bf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_iorv16bf3                                       (rtx, rtx, rtx);
static inline rtx gen_iorv16bf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_iorv16bf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_xorv16bf3                                       (rtx, rtx, rtx);
static inline rtx gen_xorv16bf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_xorv16bf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_andv8bf3                                        (rtx, rtx, rtx);
static inline rtx gen_andv8bf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_andv8bf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_iorv8bf3                                        (rtx, rtx, rtx);
static inline rtx gen_iorv8bf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_iorv8bf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_xorv8bf3                                        (rtx, rtx, rtx);
static inline rtx gen_xorv8bf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_xorv8bf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_andv16hf3                                       (rtx, rtx, rtx);
static inline rtx gen_andv16hf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_andv16hf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_iorv16hf3                                       (rtx, rtx, rtx);
static inline rtx gen_iorv16hf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_iorv16hf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_xorv16hf3                                       (rtx, rtx, rtx);
static inline rtx gen_xorv16hf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_xorv16hf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_andv8hf3                                        (rtx, rtx, rtx);
static inline rtx gen_andv8hf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_andv8hf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_iorv8hf3                                        (rtx, rtx, rtx);
static inline rtx gen_iorv8hf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_iorv8hf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_xorv8hf3                                        (rtx, rtx, rtx);
static inline rtx gen_xorv8hf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_xorv8hf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_andv8sf3                                        (rtx, rtx, rtx);
extern rtx        gen_andv8sf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_iorv8sf3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv8sf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xorv8sf3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv8sf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_andv4sf3                                        (rtx, rtx, rtx);
extern rtx        gen_andv4sf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_iorv4sf3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv4sf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xorv4sf3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv4sf3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_andv4df3                                        (rtx, rtx, rtx);
extern rtx        gen_andv4df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_iorv4df3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv4df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xorv4df3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv4df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_andv2df3                                        (rtx, rtx, rtx);
extern rtx        gen_andv2df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_iorv2df3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv2df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xorv2df3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv2df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_andv32bf3                                       (rtx, rtx, rtx);
static inline rtx gen_andv32bf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_andv32bf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_iorv32bf3                                       (rtx, rtx, rtx);
static inline rtx gen_iorv32bf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_iorv32bf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_xorv32bf3                                       (rtx, rtx, rtx);
static inline rtx gen_xorv32bf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_xorv32bf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_andv32hf3                                       (rtx, rtx, rtx);
static inline rtx gen_andv32hf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_andv32hf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_iorv32hf3                                       (rtx, rtx, rtx);
static inline rtx gen_iorv32hf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_iorv32hf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_xorv32hf3                                       (rtx, rtx, rtx);
static inline rtx gen_xorv32hf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_xorv32hf3_mask(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_andv16sf3                                       (rtx, rtx, rtx);
extern rtx        gen_andv16sf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_iorv16sf3                                       (rtx, rtx, rtx);
extern rtx        gen_iorv16sf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xorv16sf3                                       (rtx, rtx, rtx);
extern rtx        gen_xorv16sf3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_andv8df3                                        (rtx, rtx, rtx);
extern rtx        gen_andv8df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_iorv8df3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv8df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xorv8df3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv8df3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_copysignv32bf3                                  (rtx, rtx, rtx);
extern rtx        gen_copysignv16bf3                                  (rtx, rtx, rtx);
extern rtx        gen_copysignv8bf3                                   (rtx, rtx, rtx);
extern rtx        gen_copysignv32hf3                                  (rtx, rtx, rtx);
extern rtx        gen_copysignv16hf3                                  (rtx, rtx, rtx);
extern rtx        gen_copysignv8hf3                                   (rtx, rtx, rtx);
extern rtx        gen_copysignv16sf3                                  (rtx, rtx, rtx);
extern rtx        gen_copysignv8sf3                                   (rtx, rtx, rtx);
extern rtx        gen_copysignv4sf3                                   (rtx, rtx, rtx);
extern rtx        gen_copysignv8df3                                   (rtx, rtx, rtx);
extern rtx        gen_copysignv4df3                                   (rtx, rtx, rtx);
extern rtx        gen_copysignv2df3                                   (rtx, rtx, rtx);
extern rtx        gen_xorsignv32bf3                                   (rtx, rtx, rtx);
extern rtx        gen_xorsignv16bf3                                   (rtx, rtx, rtx);
extern rtx        gen_xorsignv8bf3                                    (rtx, rtx, rtx);
extern rtx        gen_xorsignv32hf3                                   (rtx, rtx, rtx);
extern rtx        gen_xorsignv16hf3                                   (rtx, rtx, rtx);
extern rtx        gen_xorsignv8hf3                                    (rtx, rtx, rtx);
extern rtx        gen_xorsignv16sf3                                   (rtx, rtx, rtx);
extern rtx        gen_xorsignv8sf3                                    (rtx, rtx, rtx);
extern rtx        gen_xorsignv4sf3                                    (rtx, rtx, rtx);
extern rtx        gen_xorsignv8df3                                    (rtx, rtx, rtx);
extern rtx        gen_xorsignv4df3                                    (rtx, rtx, rtx);
extern rtx        gen_xorsignv2df3                                    (rtx, rtx, rtx);
extern rtx        gen_signbitv16sf2                                   (rtx, rtx);
extern rtx        gen_signbitv8sf2                                    (rtx, rtx);
extern rtx        gen_signbitv4sf2                                    (rtx, rtx);
extern rtx        gen_andtf3                                          (rtx, rtx, rtx);
extern rtx        gen_iortf3                                          (rtx, rtx, rtx);
extern rtx        gen_xortf3                                          (rtx, rtx, rtx);
extern rtx        gen_fmasf4                                          (rtx, rtx, rtx, rtx);
extern rtx        gen_fmadf4                                          (rtx, rtx, rtx, rtx);
extern rtx        gen_fmav4sf4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_fmav2df4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_fmav8sf4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_fmav4df4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_fmav16sf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fmav8df4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_fmahf4                                          (rtx, rtx, rtx, rtx);
extern rtx        gen_fmav8hf4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_fmav16hf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fmav32hf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fmav8bf4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_fmav16bf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fmav32bf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fmssf4                                          (rtx, rtx, rtx, rtx);
extern rtx        gen_fmsdf4                                          (rtx, rtx, rtx, rtx);
extern rtx        gen_fmsv4sf4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_fmsv2df4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_fmsv8sf4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_fmsv4df4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_fmsv16sf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fmsv8df4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_fmshf4                                          (rtx, rtx, rtx, rtx);
extern rtx        gen_fmsv8hf4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_fmsv16hf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fmsv32hf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fmsv8bf4                                        (rtx, rtx, rtx, rtx);
extern rtx        gen_fmsv16bf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fmsv32bf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmasf4                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmadf4                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmav4sf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmav2df4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmav8sf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmav4df4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmav16sf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmav8df4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmahf4                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmav8hf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmav16hf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmav32hf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmav8bf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmav16bf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmav32bf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmssf4                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmsdf4                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmsv4sf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmsv2df4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmsv8sf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmsv4df4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmsv16sf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmsv8df4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmshf4                                         (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmsv8hf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmsv16hf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmsv32hf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmsv8bf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmsv16bf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_fnmsv32bf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmadd_sf                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmadd_df                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmadd_v4sf                                (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmadd_v2df                                (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmadd_v8sf                                (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmadd_v4df                                (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmadd_v16sf                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmadd_v8df                                (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmsub_sf                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmsub_df                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmsub_v4sf                                (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmsub_v2df                                (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmsub_v8sf                                (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmsub_v4df                                (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmsub_v16sf                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fmsub_v8df                                (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmadd_sf                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmadd_df                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmadd_v4sf                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmadd_v2df                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmadd_v8sf                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmadd_v4df                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmadd_v16sf                              (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmadd_v8df                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmsub_sf                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmsub_df                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmsub_v4sf                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmsub_v2df                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmsub_v8sf                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmsub_v4df                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmsub_v16sf                              (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_fnmsub_v8df                               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmadd_v32hf_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmadd_v32hf_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v16hf_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v16hf_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmadd_v8hf_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmadd_v8hf_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmadd_v16sf_maskz                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmadd_v16sf_maskz_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v8sf_maskz                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v8sf_maskz_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v4sf_maskz                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v4sf_maskz_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmadd_v8df_maskz                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmadd_v8df_maskz_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v4df_maskz                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v4df_maskz_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v2df_maskz                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmadd_v2df_maskz_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmav32hf                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmav16hf                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmav8hf                                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmav16sf                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmav8sf                                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmav4sf                                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmav8df                                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmav4df                                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmav2df                                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmsub_v32hf_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmsub_v32hf_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v16hf_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v16hf_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmsub_v8hf_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmsub_v8hf_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsub_v16sf_maskz                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsub_v16sf_maskz_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v8sf_maskz                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v8sf_maskz_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v4sf_maskz                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v4sf_maskz_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsub_v8df_maskz                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsub_v8df_maskz_round                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v4df_maskz                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v4df_maskz_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v2df_maskz                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsub_v2df_maskz_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmsv32hf                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmsv16hf                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmsv8hf                                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmsv16sf                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmsv8sf                                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmsv4sf                                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmsv8df                                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmsv4df                                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fmsv2df                                    (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fnmadd_v32hf_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fnmadd_v32hf_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v16hf_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v16hf_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fnmadd_v8hf_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fnmadd_v8hf_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmadd_v16sf_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmadd_v16sf_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v8sf_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v8sf_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v4sf_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v4sf_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmadd_v8df_maskz                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmadd_v8df_maskz_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v4df_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v4df_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v2df_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmadd_v2df_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmav32hf                                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmav16hf                                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmav8hf                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmav16sf                                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmav8sf                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmav4sf                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmav8df                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmav4df                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmav2df                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fnmsub_v32hf_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fnmsub_v32hf_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v16hf_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v16hf_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fnmsub_v8hf_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fnmsub_v8hf_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmsub_v16sf_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmsub_v16sf_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v8sf_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v8sf_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v4sf_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v4sf_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmsub_v8df_maskz                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fnmsub_v8df_maskz_round                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v4df_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v4df_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v2df_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fnmsub_v2df_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmsv32hf                                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmsv16hf                                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmsv8hf                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmsv16sf                                  (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmsv8sf                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmsv4sf                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmsv8df                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmsv4df                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_fnmsv2df                                   (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmaddsubv32hf4                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmaddsubv16hf4                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmaddsubv8hf4                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmaddsubv16sf4                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmaddsubv8sf4                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmaddsubv4sf4                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmaddsubv8df4                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmaddsubv4df4                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmaddsubv2df4                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmsubaddv32hf4                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmsubaddv16hf4                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmsubaddv8hf4                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmsubaddv16sf4                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmsubaddv8sf4                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmsubaddv4sf4                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmsubaddv8df4                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmsubaddv4df4                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_fmsubaddv2df4                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fmaddsub_v16sf                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_fmaddsub_v8sf                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_fmaddsub_v4sf                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_fmaddsub_v8df                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_fmaddsub_v4df                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_fmaddsub_v2df                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmaddsub_v32hf_maskz                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmaddsub_v32hf_maskz_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v16hf_maskz                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v16hf_maskz_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddsub_v8hf_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddsub_v8hf_maskz_round            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmaddsub_v16sf_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmaddsub_v16sf_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v8sf_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v8sf_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v4sf_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v4sf_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmaddsub_v8df_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmaddsub_v8df_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v4df_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v4df_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v2df_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddsub_v2df_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmsubadd_v32hf_maskz                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmsubadd_v32hf_maskz_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v16hf_maskz                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v16hf_maskz_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmsubadd_v8hf_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmsubadd_v8hf_maskz_round            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsubadd_v16sf_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsubadd_v16sf_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v8sf_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v8sf_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v4sf_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v4sf_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsubadd_v8df_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fmsubadd_v8df_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v4df_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v4df_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v2df_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmsubadd_v2df_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfmadd_v8hf                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfmadd_v8hf_round                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfmadd_v4sf                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfmadd_v4sf_round                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfmadd_v2df                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfmadd_v2df_round                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfmsub_v8hf                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfmsub_v8hf_round                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfmsub_v4sf                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfmsub_v4sf_round                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfmsub_v2df                               (rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfmsub_v2df_round                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfnmadd_v8hf                              (rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfnmadd_v8hf_round                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfnmadd_v4sf                              (rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfnmadd_v4sf_round                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfnmadd_v2df                              (rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfnmadd_v2df_round                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfnmsub_v8hf                              (rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfnmsub_v8hf_round                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfnmsub_v4sf                              (rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfnmsub_v4sf_round                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfnmsub_v2df                              (rtx, rtx, rtx, rtx);
extern rtx        gen_fmai_vmfnmsub_v2df_round                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v8hf_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v8hf_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v4sf_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v4sf_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v2df_maskz                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfmadd_v2df_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v8hf_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v8hf_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v4sf_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v4sf_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v2df_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vmfnmadd_v2df_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_vmfmadd_v4sf                              (rtx, rtx, rtx, rtx);
extern rtx        gen_fma4i_vmfmadd_v2df                              (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmaddc_v32hf_mask1                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmaddc_v32hf_mask1_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddc_v16hf_mask1                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddc_v16hf_mask1_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddc_v8hf_mask1                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddc_v8hf_mask1_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmaddc_v32hf_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fmaddc_v32hf_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddc_v16hf_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fmaddc_v16hf_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddc_v8hf_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddc_v8hf_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fcmaddc_v32hf_mask1                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fcmaddc_v32hf_mask1_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fcmaddc_v16hf_mask1                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fcmaddc_v16hf_mask1_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmaddc_v8hf_mask1                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmaddc_v8hf_mask1_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fcmaddc_v32hf_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_fcmaddc_v32hf_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fcmaddc_v16hf_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fcmaddc_v16hf_maskz_round              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmaddc_v8hf_maskz                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmaddc_v8hf_maskz_round             (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cmlav32hf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cmla_conjv32hf4                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_cmlav16hf4                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_cmla_conjv16hf4                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_cmlav8hf4                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_cmla_conjv8hf4                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_cmulv32hf3                                      (rtx, rtx, rtx);
extern rtx        gen_cmul_conjv32hf3                                 (rtx, rtx, rtx);
extern rtx        gen_cmulv16hf3                                      (rtx, rtx, rtx);
extern rtx        gen_cmul_conjv16hf3                                 (rtx, rtx, rtx);
extern rtx        gen_cmulv8hf3                                       (rtx, rtx, rtx);
extern rtx        gen_cmul_conjv8hf3                                  (rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddcsh_v8hf_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddcsh_v8hf_maskz_round            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddcsh_v8hf_mask1                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddcsh_v8hf_mask1_round            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmaddcsh_v8hf_maskz                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmaddcsh_v8hf_maskz_round           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmaddcsh_v8hf_mask1                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmaddcsh_v8hf_mask1_round           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmaddcsh_v8hf_mask3                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fcmaddcsh_v8hf_mask3_round           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddcsh_v8hf_mask3                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_fmaddcsh_v8hf_mask3_round            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_unpacks_lo_v32hf                            (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_v16hf                            (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_v8hf                             (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v32hf                            (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v16hf                            (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v8hf                             (rtx, rtx);
extern rtx        gen_lrintv32hfv32hi2                                (rtx, rtx);
extern rtx        gen_lrintv16hfv16hi2                                (rtx, rtx);
extern rtx        gen_lrintv8hfv8hi2                                  (rtx, rtx);
extern rtx        gen_floatv8hiv8hf2                                  (rtx, rtx);
extern rtx        gen_floatunsv8hiv8hf2                               (rtx, rtx);
extern rtx        gen_floatv16hiv16hf2                                (rtx, rtx);
extern rtx        gen_floatunsv16hiv16hf2                             (rtx, rtx);
extern rtx        gen_floatv32hiv32hf2                                (rtx, rtx);
extern rtx        gen_floatunsv32hiv32hf2                             (rtx, rtx);
extern rtx        gen_floatv8siv8hf2                                  (rtx, rtx);
extern rtx        gen_floatunsv8siv8hf2                               (rtx, rtx);
extern rtx        gen_floatv16siv16hf2                                (rtx, rtx);
extern rtx        gen_floatunsv16siv16hf2                             (rtx, rtx);
extern rtx        gen_floatv8div8hf2                                  (rtx, rtx);
extern rtx        gen_floatunsv8div8hf2                               (rtx, rtx);
extern rtx        gen_floatv4siv4hf2                                  (rtx, rtx);
extern rtx        gen_floatunsv4siv4hf2                               (rtx, rtx);
extern rtx        gen_floatv4div4hf2                                  (rtx, rtx);
extern rtx        gen_floatunsv4div4hf2                               (rtx, rtx);
extern rtx        gen_avx512fp16_floatv4siv4hf2                       (rtx, rtx);
extern rtx        gen_avx512fp16_floatunsv4siv4hf2                    (rtx, rtx);
extern rtx        gen_avx512fp16_floatv4div4hf2                       (rtx, rtx);
extern rtx        gen_avx512fp16_floatunsv4div4hf2                    (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtdq2ph_v4si_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtudq2ph_v4si_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtqq2ph_v4di_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuqq2ph_v4di_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_floatv2div2hf2                                  (rtx, rtx);
extern rtx        gen_floatunsv2div2hf2                               (rtx, rtx);
extern rtx        gen_avx512fp16_floatv2div2hf2                       (rtx, rtx);
extern rtx        gen_avx512fp16_floatunsv2div2hf2                    (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtqq2ph_v2di_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtuqq2ph_v2di_mask                 (rtx, rtx, rtx, rtx);
extern rtx        gen_fix_truncv8hfv8hi2                              (rtx, rtx);
extern rtx        gen_fixuns_truncv8hfv8hi2                           (rtx, rtx);
extern rtx        gen_fix_truncv16hfv16hi2                            (rtx, rtx);
extern rtx        gen_fixuns_truncv16hfv16hi2                         (rtx, rtx);
extern rtx        gen_fix_truncv32hfv32hi2                            (rtx, rtx);
extern rtx        gen_fixuns_truncv32hfv32hi2                         (rtx, rtx);
extern rtx        gen_fix_truncv8hfv8si2                              (rtx, rtx);
extern rtx        gen_fixuns_truncv8hfv8si2                           (rtx, rtx);
extern rtx        gen_fix_truncv16hfv16si2                            (rtx, rtx);
extern rtx        gen_fixuns_truncv16hfv16si2                         (rtx, rtx);
extern rtx        gen_fix_truncv8hfv8di2                              (rtx, rtx);
extern rtx        gen_fixuns_truncv8hfv8di2                           (rtx, rtx);
extern rtx        gen_fix_truncv4hfv4si2                              (rtx, rtx);
extern rtx        gen_fixuns_truncv4hfv4si2                           (rtx, rtx);
extern rtx        gen_fix_truncv4hfv4di2                              (rtx, rtx);
extern rtx        gen_fixuns_truncv4hfv4di2                           (rtx, rtx);
extern rtx        gen_fix_truncv2hfv2di2                              (rtx, rtx);
extern rtx        gen_fixuns_truncv2hfv2di2                           (rtx, rtx);
extern rtx        gen_extendv8hfv8df2                                 (rtx, rtx);
extern rtx        gen_extendv16hfv16sf2                               (rtx, rtx);
extern rtx        gen_extendv8hfv8sf2                                 (rtx, rtx);
extern rtx        gen_extendv4hfv4df2                                 (rtx, rtx);
extern rtx        gen_extendv4hfv4sf2                                 (rtx, rtx);
extern rtx        gen_extendv2hfv2df2                                 (rtx, rtx);
extern rtx        gen_truncv8dfv8hf2                                  (rtx, rtx);
extern rtx        gen_truncv16sfv16hf2                                (rtx, rtx);
extern rtx        gen_truncv8sfv8hf2                                  (rtx, rtx);
extern rtx        gen_truncv4dfv4hf2                                  (rtx, rtx);
extern rtx        gen_truncv4sfv4hf2                                  (rtx, rtx);
extern rtx        gen_avx512fp16_truncv4dfv4hf2                       (rtx, rtx);
extern rtx        gen_avx512fp16_truncv4sfv4hf2                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtpd2ph_v4df_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512fp16_vcvtps2ph_v4sf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_truncv2dfv2hf2                                  (rtx, rtx);
extern rtx        gen_avx512fp16_truncv2dfv2hf2                       (rtx, rtx);
extern rtx        gen_avx512fp16_vcvtpd2ph_v2df_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv16siv16sf2                             (rtx, rtx);
extern rtx        gen_floatunsv8siv8sf2                               (rtx, rtx);
extern rtx        gen_floatunsv4siv4sf2                               (rtx, rtx);
extern rtx        gen_fixuns_truncv8sfv8si2                           (rtx, rtx);
extern rtx        gen_fixuns_truncv4sfv4si2                           (rtx, rtx);
static inline rtx gen_floatv2siv2df2                                  (rtx, rtx);
static inline rtx
gen_floatv2siv2df2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_fix_truncv2dfv2si2                              (rtx, rtx);
static inline rtx
gen_fix_truncv2dfv2si2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_avx512dq_floatv2div2sf2                         (rtx, rtx);
extern rtx        gen_avx512dq_floatunsv2div2sf2                      (rtx, rtx);
extern rtx        gen_floatv2div2sf2                                  (rtx, rtx);
extern rtx        gen_floatunsv2div2sf2                               (rtx, rtx);
extern rtx        gen_vec_packs_float_v8di                            (rtx, rtx, rtx);
extern rtx        gen_vec_packu_float_v8di                            (rtx, rtx, rtx);
extern rtx        gen_vec_packs_float_v4di                            (rtx, rtx, rtx);
extern rtx        gen_vec_packu_float_v4di                            (rtx, rtx, rtx);
extern rtx        gen_vec_packs_float_v2di                            (rtx, rtx, rtx);
extern rtx        gen_vec_packu_float_v2di                            (rtx, rtx, rtx);
extern rtx        gen_vec_packs_float_v16si                           (rtx, rtx, rtx);
extern rtx        gen_vec_packu_float_v16si                           (rtx, rtx, rtx);
extern rtx        gen_vec_packs_float_v8si                            (rtx, rtx, rtx);
extern rtx        gen_vec_packu_float_v8si                            (rtx, rtx, rtx);
extern rtx        gen_vec_packs_float_v4si                            (rtx, rtx, rtx);
extern rtx        gen_vec_packu_float_v4si                            (rtx, rtx, rtx);
extern rtx        gen_floatv2div2sf2_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_floatunsv2div2sf2_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_cvtpd2dq256_2                               (rtx, rtx);
extern rtx        gen_fix_truncv2sfv2di2                              (rtx, rtx);
extern rtx        gen_fixuns_truncv2sfv2di2                           (rtx, rtx);
extern rtx        gen_vec_unpack_sfix_trunc_lo_v16sf                  (rtx, rtx);
extern rtx        gen_vec_unpack_ufix_trunc_lo_v16sf                  (rtx, rtx);
extern rtx        gen_vec_unpack_sfix_trunc_lo_v8sf                   (rtx, rtx);
extern rtx        gen_vec_unpack_ufix_trunc_lo_v8sf                   (rtx, rtx);
extern rtx        gen_vec_unpack_sfix_trunc_lo_v4sf                   (rtx, rtx);
extern rtx        gen_vec_unpack_ufix_trunc_lo_v4sf                   (rtx, rtx);
extern rtx        gen_vec_unpack_sfix_trunc_hi_v16sf                  (rtx, rtx);
extern rtx        gen_vec_unpack_ufix_trunc_hi_v16sf                  (rtx, rtx);
extern rtx        gen_vec_unpack_sfix_trunc_hi_v8sf                   (rtx, rtx);
extern rtx        gen_vec_unpack_ufix_trunc_hi_v8sf                   (rtx, rtx);
extern rtx        gen_vec_unpack_sfix_trunc_hi_v4sf                   (rtx, rtx);
extern rtx        gen_vec_unpack_ufix_trunc_hi_v4sf                   (rtx, rtx);
extern rtx        gen_vec_unpack_sfix_trunc_lo_v32hf                  (rtx, rtx);
extern rtx        gen_vec_unpack_ufix_trunc_lo_v32hf                  (rtx, rtx);
extern rtx        gen_vec_unpack_sfix_trunc_lo_v16hf                  (rtx, rtx);
extern rtx        gen_vec_unpack_ufix_trunc_lo_v16hf                  (rtx, rtx);
extern rtx        gen_vec_unpack_sfix_trunc_lo_v8hf                   (rtx, rtx);
extern rtx        gen_vec_unpack_ufix_trunc_lo_v8hf                   (rtx, rtx);
extern rtx        gen_vec_unpack_sfix_trunc_hi_v32hf                  (rtx, rtx);
extern rtx        gen_vec_unpack_ufix_trunc_hi_v32hf                  (rtx, rtx);
extern rtx        gen_vec_unpack_sfix_trunc_hi_v16hf                  (rtx, rtx);
extern rtx        gen_vec_unpack_ufix_trunc_hi_v16hf                  (rtx, rtx);
extern rtx        gen_vec_unpack_sfix_trunc_hi_v8hf                   (rtx, rtx);
extern rtx        gen_vec_unpack_ufix_trunc_hi_v8hf                   (rtx, rtx);
extern rtx        gen_avx_cvttpd2dq256_2                              (rtx, rtx);
extern rtx        gen_sse2_cvtpd2ps                                   (rtx, rtx);
extern rtx        gen_sse2_cvtpd2ps_mask                              (rtx, rtx, rtx, rtx);
extern rtx        gen_truncv8dfv8sf2                                  (rtx, rtx);
extern rtx        gen_truncv4dfv4sf2                                  (rtx, rtx);
extern rtx        gen_extendv8sfv8df2                                 (rtx, rtx);
extern rtx        gen_extendv4sfv4df2                                 (rtx, rtx);
extern rtx        gen_avx512bw_cvtmask2bv64qi                         (rtx, rtx);
extern rtx        gen_avx512vl_cvtmask2bv16qi                         (rtx, rtx);
extern rtx        gen_avx512vl_cvtmask2bv32qi                         (rtx, rtx);
extern rtx        gen_avx512bw_cvtmask2wv32hi                         (rtx, rtx);
extern rtx        gen_avx512vl_cvtmask2wv16hi                         (rtx, rtx);
extern rtx        gen_avx512vl_cvtmask2wv8hi                          (rtx, rtx);
extern rtx        gen_avx512f_cvtmask2dv16si                          (rtx, rtx);
extern rtx        gen_avx512vl_cvtmask2dv8si                          (rtx, rtx);
extern rtx        gen_avx512vl_cvtmask2dv4si                          (rtx, rtx);
extern rtx        gen_avx512f_cvtmask2qv8di                           (rtx, rtx);
extern rtx        gen_avx512vl_cvtmask2qv4di                          (rtx, rtx);
extern rtx        gen_avx512vl_cvtmask2qv2di                          (rtx, rtx);
static inline rtx gen_extendv2sfv2df2                                 (rtx, rtx);
static inline rtx
gen_extendv2sfv2df2(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_vec_unpacks_hi_v4sf                             (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v8sf                             (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v16sf                            (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_v4sf                             (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_v8sf                             (rtx, rtx);
extern rtx        gen_vec_unpacks_float_hi_v32hi                      (rtx, rtx);
extern rtx        gen_vec_unpacks_float_hi_v16hi                      (rtx, rtx);
extern rtx        gen_vec_unpacks_float_hi_v8hi                       (rtx, rtx);
extern rtx        gen_vec_unpacks_float_lo_v32hi                      (rtx, rtx);
extern rtx        gen_vec_unpacks_float_lo_v16hi                      (rtx, rtx);
extern rtx        gen_vec_unpacks_float_lo_v8hi                       (rtx, rtx);
extern rtx        gen_vec_unpacku_float_hi_v32hi                      (rtx, rtx);
extern rtx        gen_vec_unpacku_float_hi_v16hi                      (rtx, rtx);
extern rtx        gen_vec_unpacku_float_hi_v8hi                       (rtx, rtx);
extern rtx        gen_vec_unpacku_float_lo_v32hi                      (rtx, rtx);
extern rtx        gen_vec_unpacku_float_lo_v16hi                      (rtx, rtx);
extern rtx        gen_vec_unpacku_float_lo_v8hi                       (rtx, rtx);
extern rtx        gen_vec_unpacks_float_hi_v4si                       (rtx, rtx);
extern rtx        gen_vec_unpacks_float_lo_v4si                       (rtx, rtx);
extern rtx        gen_vec_unpacks_float_hi_v8si                       (rtx, rtx);
extern rtx        gen_vec_unpacks_float_lo_v8si                       (rtx, rtx);
extern rtx        gen_vec_unpacks_float_hi_v16si                      (rtx, rtx);
extern rtx        gen_vec_unpacks_float_lo_v16si                      (rtx, rtx);
extern rtx        gen_vec_unpacku_float_hi_v4si                       (rtx, rtx);
extern rtx        gen_vec_unpacku_float_lo_v4si                       (rtx, rtx);
extern rtx        gen_vec_unpacku_float_hi_v8si                       (rtx, rtx);
extern rtx        gen_vec_unpacku_float_hi_v16si                      (rtx, rtx);
extern rtx        gen_vec_unpacku_float_lo_v8si                       (rtx, rtx);
extern rtx        gen_vec_unpacku_float_lo_v16si                      (rtx, rtx);
extern rtx        gen_vec_pack_trunc_v8df                             (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_v4df                             (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_v16sf                            (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_v8sf                             (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_v4sf                             (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_v2df                             (rtx, rtx, rtx);
extern rtx        gen_vec_pack_sfix_trunc_v8df                        (rtx, rtx, rtx);
extern rtx        gen_vec_pack_sfix_trunc_v4df                        (rtx, rtx, rtx);
extern rtx        gen_vec_pack_sfix_trunc_v2df                        (rtx, rtx, rtx);
extern rtx        gen_vec_pack_ufix_trunc_v8df                        (rtx, rtx, rtx);
extern rtx        gen_vec_pack_ufix_trunc_v4df                        (rtx, rtx, rtx);
extern rtx        gen_vec_pack_ufix_trunc_v2df                        (rtx, rtx, rtx);
extern rtx        gen_avx512f_vec_pack_sfix_v8df                      (rtx, rtx, rtx);
extern rtx        gen_vec_pack_sfix_v4df                              (rtx, rtx, rtx);
extern rtx        gen_vec_pack_sfix_v2df                              (rtx, rtx, rtx);
extern rtx        gen_sse_movhlps_exp                                 (rtx, rtx, rtx);
extern rtx        gen_sse_movlhps_exp                                 (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv8sf                         (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv8sf                          (rtx, rtx, rtx);
extern rtx        gen_avx_shufps256                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_shufps256_mask                              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_shufps                                      (rtx, rtx, rtx, rtx);
extern rtx        gen_sse_shufps_mask                                 (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse_loadhps_exp                                 (rtx, rtx, rtx);
extern rtx        gen_sse_loadlps_exp                                 (rtx, rtx, rtx);
extern rtx        gen_vec_setv16qi                                    (rtx, rtx, rtx);
extern rtx        gen_vec_setv8hi                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv8hf                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv8bf                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv4si                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv2di                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv4sf                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv2df                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv32qi                                    (rtx, rtx, rtx);
extern rtx        gen_vec_setv16hi                                    (rtx, rtx, rtx);
extern rtx        gen_vec_setv16hf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_setv16bf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_setv8si                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv4di                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv8sf                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv4df                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv64qi                                    (rtx, rtx, rtx);
extern rtx        gen_vec_setv32hi                                    (rtx, rtx, rtx);
extern rtx        gen_vec_setv32hf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_setv32bf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_setv16si                                    (rtx, rtx, rtx);
extern rtx        gen_vec_setv8di                                     (rtx, rtx, rtx);
extern rtx        gen_vec_setv16sf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_setv8df                                     (rtx, rtx, rtx);
extern rtx        gen_avx512dq_vextractf64x2_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_vextracti64x2_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vextractf32x4_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vextracti32x4_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_vextractf32x8_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_vextracti32x8_mask                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vextractf64x4_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vextracti64x4_mask                      (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vextractf128v8si                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vextractf128v8sf                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vextractf128v4di                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vextractf128v4df                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vextractf128v32qi                           (rtx, rtx, rtx);
extern rtx        gen_avx_vextractf128v16hi                           (rtx, rtx, rtx);
extern rtx        gen_avx_vextractf128v8si                            (rtx, rtx, rtx);
extern rtx        gen_avx_vextractf128v4di                            (rtx, rtx, rtx);
extern rtx        gen_avx_vextractf128v8sf                            (rtx, rtx, rtx);
extern rtx        gen_avx_vextractf128v4df                            (rtx, rtx, rtx);
extern rtx        gen_avx_vextractf128v16hf                           (rtx, rtx, rtx);
extern rtx        gen_avx_vextractf128v16bf                           (rtx, rtx, rtx);
extern rtx        gen_vec_extractv64qiqi                              (rtx, rtx, rtx);
extern rtx        gen_vec_extractv32qiqi                              (rtx, rtx, rtx);
extern rtx        gen_vec_extractv16qiqi                              (rtx, rtx, rtx);
extern rtx        gen_vec_extractv32hihi                              (rtx, rtx, rtx);
extern rtx        gen_vec_extractv16hihi                              (rtx, rtx, rtx);
extern rtx        gen_vec_extractv8hihi                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv16sisi                              (rtx, rtx, rtx);
extern rtx        gen_vec_extractv8sisi                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv4sisi                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv8didi                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv4didi                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv2didi                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv32hfhf                              (rtx, rtx, rtx);
extern rtx        gen_vec_extractv16hfhf                              (rtx, rtx, rtx);
extern rtx        gen_vec_extractv8hfhf                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv32bfbf                              (rtx, rtx, rtx);
extern rtx        gen_vec_extractv16bfbf                              (rtx, rtx, rtx);
extern rtx        gen_vec_extractv8bfbf                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv16sfsf                              (rtx, rtx, rtx);
extern rtx        gen_vec_extractv8sfsf                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv4sfsf                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv8dfdf                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv4dfdf                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv2dfdf                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv4titi                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv2titi                               (rtx, rtx, rtx);
extern rtx        gen_vec_extractv32qiv16qi                           (rtx, rtx, rtx);
extern rtx        gen_vec_extractv16hiv8hi                            (rtx, rtx, rtx);
extern rtx        gen_vec_extractv16hfv8hf                            (rtx, rtx, rtx);
extern rtx        gen_vec_extractv16bfv8bf                            (rtx, rtx, rtx);
extern rtx        gen_vec_extractv8siv4si                             (rtx, rtx, rtx);
extern rtx        gen_vec_extractv4div2di                             (rtx, rtx, rtx);
extern rtx        gen_vec_extractv8sfv4sf                             (rtx, rtx, rtx);
extern rtx        gen_vec_extractv4dfv2df                             (rtx, rtx, rtx);
extern rtx        gen_vec_extractv64qiv32qi                           (rtx, rtx, rtx);
extern rtx        gen_vec_extractv32hiv16hi                           (rtx, rtx, rtx);
extern rtx        gen_vec_extractv32hfv16hf                           (rtx, rtx, rtx);
extern rtx        gen_vec_extractv32bfv16bf                           (rtx, rtx, rtx);
extern rtx        gen_vec_extractv16siv8si                            (rtx, rtx, rtx);
extern rtx        gen_vec_extractv8div4di                             (rtx, rtx, rtx);
extern rtx        gen_vec_extractv16sfv8sf                            (rtx, rtx, rtx);
extern rtx        gen_vec_extractv8dfv4df                             (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv4df                         (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv2df                         (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv4df                          (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv2df                          (rtx, rtx, rtx);
extern rtx        gen_avx512f_vternlogv16si_maskz                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv8si_maskz                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv4si_maskz                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vternlogv8di_maskz                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv4di_maskz                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv2di_maskz                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vternlogv16si_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv8si_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv4si_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vternlogv8di_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv4di_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vternlogv2di_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shufps512_mask                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv16sf_maskz                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv16sf_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv8sf_maskz                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv8sf_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4sf_maskz                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4sf_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv8df_maskz                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_fixupimmv8df_maskz_round                (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4df_maskz                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv4df_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv2df_maskz                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_fixupimmv2df_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv4sf_maskz                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv4sf_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv2df_maskz                     (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_sfixupimmv2df_maskz_round               (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shufpd512_mask                          (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_shufpd256                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_shufpd256_mask                              (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_shufpd                                     (rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_shufpd_mask                                (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_loadhpd_exp                                (rtx, rtx, rtx);
extern rtx        gen_sse2_loadlpd_exp                                (rtx, rtx, rtx);
extern rtx        gen_truncv16siv16qi2                                (rtx, rtx);
extern rtx        gen_truncv16siv16hi2                                (rtx, rtx);
extern rtx        gen_truncv8div8si2                                  (rtx, rtx);
extern rtx        gen_truncv8div8hi2                                  (rtx, rtx);
extern rtx        gen_avx512f_ss_truncatev16siv16qi2_mask_store       (rtx, rtx, rtx);
extern rtx        gen_avx512f_truncatev16siv16qi2_mask_store          (rtx, rtx, rtx);
extern rtx        gen_avx512f_us_truncatev16siv16qi2_mask_store       (rtx, rtx, rtx);
extern rtx        gen_avx512f_ss_truncatev16siv16hi2_mask_store       (rtx, rtx, rtx);
extern rtx        gen_avx512f_truncatev16siv16hi2_mask_store          (rtx, rtx, rtx);
extern rtx        gen_avx512f_us_truncatev16siv16hi2_mask_store       (rtx, rtx, rtx);
extern rtx        gen_avx512f_ss_truncatev8div8si2_mask_store         (rtx, rtx, rtx);
extern rtx        gen_avx512f_truncatev8div8si2_mask_store            (rtx, rtx, rtx);
extern rtx        gen_avx512f_us_truncatev8div8si2_mask_store         (rtx, rtx, rtx);
extern rtx        gen_avx512f_ss_truncatev8div8hi2_mask_store         (rtx, rtx, rtx);
extern rtx        gen_avx512f_truncatev8div8hi2_mask_store            (rtx, rtx, rtx);
extern rtx        gen_avx512f_us_truncatev8div8hi2_mask_store         (rtx, rtx, rtx);
extern rtx        gen_truncv32hiv32qi2                                (rtx, rtx);
extern rtx        gen_avx512bw_ss_truncatev32hiv32qi2_mask_store      (rtx, rtx, rtx);
extern rtx        gen_avx512bw_truncatev32hiv32qi2_mask_store         (rtx, rtx, rtx);
extern rtx        gen_avx512bw_us_truncatev32hiv32qi2_mask_store      (rtx, rtx, rtx);
extern rtx        gen_truncv4div4si2                                  (rtx, rtx);
extern rtx        gen_truncv8siv8hi2                                  (rtx, rtx);
extern rtx        gen_truncv16hiv16qi2                                (rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4div4si2_mask_store        (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4div4si2_mask_store           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4div4si2_mask_store        (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev8siv8hi2_mask_store        (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev8siv8hi2_mask_store           (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev8siv8hi2_mask_store        (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev16hiv16qi2_mask_store      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev16hiv16qi2_mask_store         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev16hiv16qi2_mask_store      (rtx, rtx, rtx);
extern rtx        gen_truncv4div4qi2                                  (rtx, rtx);
extern rtx        gen_truncv2div2qi2                                  (rtx, rtx);
extern rtx        gen_truncv8siv8qi2                                  (rtx, rtx);
extern rtx        gen_truncv4siv4qi2                                  (rtx, rtx);
extern rtx        gen_truncv8hiv8qi2                                  (rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev2div2qi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev2div2qi2_mask_store_2         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev2div2qi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4siv4qi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4siv4qi2_mask_store_2         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4siv4qi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4div4qi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4div4qi2_mask_store_2         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4div4qi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev8hiv8qi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev8hiv8qi2_mask_store_2         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev8hiv8qi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev8siv8qi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev8siv8qi2_mask_store_2         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev8siv8qi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_truncv4div4hi2                                  (rtx, rtx);
extern rtx        gen_truncv2div2hi2                                  (rtx, rtx);
extern rtx        gen_truncv4siv4hi2                                  (rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4siv4hi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4siv4hi2_mask_store_2         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4siv4hi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev4div4hi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev4div4hi2_mask_store_2         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev4div4hi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev2div2hi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev2div2hi2_mask_store_2         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev2div2hi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_truncv2div2si2                                  (rtx, rtx);
extern rtx        gen_avx512vl_ss_truncatev2div2si2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512vl_truncatev2div2si2_mask_store_2         (rtx, rtx, rtx);
extern rtx        gen_avx512vl_us_truncatev2div2si2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_truncv8div8qi2                                  (rtx, rtx);
extern rtx        gen_avx512f_ss_truncatev8div16qi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_avx512f_truncatev8div16qi2_mask_store_2         (rtx, rtx, rtx);
extern rtx        gen_avx512f_us_truncatev8div16qi2_mask_store_2      (rtx, rtx, rtx);
extern rtx        gen_negv64qi2                                       (rtx, rtx);
extern rtx        gen_negv32qi2                                       (rtx, rtx);
extern rtx        gen_negv16qi2                                       (rtx, rtx);
extern rtx        gen_negv32hi2                                       (rtx, rtx);
extern rtx        gen_negv16hi2                                       (rtx, rtx);
extern rtx        gen_negv8hi2                                        (rtx, rtx);
extern rtx        gen_negv16si2                                       (rtx, rtx);
extern rtx        gen_negv8si2                                        (rtx, rtx);
extern rtx        gen_negv4si2                                        (rtx, rtx);
extern rtx        gen_negv8di2                                        (rtx, rtx);
extern rtx        gen_negv4di2                                        (rtx, rtx);
extern rtx        gen_negv2di2                                        (rtx, rtx);
extern rtx        gen_addv64qi3                                       (rtx, rtx, rtx);
extern rtx        gen_subv64qi3                                       (rtx, rtx, rtx);
extern rtx        gen_addv32qi3                                       (rtx, rtx, rtx);
extern rtx        gen_subv32qi3                                       (rtx, rtx, rtx);
extern rtx        gen_addv16qi3                                       (rtx, rtx, rtx);
extern rtx        gen_subv16qi3                                       (rtx, rtx, rtx);
extern rtx        gen_addv32hi3                                       (rtx, rtx, rtx);
extern rtx        gen_subv32hi3                                       (rtx, rtx, rtx);
extern rtx        gen_addv16hi3                                       (rtx, rtx, rtx);
extern rtx        gen_subv16hi3                                       (rtx, rtx, rtx);
extern rtx        gen_addv8hi3                                        (rtx, rtx, rtx);
extern rtx        gen_subv8hi3                                        (rtx, rtx, rtx);
extern rtx        gen_addv16si3                                       (rtx, rtx, rtx);
extern rtx        gen_subv16si3                                       (rtx, rtx, rtx);
extern rtx        gen_addv8si3                                        (rtx, rtx, rtx);
extern rtx        gen_subv8si3                                        (rtx, rtx, rtx);
extern rtx        gen_addv4si3                                        (rtx, rtx, rtx);
extern rtx        gen_subv4si3                                        (rtx, rtx, rtx);
extern rtx        gen_addv8di3                                        (rtx, rtx, rtx);
extern rtx        gen_subv8di3                                        (rtx, rtx, rtx);
extern rtx        gen_addv4di3                                        (rtx, rtx, rtx);
extern rtx        gen_subv4di3                                        (rtx, rtx, rtx);
extern rtx        gen_addv2di3                                        (rtx, rtx, rtx);
extern rtx        gen_subv2di3                                        (rtx, rtx, rtx);
extern rtx        gen_cond_addv64qi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv64qi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv32qi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv32qi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv16qi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv16qi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv32hi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv32hi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv16hi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv16hi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv8hi                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv8hi                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv16si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv16si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv8si                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv8si                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv4si                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv4si                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv8di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv8di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv4di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv4di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_addv2di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_subv2di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv16si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv16si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv8si3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv8si3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv4si3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv4si3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv8di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv8di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv4di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv4di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv2di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv2di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv64qi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv64qi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv16qi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv16qi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv32qi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv32qi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv32hi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv32hi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv16hi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv16hi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_addv8hi3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_subv8hi3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ssaddv64qi3                                     (rtx, rtx, rtx);
extern rtx        gen_ssaddv64qi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_usaddv64qi3                                     (rtx, rtx, rtx);
extern rtx        gen_usaddv64qi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sssubv64qi3                                     (rtx, rtx, rtx);
extern rtx        gen_sssubv64qi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ussubv64qi3                                     (rtx, rtx, rtx);
extern rtx        gen_ussubv64qi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ssaddv32qi3                                     (rtx, rtx, rtx);
extern rtx        gen_ssaddv32qi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_usaddv32qi3                                     (rtx, rtx, rtx);
extern rtx        gen_usaddv32qi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sssubv32qi3                                     (rtx, rtx, rtx);
extern rtx        gen_sssubv32qi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ussubv32qi3                                     (rtx, rtx, rtx);
extern rtx        gen_ussubv32qi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ssaddv16qi3                                     (rtx, rtx, rtx);
extern rtx        gen_ssaddv16qi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_usaddv16qi3                                     (rtx, rtx, rtx);
extern rtx        gen_usaddv16qi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sssubv16qi3                                     (rtx, rtx, rtx);
extern rtx        gen_sssubv16qi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ussubv16qi3                                     (rtx, rtx, rtx);
extern rtx        gen_ussubv16qi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ssaddv32hi3                                     (rtx, rtx, rtx);
extern rtx        gen_ssaddv32hi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_usaddv32hi3                                     (rtx, rtx, rtx);
extern rtx        gen_usaddv32hi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sssubv32hi3                                     (rtx, rtx, rtx);
extern rtx        gen_sssubv32hi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ussubv32hi3                                     (rtx, rtx, rtx);
extern rtx        gen_ussubv32hi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ssaddv16hi3                                     (rtx, rtx, rtx);
extern rtx        gen_ssaddv16hi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_usaddv16hi3                                     (rtx, rtx, rtx);
extern rtx        gen_usaddv16hi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sssubv16hi3                                     (rtx, rtx, rtx);
extern rtx        gen_sssubv16hi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ussubv16hi3                                     (rtx, rtx, rtx);
extern rtx        gen_ussubv16hi3_mask                                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ssaddv8hi3                                      (rtx, rtx, rtx);
extern rtx        gen_ssaddv8hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_usaddv8hi3                                      (rtx, rtx, rtx);
extern rtx        gen_usaddv8hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sssubv8hi3                                      (rtx, rtx, rtx);
extern rtx        gen_sssubv8hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ussubv8hi3                                      (rtx, rtx, rtx);
extern rtx        gen_ussubv8hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv64qi3                                       (rtx, rtx, rtx);
extern rtx        gen_mulv32qi3                                       (rtx, rtx, rtx);
extern rtx        gen_mulv16qi3                                       (rtx, rtx, rtx);
extern rtx        gen_cond_mulv8hi                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv16hi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv32hi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv32hi3                                       (rtx, rtx, rtx);
extern rtx        gen_mulv32hi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv16hi3                                       (rtx, rtx, rtx);
extern rtx        gen_mulv16hi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv8hi3                                        (rtx, rtx, rtx);
extern rtx        gen_mulv8hi3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smulv32hi3_highpart                             (rtx, rtx, rtx);
extern rtx        gen_smulv32hi3_highpart_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umulv32hi3_highpart                             (rtx, rtx, rtx);
extern rtx        gen_umulv32hi3_highpart_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smulv16hi3_highpart                             (rtx, rtx, rtx);
extern rtx        gen_smulv16hi3_highpart_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umulv16hi3_highpart                             (rtx, rtx, rtx);
extern rtx        gen_umulv16hi3_highpart_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smulv8hi3_highpart                              (rtx, rtx, rtx);
extern rtx        gen_smulv8hi3_highpart_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umulv8hi3_highpart                              (rtx, rtx, rtx);
extern rtx        gen_umulv8hi3_highpart_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_even_v16si                      (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_even_v16si_mask                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_even_v8si                       (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_even_v8si_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_even_v4si                       (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_even_v4si_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_even_v16si                      (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_even_v16si_mask                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_even_v8si                       (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_even_v8si_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_mulv2siv2di3                             (rtx, rtx, rtx);
extern rtx        gen_sse4_1_mulv2siv2di3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pmaddwd                                    (rtx, rtx, rtx);
extern rtx        gen_sse2_pmaddwd                                    (rtx, rtx, rtx);
extern rtx        gen_cond_mulv8di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv4di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv2di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_mulv8di3                               (rtx, rtx, rtx);
extern rtx        gen_avx512dq_mulv8di3_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_mulv4di3                               (rtx, rtx, rtx);
extern rtx        gen_avx512dq_mulv4di3_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_mulv2di3                               (rtx, rtx, rtx);
extern rtx        gen_avx512dq_mulv2di3_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv16si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv8si                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_mulv4si                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv16si3                                       (rtx, rtx, rtx);
extern rtx        gen_mulv16si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv8si3                                        (rtx, rtx, rtx);
extern rtx        gen_mulv8si3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv4si3                                        (rtx, rtx, rtx);
extern rtx        gen_mulv4si3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_mulv8di3                                        (rtx, rtx, rtx);
extern rtx        gen_mulv4di3                                        (rtx, rtx, rtx);
extern rtx        gen_mulv2di3                                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_hi_v32qi                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_hi_v32qi                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_hi_v16qi                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_hi_v16qi                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_hi_v16hi                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_hi_v16hi                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_hi_v8hi                         (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_hi_v8hi                         (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_hi_v8si                         (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_hi_v8si                         (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_hi_v4si                         (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_hi_v4si                         (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_lo_v32qi                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_lo_v32qi                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_lo_v16qi                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_lo_v16qi                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_lo_v16hi                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_lo_v16hi                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_lo_v8hi                         (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_lo_v8hi                         (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_lo_v8si                         (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_lo_v8si                         (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_lo_v4si                         (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_lo_v4si                         (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_even_v4si                       (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_odd_v16si                       (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_odd_v16si                       (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_odd_v8si                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_odd_v8si                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_smult_odd_v4si                        (rtx, rtx, rtx);
extern rtx        gen_vec_widen_umult_odd_v4si                        (rtx, rtx, rtx);
extern rtx        gen_sdot_prodv16siv32hi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_sdot_prodv8siv16hi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_sdot_prodv4siv8hi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_sdot_prodv2div4si                               (rtx, rtx, rtx, rtx);
extern rtx        gen_uavgv64qi3_ceil                                 (rtx, rtx, rtx);
extern rtx        gen_uavgv32qi3_ceil                                 (rtx, rtx, rtx);
extern rtx        gen_uavgv16qi3_ceil                                 (rtx, rtx, rtx);
extern rtx        gen_uavgv32hi3_ceil                                 (rtx, rtx, rtx);
extern rtx        gen_uavgv16hi3_ceil                                 (rtx, rtx, rtx);
extern rtx        gen_uavgv8hi3_ceil                                  (rtx, rtx, rtx);
extern rtx        gen_usadv16qi                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_usadv32qi                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_usadv64qi                                       (rtx, rtx, rtx, rtx);
extern rtx        gen_ashrv32hi3                                      (rtx, rtx, rtx);
extern rtx        gen_ashrv16si3                                      (rtx, rtx, rtx);
extern rtx        gen_ashrv8di3                                       (rtx, rtx, rtx);
extern rtx        gen_ashrv4di3                                       (rtx, rtx, rtx);
extern rtx        gen_vec_shl_v16qi                                   (rtx, rtx, rtx);
extern rtx        gen_vec_shl_v8hi                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shl_v8hf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shl_v8bf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shl_v4si                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shl_v2di                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shl_v4sf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shl_v2df                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shr_v16qi                                   (rtx, rtx, rtx);
extern rtx        gen_vec_shr_v8hi                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shr_v8hf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shr_v8bf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shr_v4si                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shr_v2di                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shr_v4sf                                    (rtx, rtx, rtx);
extern rtx        gen_vec_shr_v2df                                    (rtx, rtx, rtx);
static inline rtx gen_ashlv1ti3                                       (rtx, rtx, rtx);
static inline rtx
gen_ashlv1ti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_lshrv1ti3                                       (rtx, rtx, rtx);
static inline rtx
gen_lshrv1ti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_ashrv1ti3                                       (rtx, rtx, rtx);
static inline rtx
gen_ashrv1ti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_rotlv1ti3                                       (rtx, rtx, rtx);
static inline rtx
gen_rotlv1ti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
static inline rtx gen_rotrv1ti3                                       (rtx, rtx, rtx);
static inline rtx
gen_rotrv1ti3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_smaxv32qi3                                      (rtx, rtx, rtx);
extern rtx        gen_sminv32qi3                                      (rtx, rtx, rtx);
extern rtx        gen_umaxv32qi3                                      (rtx, rtx, rtx);
extern rtx        gen_uminv32qi3                                      (rtx, rtx, rtx);
extern rtx        gen_smaxv16hi3                                      (rtx, rtx, rtx);
extern rtx        gen_sminv16hi3                                      (rtx, rtx, rtx);
extern rtx        gen_umaxv16hi3                                      (rtx, rtx, rtx);
extern rtx        gen_uminv16hi3                                      (rtx, rtx, rtx);
extern rtx        gen_smaxv8si3                                       (rtx, rtx, rtx);
extern rtx        gen_sminv8si3                                       (rtx, rtx, rtx);
extern rtx        gen_umaxv8si3                                       (rtx, rtx, rtx);
extern rtx        gen_uminv8si3                                       (rtx, rtx, rtx);
extern rtx        gen_smaxv64qi3                                      (rtx, rtx, rtx);
extern rtx        gen_sminv64qi3                                      (rtx, rtx, rtx);
extern rtx        gen_umaxv64qi3                                      (rtx, rtx, rtx);
extern rtx        gen_uminv64qi3                                      (rtx, rtx, rtx);
extern rtx        gen_smaxv32hi3                                      (rtx, rtx, rtx);
extern rtx        gen_sminv32hi3                                      (rtx, rtx, rtx);
extern rtx        gen_umaxv32hi3                                      (rtx, rtx, rtx);
extern rtx        gen_uminv32hi3                                      (rtx, rtx, rtx);
extern rtx        gen_smaxv16si3                                      (rtx, rtx, rtx);
extern rtx        gen_sminv16si3                                      (rtx, rtx, rtx);
extern rtx        gen_umaxv16si3                                      (rtx, rtx, rtx);
extern rtx        gen_uminv16si3                                      (rtx, rtx, rtx);
extern rtx        gen_cond_smaxv64qi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv64qi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_umaxv64qi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_uminv64qi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv32qi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv32qi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_umaxv32qi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_uminv32qi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv16qi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv16qi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_umaxv16qi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_uminv16qi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv32hi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv32hi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_umaxv32hi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_uminv32hi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv16hi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv16hi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_umaxv16hi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_uminv16hi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv8hi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv8hi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_umaxv8hi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_uminv8hi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv16si                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv16si                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_umaxv16si                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_uminv16si                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv8si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv8si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_umaxv8si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_uminv8si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv4si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv4si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_umaxv4si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_uminv4si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv8di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv8di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_umaxv8di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_uminv8di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv4di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv4di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_umaxv4di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_uminv4di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_smaxv2di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_sminv2di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_umaxv2di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_uminv2di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv64qi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv64qi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umaxv64qi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_uminv64qi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv32qi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv32qi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umaxv32qi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_uminv32qi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv16qi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv16qi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umaxv16qi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_uminv16qi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv32hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv32hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umaxv32hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_uminv32hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv16hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv16hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umaxv16hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_uminv16hi3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv8hi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv8hi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umaxv8hi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_uminv8hi3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv16si3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv16si3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umaxv16si3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_uminv16si3_mask                                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv8si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv8si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umaxv8si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_uminv8si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv4si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv4si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umaxv4si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_uminv4si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv8di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv8di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umaxv8di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_uminv8di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv4di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv4di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umaxv4di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_uminv4di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv2di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sminv2di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_umaxv2di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_uminv2di3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv8di3                                       (rtx, rtx, rtx);
extern rtx        gen_sminv8di3                                       (rtx, rtx, rtx);
extern rtx        gen_umaxv8di3                                       (rtx, rtx, rtx);
extern rtx        gen_uminv8di3                                       (rtx, rtx, rtx);
extern rtx        gen_smaxv4di3                                       (rtx, rtx, rtx);
extern rtx        gen_sminv4di3                                       (rtx, rtx, rtx);
extern rtx        gen_umaxv4di3                                       (rtx, rtx, rtx);
extern rtx        gen_uminv4di3                                       (rtx, rtx, rtx);
extern rtx        gen_smaxv2di3                                       (rtx, rtx, rtx);
extern rtx        gen_sminv2di3                                       (rtx, rtx, rtx);
extern rtx        gen_umaxv2di3                                       (rtx, rtx, rtx);
extern rtx        gen_uminv2di3                                       (rtx, rtx, rtx);
extern rtx        gen_smaxv16qi3                                      (rtx, rtx, rtx);
extern rtx        gen_sminv16qi3                                      (rtx, rtx, rtx);
extern rtx        gen_smaxv8hi3                                       (rtx, rtx, rtx);
extern rtx        gen_sminv8hi3                                       (rtx, rtx, rtx);
extern rtx        gen_smaxv4si3                                       (rtx, rtx, rtx);
extern rtx        gen_sminv4si3                                       (rtx, rtx, rtx);
extern rtx        gen_umaxv16qi3                                      (rtx, rtx, rtx);
extern rtx        gen_uminv16qi3                                      (rtx, rtx, rtx);
extern rtx        gen_umaxv8hi3                                       (rtx, rtx, rtx);
extern rtx        gen_uminv8hi3                                       (rtx, rtx, rtx);
extern rtx        gen_umaxv4si3                                       (rtx, rtx, rtx);
extern rtx        gen_uminv4si3                                       (rtx, rtx, rtx);
extern rtx        gen_avx512bw_eqv64qi3                               (rtx, rtx, rtx);
extern rtx        gen_avx512bw_eqv64qi3_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv16qi3                               (rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv16qi3_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv32qi3                               (rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv32qi3_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_eqv32hi3                               (rtx, rtx, rtx);
extern rtx        gen_avx512bw_eqv32hi3_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv16hi3                               (rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv16hi3_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv8hi3                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv8hi3_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_eqv16si3                                (rtx, rtx, rtx);
extern rtx        gen_avx512f_eqv16si3_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv8si3                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv8si3_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv4si3                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv4si3_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_eqv8di3                                 (rtx, rtx, rtx);
extern rtx        gen_avx512f_eqv8di3_mask                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv4di3                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv4di3_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv2di3                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_eqv2di3_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_gtv16si3                                (rtx, rtx, rtx);
extern rtx        gen_avx512f_gtv16si3_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv8si3                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv8si3_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv4si3                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv4si3_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_gtv8di3                                 (rtx, rtx, rtx);
extern rtx        gen_avx512f_gtv8di3_mask                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv4di3                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv4di3_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv2di3                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv2di3_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_gtv64qi3                               (rtx, rtx, rtx);
extern rtx        gen_avx512bw_gtv64qi3_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv16qi3                               (rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv16qi3_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv32qi3                               (rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv32qi3_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_gtv32hi3                               (rtx, rtx, rtx);
extern rtx        gen_avx512bw_gtv32hi3_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv16hi3                               (rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv16hi3_mask                          (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv8hi3                                (rtx, rtx, rtx);
extern rtx        gen_avx512vl_gtv8hi3_mask                           (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv16qi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv8hi                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv4si                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv2di                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv4sf                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv2df                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv8hf                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv32qi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv16hi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv8si                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv4di                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv8sf                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv4df                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv16hf                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv16sf                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv8df                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv16si                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv8di                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv32hi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv64qi                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_permv32hf                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_one_cmplv16si2                                  (rtx, rtx);
extern rtx        gen_one_cmplv8di2                                   (rtx, rtx);
extern rtx        gen_one_cmplv64qi2                                  (rtx, rtx);
extern rtx        gen_one_cmplv32qi2                                  (rtx, rtx);
extern rtx        gen_one_cmplv16qi2                                  (rtx, rtx);
extern rtx        gen_one_cmplv32hi2                                  (rtx, rtx);
extern rtx        gen_one_cmplv16hi2                                  (rtx, rtx);
extern rtx        gen_one_cmplv8hi2                                   (rtx, rtx);
extern rtx        gen_one_cmplv8si2                                   (rtx, rtx);
extern rtx        gen_one_cmplv4si2                                   (rtx, rtx);
extern rtx        gen_one_cmplv4di2                                   (rtx, rtx);
extern rtx        gen_one_cmplv2di2                                   (rtx, rtx);
extern rtx        gen_avx512bw_andnotv64qi3                           (rtx, rtx, rtx);
extern rtx        gen_avx2_andnotv32qi3                               (rtx, rtx, rtx);
extern rtx        gen_sse2_andnotv16qi3                               (rtx, rtx, rtx);
extern rtx        gen_avx512bw_andnotv32hi3                           (rtx, rtx, rtx);
extern rtx        gen_avx2_andnotv16hi3                               (rtx, rtx, rtx);
extern rtx        gen_sse2_andnotv8hi3                                (rtx, rtx, rtx);
extern rtx        gen_avx512f_andnotv16si3                            (rtx, rtx, rtx);
extern rtx        gen_avx2_andnotv8si3                                (rtx, rtx, rtx);
extern rtx        gen_sse2_andnotv4si3                                (rtx, rtx, rtx);
extern rtx        gen_avx512f_andnotv8di3                             (rtx, rtx, rtx);
extern rtx        gen_avx2_andnotv4di3                                (rtx, rtx, rtx);
extern rtx        gen_sse2_andnotv2di3                                (rtx, rtx, rtx);
extern rtx        gen_andnv16si3                                      (rtx, rtx, rtx);
extern rtx        gen_andnv8di3                                       (rtx, rtx, rtx);
extern rtx        gen_andnv64qi3                                      (rtx, rtx, rtx);
extern rtx        gen_andnv32qi3                                      (rtx, rtx, rtx);
extern rtx        gen_andnv16qi3                                      (rtx, rtx, rtx);
extern rtx        gen_andnv32hi3                                      (rtx, rtx, rtx);
extern rtx        gen_andnv16hi3                                      (rtx, rtx, rtx);
extern rtx        gen_andnv8hi3                                       (rtx, rtx, rtx);
extern rtx        gen_andnv8si3                                       (rtx, rtx, rtx);
extern rtx        gen_andnv4si3                                       (rtx, rtx, rtx);
extern rtx        gen_andnv4di3                                       (rtx, rtx, rtx);
extern rtx        gen_andnv2di3                                       (rtx, rtx, rtx);
extern rtx        gen_avx512f_andnotv16si3_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_andnotv8si3_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_andnotv4si3_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_andnotv8di3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_andnotv4di3_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_andnotv2di3_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_andv16si3                                       (rtx, rtx, rtx);
extern rtx        gen_iorv16si3                                       (rtx, rtx, rtx);
extern rtx        gen_xorv16si3                                       (rtx, rtx, rtx);
extern rtx        gen_andv8di3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv8di3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv8di3                                        (rtx, rtx, rtx);
extern rtx        gen_andv64qi3                                       (rtx, rtx, rtx);
extern rtx        gen_iorv64qi3                                       (rtx, rtx, rtx);
extern rtx        gen_xorv64qi3                                       (rtx, rtx, rtx);
extern rtx        gen_andv32qi3                                       (rtx, rtx, rtx);
extern rtx        gen_iorv32qi3                                       (rtx, rtx, rtx);
extern rtx        gen_xorv32qi3                                       (rtx, rtx, rtx);
extern rtx        gen_andv16qi3                                       (rtx, rtx, rtx);
extern rtx        gen_iorv16qi3                                       (rtx, rtx, rtx);
extern rtx        gen_xorv16qi3                                       (rtx, rtx, rtx);
extern rtx        gen_andv32hi3                                       (rtx, rtx, rtx);
extern rtx        gen_iorv32hi3                                       (rtx, rtx, rtx);
extern rtx        gen_xorv32hi3                                       (rtx, rtx, rtx);
extern rtx        gen_andv16hi3                                       (rtx, rtx, rtx);
extern rtx        gen_iorv16hi3                                       (rtx, rtx, rtx);
extern rtx        gen_xorv16hi3                                       (rtx, rtx, rtx);
extern rtx        gen_andv8hi3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv8hi3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv8hi3                                        (rtx, rtx, rtx);
extern rtx        gen_andv8si3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv8si3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv8si3                                        (rtx, rtx, rtx);
extern rtx        gen_andv4si3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv4si3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv4si3                                        (rtx, rtx, rtx);
extern rtx        gen_andv4di3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv4di3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv4di3                                        (rtx, rtx, rtx);
extern rtx        gen_andv2di3                                        (rtx, rtx, rtx);
extern rtx        gen_iorv2di3                                        (rtx, rtx, rtx);
extern rtx        gen_xorv2di3                                        (rtx, rtx, rtx);
extern rtx        gen_cond_andv16si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_iorv16si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_xorv16si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_andv8si                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_iorv8si                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_xorv8si                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_andv4si                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_iorv4si                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_xorv4si                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_andv8di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_iorv8di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_xorv8di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_andv4di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_iorv4di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_xorv4di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_andv2di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_iorv2di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_xorv2di                                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_andv16si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_iorv16si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xorv16si3_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_andv8si3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_iorv8si3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xorv8si3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_andv4si3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_iorv4si3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xorv4si3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_andv8di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_iorv8di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xorv8di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_andv4di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_iorv4di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xorv4di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_andv2di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_iorv2di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_xorv2di3_mask                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_one_cmplv1ti2                                   (rtx, rtx);
extern rtx        gen_vec_pack_trunc_v32hi                            (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_v16hi                            (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_v8hi                             (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_v16si                            (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_v8si                             (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_v4si                             (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_v8di                             (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_v4di                             (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_v2di                             (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_qi                               (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_hi                               (rtx, rtx, rtx);
extern rtx        gen_vec_pack_trunc_si                               (rtx, rtx, rtx);
extern rtx        gen_vec_pack_sbool_trunc_qi                         (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv32qi                        (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv16hi                        (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv8si                         (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_highv4di                         (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv32qi                         (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv16hi                         (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv8si                          (rtx, rtx, rtx);
extern rtx        gen_vec_interleave_lowv4di                          (rtx, rtx, rtx);
extern rtx        gen_avx512dq_vinsertf64x2_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_vinserti64x2_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vinsertf32x4_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vinserti32x4_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_vinsertf32x8_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_vinserti32x8_mask                      (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vinsertf64x4_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vinserti64x4_mask                       (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_shuf_i64x2_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512dq_shuf_f64x2_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shuf_f64x2_mask                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shuf_i64x2_mask                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_shuf_i32x4_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_shuf_f32x4_mask                        (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shuf_f32x4_mask                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_shuf_i32x4_mask                         (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_pshufdv3_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_pshufdv3_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pshufdv3                                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_pshufd_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_pshufd                                     (rtx, rtx, rtx);
extern rtx        gen_avx512vl_pshuflwv3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pshuflwv3                                  (rtx, rtx, rtx);
extern rtx        gen_avx512vl_pshuflw_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_pshuflw                                    (rtx, rtx, rtx);
extern rtx        gen_avx2_pshufhwv3                                  (rtx, rtx, rtx);
extern rtx        gen_avx512vl_pshufhwv3_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_pshufhw_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_pshufhw                                    (rtx, rtx, rtx);
extern rtx        gen_sse2_loadd                                      (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_v64qi                            (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_v32qi                            (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_v16qi                            (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_v32hi                            (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_v16hi                            (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_v8hi                             (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_v16si                            (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_v8si                             (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_v4si                             (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v64qi                            (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v32qi                            (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v16qi                            (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v32hi                            (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v16hi                            (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v8hi                             (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v16si                            (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v8si                             (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_v4si                             (rtx, rtx);
extern rtx        gen_vec_unpacku_lo_v64qi                            (rtx, rtx);
extern rtx        gen_vec_unpacku_lo_v32qi                            (rtx, rtx);
extern rtx        gen_vec_unpacku_lo_v16qi                            (rtx, rtx);
extern rtx        gen_vec_unpacku_lo_v32hi                            (rtx, rtx);
extern rtx        gen_vec_unpacku_lo_v16hi                            (rtx, rtx);
extern rtx        gen_vec_unpacku_lo_v8hi                             (rtx, rtx);
extern rtx        gen_vec_unpacku_lo_v16si                            (rtx, rtx);
extern rtx        gen_vec_unpacku_lo_v8si                             (rtx, rtx);
extern rtx        gen_vec_unpacku_lo_v4si                             (rtx, rtx);
extern rtx        gen_vec_unpacks_sbool_lo_qi                         (rtx, rtx, rtx);
extern rtx        gen_vec_unpacks_lo_hi                               (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_si                               (rtx, rtx);
extern rtx        gen_vec_unpacks_lo_di                               (rtx, rtx);
extern rtx        gen_vec_unpacku_hi_v64qi                            (rtx, rtx);
extern rtx        gen_vec_unpacku_hi_v32qi                            (rtx, rtx);
extern rtx        gen_vec_unpacku_hi_v16qi                            (rtx, rtx);
extern rtx        gen_vec_unpacku_hi_v32hi                            (rtx, rtx);
extern rtx        gen_vec_unpacku_hi_v16hi                            (rtx, rtx);
extern rtx        gen_vec_unpacku_hi_v8hi                             (rtx, rtx);
extern rtx        gen_vec_unpacku_hi_v16si                            (rtx, rtx);
extern rtx        gen_vec_unpacku_hi_v8si                             (rtx, rtx);
extern rtx        gen_vec_unpacku_hi_v4si                             (rtx, rtx);
extern rtx        gen_vec_unpacks_sbool_hi_qi                         (rtx, rtx, rtx);
extern rtx        gen_vec_unpacks_hi_hi                               (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_si                               (rtx, rtx);
extern rtx        gen_vec_unpacks_hi_di                               (rtx, rtx);
extern rtx        gen_avx512bw_uavgv64qi3                             (rtx, rtx, rtx);
extern rtx        gen_avx512bw_uavgv64qi3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_uavgv32qi3                                 (rtx, rtx, rtx);
extern rtx        gen_avx2_uavgv32qi3_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_uavgv16qi3                                 (rtx, rtx, rtx);
extern rtx        gen_sse2_uavgv16qi3_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_uavgv32hi3                             (rtx, rtx, rtx);
extern rtx        gen_avx512bw_uavgv32hi3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_uavgv16hi3                                 (rtx, rtx, rtx);
extern rtx        gen_avx2_uavgv16hi3_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_uavgv8hi3                                  (rtx, rtx, rtx);
extern rtx        gen_sse2_uavgv8hi3_mask                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_psadbw                                  (rtx, rtx, rtx);
extern rtx        gen_avx2_psadbw                                     (rtx, rtx, rtx);
extern rtx        gen_sse2_psadbw                                     (rtx, rtx, rtx);
extern rtx        gen_sse2_maskmovdqu                                 (rtx, rtx, rtx);
extern rtx        gen_ssse3_pmulhrswv8hi3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pmulhrswv16hi3_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_ssse3_pmulhrswv8hi3                             (rtx, rtx, rtx);
extern rtx        gen_avx2_pmulhrswv16hi3                             (rtx, rtx, rtx);
extern rtx        gen_smulhrsv32hi3                                   (rtx, rtx, rtx);
extern rtx        gen_smulhrsv16hi3                                   (rtx, rtx, rtx);
extern rtx        gen_smulhrsv8hi3                                    (rtx, rtx, rtx);
static inline rtx gen_smulhrsv4hi3                                    (rtx, rtx, rtx);
static inline rtx
gen_smulhrsv4hi3(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c))
{
  return 0;
}
extern rtx        gen_ssse3_pmulhrswv4hi3                             (rtx, rtx, rtx);
extern rtx        gen_smulhrsv2hi3                                    (rtx, rtx, rtx);
extern rtx        gen_ssse3_pshufbv8qi3                               (rtx, rtx, rtx);
extern rtx        gen_absv64qi2                                       (rtx, rtx);
extern rtx        gen_absv32qi2                                       (rtx, rtx);
extern rtx        gen_absv16qi2                                       (rtx, rtx);
extern rtx        gen_absv32hi2                                       (rtx, rtx);
extern rtx        gen_absv16hi2                                       (rtx, rtx);
extern rtx        gen_absv8hi2                                        (rtx, rtx);
extern rtx        gen_absv16si2                                       (rtx, rtx);
extern rtx        gen_absv8si2                                        (rtx, rtx);
extern rtx        gen_absv4si2                                        (rtx, rtx);
extern rtx        gen_absv8di2                                        (rtx, rtx);
extern rtx        gen_absv4di2                                        (rtx, rtx);
extern rtx        gen_absv2di2                                        (rtx, rtx);
extern rtx        gen_avx2_pblendw                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pblendph                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pblendbf                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pblendw_1                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pblendph_1                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_pblendbf_1                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_extendv16qiv16hi2                               (rtx, rtx);
extern rtx        gen_zero_extendv16qiv16hi2                          (rtx, rtx);
extern rtx        gen_extendv32qiv32hi2                               (rtx, rtx);
extern rtx        gen_zero_extendv32qiv32hi2                          (rtx, rtx);
extern rtx        gen_extendv8qiv8hi2                                 (rtx, rtx);
extern rtx        gen_zero_extendv8qiv8hi2                            (rtx, rtx);
extern rtx        gen_extendv16qiv16si2                               (rtx, rtx);
extern rtx        gen_zero_extendv16qiv16si2                          (rtx, rtx);
extern rtx        gen_extendv8qiv8si2                                 (rtx, rtx);
extern rtx        gen_zero_extendv8qiv8si2                            (rtx, rtx);
extern rtx        gen_extendv4qiv4si2                                 (rtx, rtx);
extern rtx        gen_zero_extendv4qiv4si2                            (rtx, rtx);
extern rtx        gen_extendv16hiv16si2                               (rtx, rtx);
extern rtx        gen_zero_extendv16hiv16si2                          (rtx, rtx);
extern rtx        gen_extendv8hiv8si2                                 (rtx, rtx);
extern rtx        gen_zero_extendv8hiv8si2                            (rtx, rtx);
extern rtx        gen_extendv4hiv4si2                                 (rtx, rtx);
extern rtx        gen_zero_extendv4hiv4si2                            (rtx, rtx);
extern rtx        gen_extendv8qiv8di2                                 (rtx, rtx);
extern rtx        gen_zero_extendv8qiv8di2                            (rtx, rtx);
extern rtx        gen_extendv4qiv4di2                                 (rtx, rtx);
extern rtx        gen_zero_extendv4qiv4di2                            (rtx, rtx);
extern rtx        gen_extendv2qiv2di2                                 (rtx, rtx);
extern rtx        gen_zero_extendv2qiv2di2                            (rtx, rtx);
extern rtx        gen_extendv8hiv8di2                                 (rtx, rtx);
extern rtx        gen_zero_extendv8hiv8di2                            (rtx, rtx);
extern rtx        gen_extendv4hiv4di2                                 (rtx, rtx);
extern rtx        gen_zero_extendv4hiv4di2                            (rtx, rtx);
extern rtx        gen_extendv2hiv2di2                                 (rtx, rtx);
extern rtx        gen_zero_extendv2hiv2di2                            (rtx, rtx);
extern rtx        gen_extendv8siv8di2                                 (rtx, rtx);
extern rtx        gen_zero_extendv8siv8di2                            (rtx, rtx);
extern rtx        gen_extendv4siv4di2                                 (rtx, rtx);
extern rtx        gen_zero_extendv4siv4di2                            (rtx, rtx);
extern rtx        gen_extendv2siv2di2                                 (rtx, rtx);
extern rtx        gen_zero_extendv2siv2di2                            (rtx, rtx);
extern rtx        gen_sse4_1_ptestzv16qi                              (rtx, rtx);
extern rtx        gen_sse4_1_ptestzv8hi                               (rtx, rtx);
extern rtx        gen_sse4_1_ptestzv4si                               (rtx, rtx);
extern rtx        gen_sse4_1_ptestzv2di                               (rtx, rtx);
extern rtx        gen_sse4_1_ptestzv1ti                               (rtx, rtx);
extern rtx        gen_sse4_1_ptestzv4sf                               (rtx, rtx);
extern rtx        gen_sse4_1_ptestzv2df                               (rtx, rtx);
extern rtx        gen_avx_ptestzv32qi                                 (rtx, rtx);
extern rtx        gen_avx_ptestzv16hi                                 (rtx, rtx);
extern rtx        gen_avx_ptestzv8si                                  (rtx, rtx);
extern rtx        gen_avx_ptestzv4di                                  (rtx, rtx);
extern rtx        gen_avx_ptestzv2ti                                  (rtx, rtx);
extern rtx        gen_avx_ptestzv8sf                                  (rtx, rtx);
extern rtx        gen_avx_ptestzv4df                                  (rtx, rtx);
extern rtx        gen_sse4_1_ptestcv16qi                              (rtx, rtx);
extern rtx        gen_sse4_1_ptestcv8hi                               (rtx, rtx);
extern rtx        gen_sse4_1_ptestcv4si                               (rtx, rtx);
extern rtx        gen_sse4_1_ptestcv2di                               (rtx, rtx);
extern rtx        gen_sse4_1_ptestcv1ti                               (rtx, rtx);
extern rtx        gen_sse4_1_ptestcv4sf                               (rtx, rtx);
extern rtx        gen_sse4_1_ptestcv2df                               (rtx, rtx);
extern rtx        gen_avx_ptestcv32qi                                 (rtx, rtx);
extern rtx        gen_avx_ptestcv16hi                                 (rtx, rtx);
extern rtx        gen_avx_ptestcv8si                                  (rtx, rtx);
extern rtx        gen_avx_ptestcv4di                                  (rtx, rtx);
extern rtx        gen_avx_ptestcv2ti                                  (rtx, rtx);
extern rtx        gen_avx_ptestcv8sf                                  (rtx, rtx);
extern rtx        gen_avx_ptestcv4df                                  (rtx, rtx);
extern rtx        gen_sse4_1_ptestv16qi                               (rtx, rtx);
extern rtx        gen_sse4_1_ptestv8hi                                (rtx, rtx);
extern rtx        gen_sse4_1_ptestv4si                                (rtx, rtx);
extern rtx        gen_sse4_1_ptestv2di                                (rtx, rtx);
extern rtx        gen_sse4_1_ptestv1ti                                (rtx, rtx);
extern rtx        gen_sse4_1_ptestv4sf                                (rtx, rtx);
extern rtx        gen_sse4_1_ptestv2df                                (rtx, rtx);
extern rtx        gen_avx_ptestv32qi                                  (rtx, rtx);
extern rtx        gen_avx_ptestv16hi                                  (rtx, rtx);
extern rtx        gen_avx_ptestv8si                                   (rtx, rtx);
extern rtx        gen_avx_ptestv4di                                   (rtx, rtx);
extern rtx        gen_avx_ptestv2ti                                   (rtx, rtx);
extern rtx        gen_avx_ptestv8sf                                   (rtx, rtx);
extern rtx        gen_avx_ptestv4df                                   (rtx, rtx);
extern rtx        gen_nearbyintv32hf2                                 (rtx, rtx);
extern rtx        gen_nearbyintv16hf2                                 (rtx, rtx);
extern rtx        gen_nearbyintv8hf2                                  (rtx, rtx);
extern rtx        gen_nearbyintv16sf2                                 (rtx, rtx);
extern rtx        gen_nearbyintv8sf2                                  (rtx, rtx);
extern rtx        gen_nearbyintv4sf2                                  (rtx, rtx);
extern rtx        gen_nearbyintv8df2                                  (rtx, rtx);
extern rtx        gen_nearbyintv4df2                                  (rtx, rtx);
extern rtx        gen_nearbyintv2df2                                  (rtx, rtx);
extern rtx        gen_rintv32hf2                                      (rtx, rtx);
extern rtx        gen_rintv16hf2                                      (rtx, rtx);
extern rtx        gen_rintv8hf2                                       (rtx, rtx);
extern rtx        gen_rintv16sf2                                      (rtx, rtx);
extern rtx        gen_rintv8sf2                                       (rtx, rtx);
extern rtx        gen_rintv4sf2                                       (rtx, rtx);
extern rtx        gen_rintv8df2                                       (rtx, rtx);
extern rtx        gen_rintv4df2                                       (rtx, rtx);
extern rtx        gen_rintv2df2                                       (rtx, rtx);
extern rtx        gen_lrintv16sfv16si2                                (rtx, rtx);
extern rtx        gen_lrintv8sfv8si2                                  (rtx, rtx);
extern rtx        gen_lrintv4sfv4si2                                  (rtx, rtx);
extern rtx        gen_lrintv8dfv8di2                                  (rtx, rtx);
extern rtx        gen_lrintv4dfv4di2                                  (rtx, rtx);
extern rtx        gen_lrintv2dfv2di2                                  (rtx, rtx);
extern rtx        gen_avx_roundps_sfix256                             (rtx, rtx, rtx);
extern rtx        gen_sse4_1_roundps_sfix                             (rtx, rtx, rtx);
extern rtx        gen_avx512f_roundps512                              (rtx, rtx, rtx);
extern rtx        gen_avx512f_roundpd512                              (rtx, rtx, rtx);
extern rtx        gen_avx512f_roundps512_sfix                         (rtx, rtx, rtx);
extern rtx        gen_avx512f_roundpd_vec_pack_sfix512                (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_roundpd_vec_pack_sfix256                    (rtx, rtx, rtx, rtx);
extern rtx        gen_sse4_1_roundpd_vec_pack_sfix                    (rtx, rtx, rtx, rtx);
extern rtx        gen_floorv32hf2                                     (rtx, rtx);
extern rtx        gen_floorv16hf2                                     (rtx, rtx);
extern rtx        gen_floorv8hf2                                      (rtx, rtx);
extern rtx        gen_floorv16sf2                                     (rtx, rtx);
extern rtx        gen_floorv8sf2                                      (rtx, rtx);
extern rtx        gen_floorv4sf2                                      (rtx, rtx);
extern rtx        gen_floorv8df2                                      (rtx, rtx);
extern rtx        gen_floorv4df2                                      (rtx, rtx);
extern rtx        gen_floorv2df2                                      (rtx, rtx);
extern rtx        gen_lfloorv32hfv32hi2                               (rtx, rtx);
extern rtx        gen_lfloorv16hfv16hi2                               (rtx, rtx);
extern rtx        gen_lfloorv8hfv8hi2                                 (rtx, rtx);
extern rtx        gen_lfloorv16sfv16si2                               (rtx, rtx);
extern rtx        gen_lfloorv8sfv8si2                                 (rtx, rtx);
extern rtx        gen_lfloorv4sfv4si2                                 (rtx, rtx);
extern rtx        gen_lfloorv8dfv8di2                                 (rtx, rtx);
extern rtx        gen_lfloorv4dfv4di2                                 (rtx, rtx);
extern rtx        gen_lfloorv2dfv2di2                                 (rtx, rtx);
extern rtx        gen_ceilv32hf2                                      (rtx, rtx);
extern rtx        gen_ceilv16hf2                                      (rtx, rtx);
extern rtx        gen_ceilv8hf2                                       (rtx, rtx);
extern rtx        gen_ceilv16sf2                                      (rtx, rtx);
extern rtx        gen_ceilv8sf2                                       (rtx, rtx);
extern rtx        gen_ceilv4sf2                                       (rtx, rtx);
extern rtx        gen_ceilv8df2                                       (rtx, rtx);
extern rtx        gen_ceilv4df2                                       (rtx, rtx);
extern rtx        gen_ceilv2df2                                       (rtx, rtx);
extern rtx        gen_lceilv32hfv32hi2                                (rtx, rtx);
extern rtx        gen_lceilv16hfv16hi2                                (rtx, rtx);
extern rtx        gen_lceilv8hfv8hi2                                  (rtx, rtx);
extern rtx        gen_lceilv16sfv16si2                                (rtx, rtx);
extern rtx        gen_lceilv8sfv8si2                                  (rtx, rtx);
extern rtx        gen_lceilv4sfv4si2                                  (rtx, rtx);
extern rtx        gen_lceilv8dfv8di2                                  (rtx, rtx);
extern rtx        gen_lceilv4dfv4di2                                  (rtx, rtx);
extern rtx        gen_lceilv2dfv2di2                                  (rtx, rtx);
extern rtx        gen_btruncv32hf2                                    (rtx, rtx);
extern rtx        gen_btruncv16hf2                                    (rtx, rtx);
extern rtx        gen_btruncv8hf2                                     (rtx, rtx);
extern rtx        gen_btruncv16sf2                                    (rtx, rtx);
extern rtx        gen_btruncv8sf2                                     (rtx, rtx);
extern rtx        gen_btruncv4sf2                                     (rtx, rtx);
extern rtx        gen_btruncv8df2                                     (rtx, rtx);
extern rtx        gen_btruncv4df2                                     (rtx, rtx);
extern rtx        gen_btruncv2df2                                     (rtx, rtx);
extern rtx        gen_roundv32hf2                                     (rtx, rtx);
extern rtx        gen_roundv16hf2                                     (rtx, rtx);
extern rtx        gen_roundv8hf2                                      (rtx, rtx);
extern rtx        gen_roundv16sf2                                     (rtx, rtx);
extern rtx        gen_roundv8sf2                                      (rtx, rtx);
extern rtx        gen_roundv4sf2                                      (rtx, rtx);
extern rtx        gen_roundv8df2                                      (rtx, rtx);
extern rtx        gen_roundv4df2                                      (rtx, rtx);
extern rtx        gen_roundv2df2                                      (rtx, rtx);
extern rtx        gen_lroundv32hfv32hi2                               (rtx, rtx);
extern rtx        gen_lroundv16hfv16hi2                               (rtx, rtx);
extern rtx        gen_lroundv8hfv8hi2                                 (rtx, rtx);
extern rtx        gen_lroundv16sfv16si2                               (rtx, rtx);
extern rtx        gen_lroundv8sfv8si2                                 (rtx, rtx);
extern rtx        gen_lroundv4sfv4si2                                 (rtx, rtx);
extern rtx        gen_lroundv8dfv8di2                                 (rtx, rtx);
extern rtx        gen_lroundv4dfv4di2                                 (rtx, rtx);
extern rtx        gen_lroundv2dfv2di2                                 (rtx, rtx);
extern rtx        gen_roundv16sf2_sfix                                (rtx, rtx);
extern rtx        gen_roundv8sf2_sfix                                 (rtx, rtx);
extern rtx        gen_roundv4sf2_sfix                                 (rtx, rtx);
extern rtx        gen_roundv8df2_vec_pack_sfix                        (rtx, rtx, rtx);
extern rtx        gen_roundv4df2_vec_pack_sfix                        (rtx, rtx, rtx);
extern rtx        gen_roundv2df2_vec_pack_sfix                        (rtx, rtx, rtx);
extern rtx        gen_rotlv16qi3                                      (rtx, rtx, rtx);
extern rtx        gen_rotlv8hi3                                       (rtx, rtx, rtx);
extern rtx        gen_rotlv4si3                                       (rtx, rtx, rtx);
extern rtx        gen_rotlv2di3                                       (rtx, rtx, rtx);
extern rtx        gen_rotrv16qi3                                      (rtx, rtx, rtx);
extern rtx        gen_rotrv8hi3                                       (rtx, rtx, rtx);
extern rtx        gen_rotrv4si3                                       (rtx, rtx, rtx);
extern rtx        gen_rotrv2di3                                       (rtx, rtx, rtx);
extern rtx        gen_vrotrv16qi3                                     (rtx, rtx, rtx);
extern rtx        gen_vrotrv8hi3                                      (rtx, rtx, rtx);
extern rtx        gen_vrotrv4si3                                      (rtx, rtx, rtx);
extern rtx        gen_vrotrv2di3                                      (rtx, rtx, rtx);
extern rtx        gen_vrotlv16qi3                                     (rtx, rtx, rtx);
extern rtx        gen_vrotlv8hi3                                      (rtx, rtx, rtx);
extern rtx        gen_vrotlv4si3                                      (rtx, rtx, rtx);
extern rtx        gen_vrotlv2di3                                      (rtx, rtx, rtx);
extern rtx        gen_vlshrv16qi3                                     (rtx, rtx, rtx);
extern rtx        gen_vlshrv8hi3                                      (rtx, rtx, rtx);
extern rtx        gen_vlshrv4si3                                      (rtx, rtx, rtx);
extern rtx        gen_vlshrv2di3                                      (rtx, rtx, rtx);
extern rtx        gen_vashlv64qi3                                     (rtx, rtx, rtx);
extern rtx        gen_vlshrv64qi3                                     (rtx, rtx, rtx);
extern rtx        gen_vashrv64qi3                                     (rtx, rtx, rtx);
extern rtx        gen_vashlv32qi3                                     (rtx, rtx, rtx);
extern rtx        gen_vlshrv32qi3                                     (rtx, rtx, rtx);
extern rtx        gen_vashrv32qi3                                     (rtx, rtx, rtx);
extern rtx        gen_vashlv32hi3                                     (rtx, rtx, rtx);
extern rtx        gen_vlshrv32hi3                                     (rtx, rtx, rtx);
extern rtx        gen_vashrv32hi3                                     (rtx, rtx, rtx);
extern rtx        gen_vashlv16hi3                                     (rtx, rtx, rtx);
extern rtx        gen_vlshrv16hi3                                     (rtx, rtx, rtx);
extern rtx        gen_vashrv16hi3                                     (rtx, rtx, rtx);
extern rtx        gen_vlshrv16si3                                     (rtx, rtx, rtx);
extern rtx        gen_vlshrv8di3                                      (rtx, rtx, rtx);
extern rtx        gen_vlshrv8si3                                      (rtx, rtx, rtx);
extern rtx        gen_vlshrv4di3                                      (rtx, rtx, rtx);
extern rtx        gen_vashrv8di3                                      (rtx, rtx, rtx);
extern rtx        gen_vashrv4di3                                      (rtx, rtx, rtx);
extern rtx        gen_vashrv16qi3                                     (rtx, rtx, rtx);
extern rtx        gen_vashrv8hi3                                      (rtx, rtx, rtx);
extern rtx        gen_vashrv2di3                                      (rtx, rtx, rtx);
extern rtx        gen_vashrv4si3                                      (rtx, rtx, rtx);
extern rtx        gen_vashrv16si3                                     (rtx, rtx, rtx);
extern rtx        gen_vashrv8si3                                      (rtx, rtx, rtx);
extern rtx        gen_vashlv16qi3                                     (rtx, rtx, rtx);
extern rtx        gen_vashlv8hi3                                      (rtx, rtx, rtx);
extern rtx        gen_vashlv4si3                                      (rtx, rtx, rtx);
extern rtx        gen_vashlv2di3                                      (rtx, rtx, rtx);
extern rtx        gen_vashlv16si3                                     (rtx, rtx, rtx);
extern rtx        gen_vashlv8di3                                      (rtx, rtx, rtx);
extern rtx        gen_vashlv8si3                                      (rtx, rtx, rtx);
extern rtx        gen_vashlv4di3                                      (rtx, rtx, rtx);
extern rtx        gen_ashlv64qi3                                      (rtx, rtx, rtx);
extern rtx        gen_lshrv64qi3                                      (rtx, rtx, rtx);
extern rtx        gen_ashrv64qi3                                      (rtx, rtx, rtx);
extern rtx        gen_ashlv32qi3                                      (rtx, rtx, rtx);
extern rtx        gen_lshrv32qi3                                      (rtx, rtx, rtx);
extern rtx        gen_ashrv32qi3                                      (rtx, rtx, rtx);
extern rtx        gen_ashlv16qi3                                      (rtx, rtx, rtx);
extern rtx        gen_lshrv16qi3                                      (rtx, rtx, rtx);
extern rtx        gen_ashrv16qi3                                      (rtx, rtx, rtx);
extern rtx        gen_ashrv2di3                                       (rtx, rtx, rtx);
extern rtx        gen_xop_vmfrczv4sf2                                 (rtx, rtx);
extern rtx        gen_xop_vmfrczv2df2                                 (rtx, rtx);
extern rtx        gen_avx_vzeroall                                    (void);
extern rtx        gen_avx_vzeroupper                                  (void);
extern rtx        gen_avx512f_vpermilv8df                             (rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermilv8df_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vpermilv4df                                 (rtx, rtx, rtx);
extern rtx        gen_avx_vpermilv4df_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vpermilv2df                                 (rtx, rtx, rtx);
extern rtx        gen_avx_vpermilv2df_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermilv16sf                            (rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermilv16sf_mask                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vpermilv8sf                                 (rtx, rtx, rtx);
extern rtx        gen_avx_vpermilv8sf_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vpermilv4sf                                 (rtx, rtx, rtx);
extern rtx        gen_avx_vpermilv4sf_mask                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_permv4di                                   (rtx, rtx, rtx);
extern rtx        gen_avx2_permv4df                                   (rtx, rtx, rtx);
extern rtx        gen_avx512vl_permv4di_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_permv4df_mask                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_permv8df                                (rtx, rtx, rtx);
extern rtx        gen_avx512f_permv8di                                (rtx, rtx, rtx);
extern rtx        gen_avx512f_permv8df_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_permv8di_mask                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermi2varv16si3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermi2varv16sf3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermi2varv8di3_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermi2varv8df3_mask                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermi2varv8si3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermi2varv8sf3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermi2varv4di3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermi2varv4df3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermi2varv4si3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermi2varv4sf3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermi2varv2di3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermi2varv2df3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vpermi2varv32hi3_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermi2varv16hi3_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermi2varv8hi3_mask                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vpermi2varv64qi3_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermi2varv32qi3_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermi2varv16qi3_mask                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv16si3_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv16sf3_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv8di3_maskz                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_vpermt2varv8df3_maskz                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv8si3_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv8sf3_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4di3_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4df3_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4si3_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv4sf3_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv2di3_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv2df3_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vpermt2varv32hi3_maskz                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv16hi3_maskz                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv8hi3_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512bw_vpermt2varv64qi3_maskz                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv32qi3_maskz                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vpermt2varv16qi3_maskz                 (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vperm2f128v8si3                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vperm2f128v8sf3                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vperm2f128v4df3                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vinsertv8si                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vinsertv8sf                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vinsertv4di                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_vinsertv4df                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vinsertf128v32qi                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vinsertf128v16hi                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vinsertf128v8si                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vinsertf128v4di                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vinsertf128v8sf                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vinsertf128v4df                             (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vinsertf128v16hf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_avx_vinsertf128v16bf                            (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv4sfv4si_1                              (rtx, rtx, rtx);
extern rtx        gen_maskloadv2dfv2di_1                              (rtx, rtx, rtx);
extern rtx        gen_maskloadv4div4di_1                              (rtx, rtx, rtx);
extern rtx        gen_maskloadv2div2di_1                              (rtx, rtx, rtx);
extern rtx        gen_maskloadv8sfv8si_1                              (rtx, rtx, rtx);
extern rtx        gen_maskloadv4dfv4di_1                              (rtx, rtx, rtx);
extern rtx        gen_maskloadv8siv8si_1                              (rtx, rtx, rtx);
extern rtx        gen_maskloadv4siv4si_1                              (rtx, rtx, rtx);
extern rtx        gen_maskloadv4sfv4si                                (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv2dfv2di                                (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv4div4di                                (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv2div2di                                (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv8sfv8si                                (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv4dfv4di                                (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv8siv8si                                (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv4siv4si                                (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv16sihi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv8siqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv4siqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv8diqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv4diqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv2diqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv16sfhi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv8sfqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv4sfqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv8dfqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv4dfqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv2dfqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv64qidi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv16qihi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv32qisi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv32hisi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv16hihi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv8hiqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv32hfsi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv16hfhi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv8hfqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv32bfsi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv16bfhi                                 (rtx, rtx, rtx, rtx);
extern rtx        gen_maskloadv8bfqi                                  (rtx, rtx, rtx, rtx);
extern rtx        gen_maskstorev4sfv4si                               (rtx, rtx, rtx);
extern rtx        gen_maskstorev2dfv2di                               (rtx, rtx, rtx);
extern rtx        gen_maskstorev4div4di                               (rtx, rtx, rtx);
extern rtx        gen_maskstorev2div2di                               (rtx, rtx, rtx);
extern rtx        gen_maskstorev8sfv8si                               (rtx, rtx, rtx);
extern rtx        gen_maskstorev4dfv4di                               (rtx, rtx, rtx);
extern rtx        gen_maskstorev8siv8si                               (rtx, rtx, rtx);
extern rtx        gen_maskstorev4siv4si                               (rtx, rtx, rtx);
extern rtx        gen_maskstorev16sihi                                (rtx, rtx, rtx);
extern rtx        gen_maskstorev8siqi                                 (rtx, rtx, rtx);
extern rtx        gen_maskstorev4siqi                                 (rtx, rtx, rtx);
extern rtx        gen_maskstorev8diqi                                 (rtx, rtx, rtx);
extern rtx        gen_maskstorev4diqi                                 (rtx, rtx, rtx);
extern rtx        gen_maskstorev2diqi                                 (rtx, rtx, rtx);
extern rtx        gen_maskstorev16sfhi                                (rtx, rtx, rtx);
extern rtx        gen_maskstorev8sfqi                                 (rtx, rtx, rtx);
extern rtx        gen_maskstorev4sfqi                                 (rtx, rtx, rtx);
extern rtx        gen_maskstorev8dfqi                                 (rtx, rtx, rtx);
extern rtx        gen_maskstorev4dfqi                                 (rtx, rtx, rtx);
extern rtx        gen_maskstorev2dfqi                                 (rtx, rtx, rtx);
extern rtx        gen_maskstorev64qidi                                (rtx, rtx, rtx);
extern rtx        gen_maskstorev16qihi                                (rtx, rtx, rtx);
extern rtx        gen_maskstorev32qisi                                (rtx, rtx, rtx);
extern rtx        gen_maskstorev32hisi                                (rtx, rtx, rtx);
extern rtx        gen_maskstorev16hihi                                (rtx, rtx, rtx);
extern rtx        gen_maskstorev8hiqi                                 (rtx, rtx, rtx);
extern rtx        gen_maskstorev32hfsi                                (rtx, rtx, rtx);
extern rtx        gen_maskstorev16hfhi                                (rtx, rtx, rtx);
extern rtx        gen_maskstorev8hfqi                                 (rtx, rtx, rtx);
extern rtx        gen_maskstorev32bfsi                                (rtx, rtx, rtx);
extern rtx        gen_maskstorev16bfhi                                (rtx, rtx, rtx);
extern rtx        gen_maskstorev8bfqi                                 (rtx, rtx, rtx);
extern rtx        gen_cbranchv64qi4                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchv32qi4                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchv16qi4                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchv32hi4                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchv16hi4                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchv8hi4                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchv16si4                                   (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchv8si4                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchv4si4                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchv8di4                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchv4di4                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_cbranchv2di4                                    (rtx, rtx, rtx, rtx);
extern rtx        gen_vec_initv64qiqi                                 (rtx, rtx);
extern rtx        gen_vec_initv32qiqi                                 (rtx, rtx);
extern rtx        gen_vec_initv16qiqi                                 (rtx, rtx);
extern rtx        gen_vec_initv32hihi                                 (rtx, rtx);
extern rtx        gen_vec_initv16hihi                                 (rtx, rtx);
extern rtx        gen_vec_initv8hihi                                  (rtx, rtx);
extern rtx        gen_vec_initv16sisi                                 (rtx, rtx);
extern rtx        gen_vec_initv8sisi                                  (rtx, rtx);
extern rtx        gen_vec_initv4sisi                                  (rtx, rtx);
extern rtx        gen_vec_initv8didi                                  (rtx, rtx);
extern rtx        gen_vec_initv4didi                                  (rtx, rtx);
extern rtx        gen_vec_initv2didi                                  (rtx, rtx);
extern rtx        gen_vec_initv32hfhf                                 (rtx, rtx);
extern rtx        gen_vec_initv16hfhf                                 (rtx, rtx);
extern rtx        gen_vec_initv8hfhf                                  (rtx, rtx);
extern rtx        gen_vec_initv32bfbf                                 (rtx, rtx);
extern rtx        gen_vec_initv16bfbf                                 (rtx, rtx);
extern rtx        gen_vec_initv8bfbf                                  (rtx, rtx);
extern rtx        gen_vec_initv16sfsf                                 (rtx, rtx);
extern rtx        gen_vec_initv8sfsf                                  (rtx, rtx);
extern rtx        gen_vec_initv4sfsf                                  (rtx, rtx);
extern rtx        gen_vec_initv8dfdf                                  (rtx, rtx);
extern rtx        gen_vec_initv4dfdf                                  (rtx, rtx);
extern rtx        gen_vec_initv2dfdf                                  (rtx, rtx);
extern rtx        gen_vec_initv4titi                                  (rtx, rtx);
extern rtx        gen_vec_initv2titi                                  (rtx, rtx);
extern rtx        gen_vec_initv64qiv32qi                              (rtx, rtx);
extern rtx        gen_vec_initv32qiv16qi                              (rtx, rtx);
extern rtx        gen_vec_initv16qiv8qi                               (rtx, rtx);
extern rtx        gen_vec_initv32hiv16hi                              (rtx, rtx);
extern rtx        gen_vec_initv16hiv8hi                               (rtx, rtx);
extern rtx        gen_vec_initv8hiv4hi                                (rtx, rtx);
extern rtx        gen_vec_initv16siv8si                               (rtx, rtx);
extern rtx        gen_vec_initv8siv4si                                (rtx, rtx);
extern rtx        gen_vec_initv4siv2si                                (rtx, rtx);
extern rtx        gen_vec_initv8div4di                                (rtx, rtx);
extern rtx        gen_vec_initv4div2di                                (rtx, rtx);
extern rtx        gen_vec_initv32hfv16hf                              (rtx, rtx);
extern rtx        gen_vec_initv16hfv8hf                               (rtx, rtx);
extern rtx        gen_vec_initv8hfv4hf                                (rtx, rtx);
extern rtx        gen_vec_initv32bfv16bf                              (rtx, rtx);
extern rtx        gen_vec_initv16bfv8bf                               (rtx, rtx);
extern rtx        gen_vec_initv8bfv4bf                                (rtx, rtx);
extern rtx        gen_vec_initv16sfv8sf                               (rtx, rtx);
extern rtx        gen_vec_initv8sfv4sf                                (rtx, rtx);
extern rtx        gen_vec_initv4sfv2sf                                (rtx, rtx);
extern rtx        gen_vec_initv8dfv4df                                (rtx, rtx);
extern rtx        gen_vec_initv4dfv2df                                (rtx, rtx);
extern rtx        gen_vec_initv4tiv2ti                                (rtx, rtx);
extern rtx        gen_cond_ashlv32hi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_lshrv32hi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashrv32hi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashlv16hi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_lshrv16hi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashrv16hi                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashlv8hi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_lshrv8hi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashrv8hi                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashlv16si                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_lshrv16si                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashrv16si                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashlv8si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_lshrv8si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashrv8si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashlv4si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_lshrv4si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashrv4si                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashlv8di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_lshrv8di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashrv8di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashlv4di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_lshrv4di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashrv4di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashlv2di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_lshrv2di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_cond_ashrv2di                                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtps2ph_mask                                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtps2ph                                       (rtx, rtx, rtx);
extern rtx        gen_avx512f_vcvtps2ph512_mask_sae                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gathersiv2di                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gathersiv2df                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gathersiv4di                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gathersiv4df                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gathersiv4si                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gathersiv4sf                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gathersiv8si                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gathersiv8sf                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gatherdiv2di                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gatherdiv2df                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gatherdiv4di                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gatherdiv4df                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gatherdiv4si                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gatherdiv4sf                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gatherdiv8si                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx2_gatherdiv8sf                               (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_gathersiv16si                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_gathersiv16sf                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_gathersiv8di                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_gathersiv8df                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gathersiv8si                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gathersiv8sf                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gathersiv4di                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gathersiv4df                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gathersiv4si                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gathersiv4sf                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gathersiv2di                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gathersiv2df                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_gatherdiv16si                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_gatherdiv16sf                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_gatherdiv8di                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_gatherdiv8df                            (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gatherdiv8si                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gatherdiv8sf                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gatherdiv4di                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gatherdiv4df                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gatherdiv4si                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gatherdiv4sf                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gatherdiv2di                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_gatherdiv2df                           (rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scattersiv16si                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scattersiv16sf                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scattersiv8di                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scattersiv8df                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scattersiv8si                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scattersiv8sf                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scattersiv4di                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scattersiv4df                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scattersiv4si                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scattersiv4sf                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scattersiv2di                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scattersiv2df                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scatterdiv16si                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scatterdiv16sf                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scatterdiv8di                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_scatterdiv8df                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scatterdiv8si                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scatterdiv8sf                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scatterdiv4di                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scatterdiv4df                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scatterdiv4si                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scatterdiv4sf                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scatterdiv2di                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_scatterdiv2df                          (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_expandv16si_maskz                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_expandv16sf_maskz                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_expandv8di_maskz                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_expandv8df_maskz                        (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_expandv8si_maskz                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_expandv8sf_maskz                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_expandv4di_maskz                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_expandv4df_maskz                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_expandv4si_maskz                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_expandv4sf_maskz                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_expandv2di_maskz                       (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512vl_expandv2df_maskz                       (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv64qi_maskz                               (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv16qi_maskz                               (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv32qi_maskz                               (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv32hi_maskz                               (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv16hi_maskz                               (rtx, rtx, rtx, rtx);
extern rtx        gen_expandv8hi_maskz                                (rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52huqv8di_maskz                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52huqv4di_maskz                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52huqv2di_maskz                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52luqv8di_maskz                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52luqv4di_maskz                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpmadd52luqv2di_maskz                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_popcountv16si2                                  (rtx, rtx);
extern rtx        gen_popcountv8si2                                   (rtx, rtx);
extern rtx        gen_popcountv4si2                                   (rtx, rtx);
extern rtx        gen_popcountv8di2                                   (rtx, rtx);
extern rtx        gen_popcountv4di2                                   (rtx, rtx);
extern rtx        gen_popcountv2di2                                   (rtx, rtx);
extern rtx        gen_popcountv64qi2                                  (rtx, rtx);
extern rtx        gen_popcountv16qi2                                  (rtx, rtx);
extern rtx        gen_popcountv32qi2                                  (rtx, rtx);
extern rtx        gen_popcountv32hi2                                  (rtx, rtx);
extern rtx        gen_popcountv16hi2                                  (rtx, rtx);
extern rtx        gen_popcountv8hi2                                   (rtx, rtx);
extern rtx        gen_vpshrdv_v32hi_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v16si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v8di_maskz                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v16hi_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v8si_maskz                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v4di_maskz                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v8hi_maskz                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v4si_maskz                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshrdv_v2di_maskz                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v32hi_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v16si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v8di_maskz                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v16hi_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v8si_maskz                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v4di_maskz                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v8hi_maskz                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v4si_maskz                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpshldv_v2di_maskz                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_usdot_prodv16siv64qi                            (rtx, rtx, rtx, rtx);
extern rtx        gen_usdot_prodv8siv32qi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_usdot_prodv4siv16qi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusd_v16si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusd_v8si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusd_v4si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusds_v16si_maskz                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusds_v8si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbusds_v4si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssd_v16si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssd_v8si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssd_v4si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssds_v16si_maskz                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssds_v8si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwssds_v4si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_movp2qi                                         (rtx, rtx);
extern rtx        gen_movp2hi                                         (rtx, rtx);
extern rtx        gen_avx512f_cvtne2ps2bf16_v32bf_maskz               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtne2ps2bf16_v16bf_maskz               (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtne2ps2bf16_v8bf_maskz                (rtx, rtx, rtx, rtx);
extern rtx        gen_truncv4sfv4bf2                                  (rtx, rtx);
extern rtx        gen_vcvtneps2bf16_v4sf                              (rtx, rtx);
extern rtx        gen_avx512f_cvtneps2bf16_v4sf_maskz                 (rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtneps2bf16_v4sf_mask                  (rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtneps2bf16_v16sf_maskz                (rtx, rtx, rtx);
extern rtx        gen_avx512f_cvtneps2bf16_v8sf_maskz                 (rtx, rtx, rtx);
extern rtx        gen_truncv8sfv8bf2                                  (rtx, rtx);
extern rtx        gen_truncv16sfv16bf2                                (rtx, rtx);
extern rtx        gen_extendv16bfv16sf2                               (rtx, rtx);
extern rtx        gen_extendv8bfv8sf2                                 (rtx, rtx);
extern rtx        gen_extendv4bfv4sf2                                 (rtx, rtx);
extern rtx        gen_avx512f_dpbf16ps_v16sf_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_dpbf16ps_v8sf_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx512f_dpbf16ps_v4sf_maskz                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_encodekey128u32                                 (rtx, rtx);
extern rtx        gen_encodekey256u32                                 (rtx, rtx);
extern rtx        gen_aesdecwide128klu8                               (rtx);
extern rtx        gen_aesdecwide256klu8                               (rtx);
extern rtx        gen_aesencwide128klu8                               (rtx);
extern rtx        gen_aesencwide256klu8                               (rtx);
extern rtx        gen_vec_duplicatev64qi                              (rtx, rtx);
extern rtx        gen_vec_duplicatev32qi                              (rtx, rtx);
extern rtx        gen_vec_duplicatev16qi                              (rtx, rtx);
extern rtx        gen_vec_duplicatev32hi                              (rtx, rtx);
extern rtx        gen_vec_duplicatev16hi                              (rtx, rtx);
extern rtx        gen_vec_duplicatev8hi                               (rtx, rtx);
extern rtx        gen_vec_duplicatev16si                              (rtx, rtx);
extern rtx        gen_vec_duplicatev8si                               (rtx, rtx);
extern rtx        gen_vec_duplicatev4si                               (rtx, rtx);
static inline rtx gen_vec_duplicatev8di                               (rtx, rtx);
static inline rtx
gen_vec_duplicatev8di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_vec_duplicatev4di                               (rtx, rtx);
static inline rtx
gen_vec_duplicatev4di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
static inline rtx gen_vec_duplicatev2di                               (rtx, rtx);
static inline rtx
gen_vec_duplicatev2di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b))
{
  return 0;
}
extern rtx        gen_sdot_prodv16siv64qi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_sdot_prodv8siv32qi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_sdot_prodv4siv16qi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_udot_prodv16siv64qi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_udot_prodv8siv32qi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_udot_prodv4siv16qi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssd_v16si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssds_v16si_maskz                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsud_v16si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsuds_v16si_maskz                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuud_v16si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuuds_v16si_maskz                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssd_v8si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssds_v8si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsud_v8si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsuds_v8si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuud_v8si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuuds_v8si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssd_v4si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbssds_v4si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsud_v4si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbsuds_v4si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuud_v4si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpbuuds_v4si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2bf8v8hf                              (rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2bf8sv8hf                             (rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2hf8v8hf                              (rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2hf8sv8hf                             (rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2bf8v8hf_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2bf8sv8hf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2hf8v8hf_mask                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtbiasph2hf8sv8hf_mask                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtph2bf8v8hf                                  (rtx, rtx);
extern rtx        gen_vcvtph2bf8sv8hf                                 (rtx, rtx);
extern rtx        gen_vcvtph2hf8v8hf                                  (rtx, rtx);
extern rtx        gen_vcvtph2hf8sv8hf                                 (rtx, rtx);
extern rtx        gen_vcvtph2bf8v8hf_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtph2bf8sv8hf_mask                            (rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtph2hf8v8hf_mask                             (rtx, rtx, rtx, rtx);
extern rtx        gen_vcvtph2hf8sv8hf_mask                            (rtx, rtx, rtx, rtx);
extern rtx        gen_usdot_prodv16siv32hi                            (rtx, rtx, rtx, rtx);
extern rtx        gen_usdot_prodv8siv16hi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_usdot_prodv4siv8hi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_udot_prodv16siv32hi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_udot_prodv8siv16hi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_udot_prodv4siv8hi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusd_v16si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusds_v16si_maskz                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsud_v16si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsuds_v16si_maskz                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuud_v16si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuuds_v16si_maskz                           (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusd_v8si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusds_v8si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsud_v8si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsuds_v8si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuud_v8si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuuds_v8si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusd_v4si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwusds_v4si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsud_v4si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwsuds_v4si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuud_v4si_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vpdpwuuds_v4si_maskz                            (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vdpphps_v16sf_maskz                             (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vdpphps_v8sf_maskz                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_vdpphps_v4sf_maskz                              (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_smaxv32bf3                                      (rtx, rtx, rtx);
extern rtx        gen_sminv32bf3                                      (rtx, rtx, rtx);
extern rtx        gen_smaxv16bf3                                      (rtx, rtx, rtx);
extern rtx        gen_sminv16bf3                                      (rtx, rtx, rtx);
extern rtx        gen_smaxv8bf3                                       (rtx, rtx, rtx);
extern rtx        gen_sminv8bf3                                       (rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v32bf_maskz                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v16bf_maskz                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmaddbf16_v8bf_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v32bf_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v16bf_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmaddbf16_v8bf_maskz                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v32bf_maskz                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v16bf_maskz                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fmsubbf16_v8bf_maskz                    (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v32bf_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v16bf_maskz                  (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_avx10_2_fnmsubbf16_v8bf_maskz                   (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_sse2_lfence                                     (void);
extern rtx        gen_sse_sfence                                      (void);
extern rtx        gen_sse2_mfence                                     (void);
extern rtx        gen_mem_thread_fence                                (rtx);
extern rtx        gen_atomic_loadqi                                   (rtx, rtx, rtx);
extern rtx        gen_atomic_loadhi                                   (rtx, rtx, rtx);
extern rtx        gen_atomic_loadsi                                   (rtx, rtx, rtx);
extern rtx        gen_atomic_loaddi                                   (rtx, rtx, rtx);
extern rtx        gen_atomic_storeqi                                  (rtx, rtx, rtx);
extern rtx        gen_atomic_storehi                                  (rtx, rtx, rtx);
extern rtx        gen_atomic_storesi                                  (rtx, rtx, rtx);
extern rtx        gen_atomic_storedi                                  (rtx, rtx, rtx);
extern rtx        gen_atomic_compare_and_swapqi                       (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_compare_and_swaphi                       (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_compare_and_swapsi                       (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_compare_and_swapdi                       (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_atomic_compare_and_swapti                       (rtx, rtx, rtx, rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_compare_and_swapti(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e), rtx ARG_UNUSED (f), rtx ARG_UNUSED (g), rtx ARG_UNUSED (h))
{
  return 0;
}
extern rtx        gen_atomic_fetch_andqi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_orqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_xorqi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_andhi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_orhi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_xorhi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_andsi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_orsi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_xorsi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_and_fetchqi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_or_fetchqi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_xor_fetchqi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_and_fetchhi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_or_fetchhi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_xor_fetchhi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_and_fetchsi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_or_fetchsi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_xor_fetchsi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_nandqi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_nandhi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_nandsi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_nand_fetchqi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_nand_fetchhi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_nand_fetchsi                             (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_anddi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_ordi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_fetch_xordi                              (rtx, rtx, rtx, rtx);
static inline rtx gen_atomic_fetch_andti                              (rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_fetch_andti(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_atomic_fetch_orti                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_fetch_orti(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_atomic_fetch_xorti                              (rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_fetch_xorti(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_atomic_and_fetchdi                              (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_or_fetchdi                               (rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_xor_fetchdi                              (rtx, rtx, rtx, rtx);
static inline rtx gen_atomic_and_fetchti                              (rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_and_fetchti(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_atomic_or_fetchti                               (rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_or_fetchti(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
static inline rtx gen_atomic_xor_fetchti                              (rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_xor_fetchti(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_atomic_fetch_nanddi                             (rtx, rtx, rtx, rtx);
static inline rtx gen_atomic_fetch_nandti                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_fetch_nandti(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_atomic_nand_fetchdi                             (rtx, rtx, rtx, rtx);
static inline rtx gen_atomic_nand_fetchti                             (rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_nand_fetchti(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d))
{
  return 0;
}
extern rtx        gen_atomic_bit_test_and_sethi                       (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_bit_test_and_setsi                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_atomic_bit_test_and_setdi                       (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_bit_test_and_setdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_atomic_bit_test_and_complementhi                (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_bit_test_and_complementsi                (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_atomic_bit_test_and_complementdi                (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_bit_test_and_complementdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_atomic_bit_test_and_resethi                     (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_bit_test_and_resetsi                     (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_atomic_bit_test_and_resetdi                     (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_bit_test_and_resetdi(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_atomic_add_fetch_cmp_0qi                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_sub_fetch_cmp_0qi                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_add_fetch_cmp_0hi                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_sub_fetch_cmp_0hi                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_add_fetch_cmp_0si                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_sub_fetch_cmp_0si                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_atomic_add_fetch_cmp_0di                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_add_fetch_cmp_0di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
static inline rtx gen_atomic_sub_fetch_cmp_0di                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_sub_fetch_cmp_0di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
extern rtx        gen_atomic_and_fetch_cmp_0qi                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_or_fetch_cmp_0qi                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_xor_fetch_cmp_0qi                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_and_fetch_cmp_0hi                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_or_fetch_cmp_0hi                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_xor_fetch_cmp_0hi                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_and_fetch_cmp_0si                        (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_or_fetch_cmp_0si                         (rtx, rtx, rtx, rtx, rtx);
extern rtx        gen_atomic_xor_fetch_cmp_0si                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx gen_atomic_and_fetch_cmp_0di                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_and_fetch_cmp_0di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
static inline rtx gen_atomic_or_fetch_cmp_0di                         (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_or_fetch_cmp_0di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}
static inline rtx gen_atomic_xor_fetch_cmp_0di                        (rtx, rtx, rtx, rtx, rtx);
static inline rtx
gen_atomic_xor_fetch_cmp_0di(rtx ARG_UNUSED (a), rtx ARG_UNUSED (b), rtx ARG_UNUSED (c), rtx ARG_UNUSED (d), rtx ARG_UNUSED (e))
{
  return 0;
}

#endif /* GCC_INSN_FLAGS_H */
